{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 22 21:08:30 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   44C    P0              60W / 184W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/bin/python\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!which python | grep DYY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SinousEmbedding(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        assert dim%2==0,NotImplementedError()\n",
    "        self.angles = (10000.**(-2/dim))**torch.arange(1,dim//2+1,1,dtype=torch.float).cuda()\n",
    "        self.angles.requires_grad_(False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        angles = torch.einsum('m,i->im',self.angles,x.float())\n",
    "        return torch.cat((torch.sin(angles),torch.cos(angles)),dim=1)\n",
    "    \n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,dim,head):\n",
    "        super().__init__()\n",
    "        assert dim%head==0,NotImplementedError()\n",
    "        self.head = head\n",
    "        self.head_dim = dim // head\n",
    "        self.Q = nn.Linear(dim,dim,bias=False)\n",
    "        self.K = nn.Linear(dim,dim,bias=False)\n",
    "        self.V = nn.Linear(dim,dim,bias=False)\n",
    "        self.out_proj = nn.Linear(dim,dim)\n",
    "        self.apply_init()\n",
    "\n",
    "    def apply_init(self):\n",
    "        self.Q.weight.data.normal_(0,0.02)\n",
    "        self.K.weight.data.normal_(0,0.02)\n",
    "        self.V.weight.data.normal_(0,0.02)\n",
    "        self.out_proj.weight.data.zero_()\n",
    "        self.out_proj.bias.data.zero_()\n",
    "    \n",
    "    def forward(self,query,context):\n",
    "        q = self.Q(query).reshape(*query.shape[:2],self.head,self.head_dim)\n",
    "        k = self.K(context).reshape(*context.shape[:2],self.head,self.head_dim)\n",
    "        v = self.V(context).reshape(*context.shape[:2],self.head,self.head_dim)\n",
    "        score = torch.einsum('bihd,bjhd->bijh',q,k) / (self.head_dim**0.5)\n",
    "        score = F.softmax(score,dim=2)\n",
    "        out = torch.einsum('bijh,bjhd->bihd',score,v).reshape_as(query)\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return 0.5 * x * (1 + torch.tanh(0.7978845608 * (x + 0.044715 * x**3)))\n",
    "    \n",
    "class SiLU(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "class AdaptiveLayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim,elementwise_affine=False)\n",
    "        self.beta_gamma = nn.Sequential(\n",
    "            SiLU(),\n",
    "            nn.Linear(dim,2*dim)\n",
    "        )\n",
    "        self.beta_gamma[-1].weight.data.zero_()\n",
    "        self.beta_gamma[-1].bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, conditioned):\n",
    "        x = self.norm(x)\n",
    "        beta,gamma = self.beta_gamma(conditioned).unsqueeze(1).chunk(2,dim=-1)\n",
    "        return x * (1+gamma) + beta\n",
    "\n",
    "class Layer(nn.Module):\n",
    "\n",
    "    def __init__(self,dim,head):\n",
    "        super().__init__()\n",
    "        self.attn = Attn(dim,head)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            GELU(),\n",
    "            nn.Linear(dim,dim)\n",
    "        )\n",
    "        self.condition_mlp = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            SiLU(),\n",
    "            nn.Linear(dim,2*dim)\n",
    "        )\n",
    "        self.norm1 = AdaptiveLayerNorm(dim)\n",
    "        self.norm2 = AdaptiveLayerNorm(dim)\n",
    "        self.apply_init()\n",
    "\n",
    "    def apply_init(self):\n",
    "        self.condition_mlp[2].weight.data.zero_()\n",
    "        self.condition_mlp[2].bias.data.zero_()\n",
    "\n",
    "    def forward(self,x,condition):\n",
    "        alpha1,alpha2 = self.condition_mlp(condition).unsqueeze(1).chunk(2,dim=-1)\n",
    "\n",
    "        # first half\n",
    "        xc = x.clone()\n",
    "        x = self.norm1(x,condition)\n",
    "        x = self.attn(x,x)\n",
    "        x = x * alpha1\n",
    "        x = x + xc\n",
    "\n",
    "        # second half\n",
    "        xc = x.clone()\n",
    "        x = self.norm2(x,condition)\n",
    "        x = self.mlp(x)\n",
    "        x = x * alpha2\n",
    "        x = x + xc\n",
    "\n",
    "        return x\n",
    "\n",
    "class DiT(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "            patch_size=4,\n",
    "            hidden_dim=256,\n",
    "            num_layers=6,\n",
    "            image_size = 28*28,\n",
    "            num_heads=8\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.num_patches = image_size // (patch_size * patch_size)\n",
    "        self.embedding = nn.Linear(patch_size * patch_size, hidden_dim, bias=False)\n",
    "        self.pos_embedding = SinousEmbedding(hidden_dim)\n",
    "        self.t_embedding = nn.Sequential(\n",
    "            SinousEmbedding(64),\n",
    "            nn.Linear(64,hidden_dim),\n",
    "            SiLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim)\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList([Layer(hidden_dim,num_heads) for _ in range(num_layers)])\n",
    "        self.out_norm = AdaptiveLayerNorm(hidden_dim)\n",
    "        # self.out_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, patch_size * patch_size, bias=False)\n",
    "        self.apply_init()\n",
    "\n",
    "    def apply_init(self):\n",
    "        self.embedding.weight.data.normal_(0,0.02)\n",
    "        self.out_proj.weight.data.zero_()\n",
    "\n",
    "    def first(self,x,t):\n",
    "        t_embed = self.t_embedding(t)\n",
    "        # patchify the image x\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size) # [B, 1, H/4, W/4, 4, 4]\n",
    "        x_embed = self.embedding(x.reshape(x.shape[0],-1,self.patch_size*self.patch_size))\n",
    "        # print('x range:',x_embed.min(),x_embed.max(),x_embed.std())\n",
    "        x_embed += self.pos_embedding(torch.arange(x_embed.shape[1],device=device))\n",
    "        # print('position embedding range:',x_embed.min(),x_embed.max(),x_embed.std())\n",
    "        data = torch.cat((x_embed,t_embed.unsqueeze(1)),dim=1)\n",
    "        # print('position embedding range:',pos_embed.min(),pos_embed.max(),pos_embed.std())\n",
    "        # print('x range:',data.min(),data.max(),data.std())\n",
    "        return data, t_embed\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        x = x.reshape(x.shape[0],1,28,28)\n",
    "        inputs, conditioned = self.first(x,t)\n",
    "        for i,ly in enumerate(self.layers):\n",
    "            inputs = ly(inputs,conditioned)\n",
    "            # print('layer',i,'input.range:',inputs.min(),inputs.max())\n",
    "        # remove t token\n",
    "        inputs = inputs[:,:-1,:]\n",
    "        inputs = self.out_norm(inputs,conditioned)\n",
    "        # print('output range:',inputs.min(),inputs.max())\n",
    "        inputs = self.out_proj(inputs)\n",
    "        # print('output range:',inputs.min(),inputs.max())\n",
    "        # patchify the image x\n",
    "        length = 28 // self.patch_size\n",
    "        inputs = inputs.reshape(inputs.shape[0],length,length,self.patch_size,self.patch_size).permute(0,3,4,1,2).reshape(inputs.shape[0],self.patch_size*self.patch_size,length*length)\n",
    "        x = F.fold(inputs, (28,28), self.patch_size, stride=self.patch_size)\n",
    "        return x.reshape(x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended /home/zhh24/DeepLearning\n",
      "tensor([9.9996e-01, 9.9991e-01, 9.9986e-01, 9.9981e-01, 9.9975e-01, 9.9968e-01,\n",
      "        9.9961e-01, 9.9953e-01, 9.9945e-01, 9.9937e-01, 9.9928e-01, 9.9918e-01,\n",
      "        9.9908e-01, 9.9898e-01, 9.9887e-01, 9.9876e-01, 9.9864e-01, 9.9851e-01,\n",
      "        9.9839e-01, 9.9825e-01, 9.9811e-01, 9.9797e-01, 9.9782e-01, 9.9767e-01,\n",
      "        9.9751e-01, 9.9735e-01, 9.9718e-01, 9.9701e-01, 9.9683e-01, 9.9665e-01,\n",
      "        9.9647e-01, 9.9627e-01, 9.9608e-01, 9.9588e-01, 9.9567e-01, 9.9546e-01,\n",
      "        9.9525e-01, 9.9502e-01, 9.9480e-01, 9.9457e-01, 9.9434e-01, 9.9410e-01,\n",
      "        9.9385e-01, 9.9360e-01, 9.9335e-01, 9.9309e-01, 9.9283e-01, 9.9256e-01,\n",
      "        9.9229e-01, 9.9201e-01, 9.9172e-01, 9.9144e-01, 9.9115e-01, 9.9085e-01,\n",
      "        9.9055e-01, 9.9024e-01, 9.8993e-01, 9.8961e-01, 9.8929e-01, 9.8897e-01,\n",
      "        9.8864e-01, 9.8830e-01, 9.8796e-01, 9.8762e-01, 9.8727e-01, 9.8691e-01,\n",
      "        9.8656e-01, 9.8619e-01, 9.8582e-01, 9.8545e-01, 9.8507e-01, 9.8469e-01,\n",
      "        9.8430e-01, 9.8391e-01, 9.8352e-01, 9.8312e-01, 9.8271e-01, 9.8230e-01,\n",
      "        9.8188e-01, 9.8146e-01, 9.8104e-01, 9.8061e-01, 9.8018e-01, 9.7974e-01,\n",
      "        9.7930e-01, 9.7885e-01, 9.7840e-01, 9.7794e-01, 9.7748e-01, 9.7701e-01,\n",
      "        9.7654e-01, 9.7606e-01, 9.7558e-01, 9.7510e-01, 9.7461e-01, 9.7412e-01,\n",
      "        9.7362e-01, 9.7311e-01, 9.7261e-01, 9.7209e-01, 9.7158e-01, 9.7105e-01,\n",
      "        9.7053e-01, 9.7000e-01, 9.6946e-01, 9.6892e-01, 9.6838e-01, 9.6783e-01,\n",
      "        9.6727e-01, 9.6672e-01, 9.6615e-01, 9.6559e-01, 9.6502e-01, 9.6444e-01,\n",
      "        9.6386e-01, 9.6327e-01, 9.6268e-01, 9.6209e-01, 9.6149e-01, 9.6089e-01,\n",
      "        9.6028e-01, 9.5967e-01, 9.5905e-01, 9.5843e-01, 9.5780e-01, 9.5717e-01,\n",
      "        9.5654e-01, 9.5590e-01, 9.5526e-01, 9.5461e-01, 9.5396e-01, 9.5330e-01,\n",
      "        9.5264e-01, 9.5198e-01, 9.5131e-01, 9.5063e-01, 9.4995e-01, 9.4927e-01,\n",
      "        9.4858e-01, 9.4789e-01, 9.4720e-01, 9.4650e-01, 9.4579e-01, 9.4508e-01,\n",
      "        9.4437e-01, 9.4365e-01, 9.4293e-01, 9.4220e-01, 9.4147e-01, 9.4074e-01,\n",
      "        9.4000e-01, 9.3926e-01, 9.3851e-01, 9.3776e-01, 9.3700e-01, 9.3624e-01,\n",
      "        9.3548e-01, 9.3471e-01, 9.3393e-01, 9.3316e-01, 9.3238e-01, 9.3159e-01,\n",
      "        9.3080e-01, 9.3001e-01, 9.2921e-01, 9.2841e-01, 9.2760e-01, 9.2679e-01,\n",
      "        9.2597e-01, 9.2516e-01, 9.2433e-01, 9.2351e-01, 9.2267e-01, 9.2184e-01,\n",
      "        9.2100e-01, 9.2016e-01, 9.1931e-01, 9.1846e-01, 9.1760e-01, 9.1674e-01,\n",
      "        9.1588e-01, 9.1501e-01, 9.1414e-01, 9.1326e-01, 9.1238e-01, 9.1150e-01,\n",
      "        9.1061e-01, 9.0972e-01, 9.0882e-01, 9.0792e-01, 9.0702e-01, 9.0611e-01,\n",
      "        9.0520e-01, 9.0428e-01, 9.0336e-01, 9.0244e-01, 9.0151e-01, 9.0058e-01,\n",
      "        8.9965e-01, 8.9871e-01, 8.9776e-01, 8.9682e-01, 8.9587e-01, 8.9491e-01,\n",
      "        8.9395e-01, 8.9299e-01, 8.9202e-01, 8.9105e-01, 8.9008e-01, 8.8910e-01,\n",
      "        8.8812e-01, 8.8714e-01, 8.8615e-01, 8.8516e-01, 8.8416e-01, 8.8316e-01,\n",
      "        8.8216e-01, 8.8115e-01, 8.8014e-01, 8.7912e-01, 8.7810e-01, 8.7708e-01,\n",
      "        8.7606e-01, 8.7503e-01, 8.7399e-01, 8.7296e-01, 8.7192e-01, 8.7087e-01,\n",
      "        8.6982e-01, 8.6877e-01, 8.6772e-01, 8.6666e-01, 8.6560e-01, 8.6453e-01,\n",
      "        8.6346e-01, 8.6239e-01, 8.6132e-01, 8.6024e-01, 8.5915e-01, 8.5807e-01,\n",
      "        8.5698e-01, 8.5588e-01, 8.5479e-01, 8.5369e-01, 8.5258e-01, 8.5147e-01,\n",
      "        8.5036e-01, 8.4925e-01, 8.4813e-01, 8.4701e-01, 8.4589e-01, 8.4476e-01,\n",
      "        8.4363e-01, 8.4250e-01, 8.4136e-01, 8.4022e-01, 8.3907e-01, 8.3793e-01,\n",
      "        8.3677e-01, 8.3562e-01, 8.3446e-01, 8.3330e-01, 8.3214e-01, 8.3097e-01,\n",
      "        8.2980e-01, 8.2863e-01, 8.2745e-01, 8.2627e-01, 8.2509e-01, 8.2390e-01,\n",
      "        8.2271e-01, 8.2152e-01, 8.2033e-01, 8.1913e-01, 8.1793e-01, 8.1672e-01,\n",
      "        8.1551e-01, 8.1430e-01, 8.1309e-01, 8.1187e-01, 8.1065e-01, 8.0943e-01,\n",
      "        8.0820e-01, 8.0697e-01, 8.0574e-01, 8.0451e-01, 8.0327e-01, 8.0203e-01,\n",
      "        8.0078e-01, 7.9954e-01, 7.9829e-01, 7.9703e-01, 7.9578e-01, 7.9452e-01,\n",
      "        7.9326e-01, 7.9200e-01, 7.9073e-01, 7.8946e-01, 7.8819e-01, 7.8691e-01,\n",
      "        7.8563e-01, 7.8435e-01, 7.8307e-01, 7.8178e-01, 7.8049e-01, 7.7920e-01,\n",
      "        7.7791e-01, 7.7661e-01, 7.7531e-01, 7.7401e-01, 7.7270e-01, 7.7139e-01,\n",
      "        7.7008e-01, 7.6877e-01, 7.6745e-01, 7.6614e-01, 7.6481e-01, 7.6349e-01,\n",
      "        7.6216e-01, 7.6084e-01, 7.5950e-01, 7.5817e-01, 7.5683e-01, 7.5550e-01,\n",
      "        7.5415e-01, 7.5281e-01, 7.5147e-01, 7.5012e-01, 7.4877e-01, 7.4741e-01,\n",
      "        7.4606e-01, 7.4470e-01, 7.4334e-01, 7.4197e-01, 7.4061e-01, 7.3924e-01,\n",
      "        7.3787e-01, 7.3650e-01, 7.3513e-01, 7.3375e-01, 7.3237e-01, 7.3099e-01,\n",
      "        7.2960e-01, 7.2822e-01, 7.2683e-01, 7.2544e-01, 7.2405e-01, 7.2265e-01,\n",
      "        7.2126e-01, 7.1986e-01, 7.1846e-01, 7.1705e-01, 7.1565e-01, 7.1424e-01,\n",
      "        7.1283e-01, 7.1142e-01, 7.1001e-01, 7.0859e-01, 7.0717e-01, 7.0575e-01,\n",
      "        7.0433e-01, 7.0291e-01, 7.0148e-01, 7.0005e-01, 6.9863e-01, 6.9719e-01,\n",
      "        6.9576e-01, 6.9433e-01, 6.9289e-01, 6.9145e-01, 6.9001e-01, 6.8857e-01,\n",
      "        6.8712e-01, 6.8567e-01, 6.8423e-01, 6.8278e-01, 6.8132e-01, 6.7987e-01,\n",
      "        6.7842e-01, 6.7696e-01, 6.7550e-01, 6.7404e-01, 6.7258e-01, 6.7111e-01,\n",
      "        6.6965e-01, 6.6818e-01, 6.6671e-01, 6.6524e-01, 6.6377e-01, 6.6230e-01,\n",
      "        6.6082e-01, 6.5935e-01, 6.5787e-01, 6.5639e-01, 6.5491e-01, 6.5342e-01,\n",
      "        6.5194e-01, 6.5045e-01, 6.4897e-01, 6.4748e-01, 6.4599e-01, 6.4450e-01,\n",
      "        6.4300e-01, 6.4151e-01, 6.4001e-01, 6.3852e-01, 6.3702e-01, 6.3552e-01,\n",
      "        6.3402e-01, 6.3252e-01, 6.3101e-01, 6.2951e-01, 6.2800e-01, 6.2649e-01,\n",
      "        6.2499e-01, 6.2348e-01, 6.2196e-01, 6.2045e-01, 6.1894e-01, 6.1742e-01,\n",
      "        6.1591e-01, 6.1439e-01, 6.1287e-01, 6.1136e-01, 6.0984e-01, 6.0831e-01,\n",
      "        6.0679e-01, 6.0527e-01, 6.0374e-01, 6.0222e-01, 6.0069e-01, 5.9917e-01,\n",
      "        5.9764e-01, 5.9611e-01, 5.9458e-01, 5.9305e-01, 5.9152e-01, 5.8998e-01,\n",
      "        5.8845e-01, 5.8692e-01, 5.8538e-01, 5.8384e-01, 5.8231e-01, 5.8077e-01,\n",
      "        5.7923e-01, 5.7769e-01, 5.7615e-01, 5.7461e-01, 5.7307e-01, 5.7153e-01,\n",
      "        5.6998e-01, 5.6844e-01, 5.6690e-01, 5.6535e-01, 5.6381e-01, 5.6226e-01,\n",
      "        5.6071e-01, 5.5916e-01, 5.5762e-01, 5.5607e-01, 5.5452e-01, 5.5297e-01,\n",
      "        5.5142e-01, 5.4987e-01, 5.4832e-01, 5.4677e-01, 5.4521e-01, 5.4366e-01,\n",
      "        5.4211e-01, 5.4056e-01, 5.3900e-01, 5.3745e-01, 5.3589e-01, 5.3434e-01,\n",
      "        5.3278e-01, 5.3123e-01, 5.2967e-01, 5.2812e-01, 5.2656e-01, 5.2500e-01,\n",
      "        5.2345e-01, 5.2189e-01, 5.2033e-01, 5.1878e-01, 5.1722e-01, 5.1566e-01,\n",
      "        5.1410e-01, 5.1255e-01, 5.1099e-01, 5.0943e-01, 5.0787e-01, 5.0631e-01,\n",
      "        5.0475e-01, 5.0319e-01, 5.0164e-01, 5.0008e-01, 4.9852e-01, 4.9696e-01,\n",
      "        4.9540e-01, 4.9384e-01, 4.9229e-01, 4.9073e-01, 4.8917e-01, 4.8761e-01,\n",
      "        4.8605e-01, 4.8449e-01, 4.8294e-01, 4.8138e-01, 4.7982e-01, 4.7826e-01,\n",
      "        4.7671e-01, 4.7515e-01, 4.7359e-01, 4.7204e-01, 4.7048e-01, 4.6893e-01,\n",
      "        4.6737e-01, 4.6582e-01, 4.6426e-01, 4.6271e-01, 4.6115e-01, 4.5960e-01,\n",
      "        4.5805e-01, 4.5649e-01, 4.5494e-01, 4.5339e-01, 4.5184e-01, 4.5029e-01,\n",
      "        4.4874e-01, 4.4719e-01, 4.4564e-01, 4.4409e-01, 4.4254e-01, 4.4099e-01,\n",
      "        4.3944e-01, 4.3790e-01, 4.3635e-01, 4.3480e-01, 4.3326e-01, 4.3172e-01,\n",
      "        4.3017e-01, 4.2863e-01, 4.2709e-01, 4.2555e-01, 4.2400e-01, 4.2246e-01,\n",
      "        4.2093e-01, 4.1939e-01, 4.1785e-01, 4.1631e-01, 4.1478e-01, 4.1324e-01,\n",
      "        4.1171e-01, 4.1017e-01, 4.0864e-01, 4.0711e-01, 4.0558e-01, 4.0405e-01,\n",
      "        4.0252e-01, 4.0099e-01, 3.9946e-01, 3.9794e-01, 3.9641e-01, 3.9489e-01,\n",
      "        3.9336e-01, 3.9184e-01, 3.9032e-01, 3.8880e-01, 3.8728e-01, 3.8576e-01,\n",
      "        3.8425e-01, 3.8273e-01, 3.8122e-01, 3.7970e-01, 3.7819e-01, 3.7668e-01,\n",
      "        3.7517e-01, 3.7366e-01, 3.7215e-01, 3.7065e-01, 3.6914e-01, 3.6764e-01,\n",
      "        3.6614e-01, 3.6464e-01, 3.6314e-01, 3.6164e-01, 3.6014e-01, 3.5865e-01,\n",
      "        3.5715e-01, 3.5566e-01, 3.5417e-01, 3.5268e-01, 3.5119e-01, 3.4970e-01,\n",
      "        3.4822e-01, 3.4673e-01, 3.4525e-01, 3.4377e-01, 3.4229e-01, 3.4081e-01,\n",
      "        3.3933e-01, 3.3786e-01, 3.3638e-01, 3.3491e-01, 3.3344e-01, 3.3197e-01,\n",
      "        3.3051e-01, 3.2904e-01, 3.2758e-01, 3.2612e-01, 3.2466e-01, 3.2320e-01,\n",
      "        3.2174e-01, 3.2028e-01, 3.1883e-01, 3.1738e-01, 3.1593e-01, 3.1448e-01,\n",
      "        3.1303e-01, 3.1159e-01, 3.1015e-01, 3.0871e-01, 3.0727e-01, 3.0583e-01,\n",
      "        3.0440e-01, 3.0296e-01, 3.0153e-01, 3.0010e-01, 2.9867e-01, 2.9725e-01,\n",
      "        2.9582e-01, 2.9440e-01, 2.9298e-01, 2.9157e-01, 2.9015e-01, 2.8874e-01,\n",
      "        2.8732e-01, 2.8592e-01, 2.8451e-01, 2.8310e-01, 2.8170e-01, 2.8030e-01,\n",
      "        2.7890e-01, 2.7750e-01, 2.7611e-01, 2.7472e-01, 2.7333e-01, 2.7194e-01,\n",
      "        2.7055e-01, 2.6917e-01, 2.6779e-01, 2.6641e-01, 2.6503e-01, 2.6366e-01,\n",
      "        2.6228e-01, 2.6091e-01, 2.5955e-01, 2.5818e-01, 2.5682e-01, 2.5546e-01,\n",
      "        2.5410e-01, 2.5274e-01, 2.5139e-01, 2.5004e-01, 2.4869e-01, 2.4734e-01,\n",
      "        2.4600e-01, 2.4466e-01, 2.4332e-01, 2.4198e-01, 2.4065e-01, 2.3932e-01,\n",
      "        2.3799e-01, 2.3667e-01, 2.3534e-01, 2.3402e-01, 2.3270e-01, 2.3139e-01,\n",
      "        2.3007e-01, 2.2876e-01, 2.2746e-01, 2.2615e-01, 2.2485e-01, 2.2355e-01,\n",
      "        2.2225e-01, 2.2096e-01, 2.1966e-01, 2.1837e-01, 2.1709e-01, 2.1580e-01,\n",
      "        2.1452e-01, 2.1325e-01, 2.1197e-01, 2.1070e-01, 2.0943e-01, 2.0816e-01,\n",
      "        2.0690e-01, 2.0564e-01, 2.0438e-01, 2.0312e-01, 2.0187e-01, 2.0062e-01,\n",
      "        1.9937e-01, 1.9813e-01, 1.9689e-01, 1.9565e-01, 1.9442e-01, 1.9318e-01,\n",
      "        1.9195e-01, 1.9073e-01, 1.8950e-01, 1.8828e-01, 1.8707e-01, 1.8585e-01,\n",
      "        1.8464e-01, 1.8344e-01, 1.8223e-01, 1.8103e-01, 1.7983e-01, 1.7863e-01,\n",
      "        1.7744e-01, 1.7625e-01, 1.7507e-01, 1.7388e-01, 1.7270e-01, 1.7153e-01,\n",
      "        1.7035e-01, 1.6918e-01, 1.6802e-01, 1.6685e-01, 1.6569e-01, 1.6454e-01,\n",
      "        1.6338e-01, 1.6223e-01, 1.6108e-01, 1.5994e-01, 1.5880e-01, 1.5766e-01,\n",
      "        1.5653e-01, 1.5540e-01, 1.5427e-01, 1.5314e-01, 1.5202e-01, 1.5091e-01,\n",
      "        1.4979e-01, 1.4868e-01, 1.4757e-01, 1.4647e-01, 1.4537e-01, 1.4427e-01,\n",
      "        1.4318e-01, 1.4209e-01, 1.4100e-01, 1.3992e-01, 1.3884e-01, 1.3776e-01,\n",
      "        1.3669e-01, 1.3562e-01, 1.3456e-01, 1.3350e-01, 1.3244e-01, 1.3138e-01,\n",
      "        1.3033e-01, 1.2928e-01, 1.2824e-01, 1.2720e-01, 1.2616e-01, 1.2513e-01,\n",
      "        1.2410e-01, 1.2307e-01, 1.2205e-01, 1.2103e-01, 1.2002e-01, 1.1901e-01,\n",
      "        1.1800e-01, 1.1700e-01, 1.1600e-01, 1.1500e-01, 1.1401e-01, 1.1302e-01,\n",
      "        1.1203e-01, 1.1105e-01, 1.1008e-01, 1.0910e-01, 1.0813e-01, 1.0717e-01,\n",
      "        1.0620e-01, 1.0525e-01, 1.0429e-01, 1.0334e-01, 1.0239e-01, 1.0145e-01,\n",
      "        1.0051e-01, 9.9576e-02, 9.8644e-02, 9.7717e-02, 9.6793e-02, 9.5874e-02,\n",
      "        9.4958e-02, 9.4046e-02, 9.3138e-02, 9.2234e-02, 9.1334e-02, 9.0438e-02,\n",
      "        8.9547e-02, 8.8659e-02, 8.7775e-02, 8.6895e-02, 8.6019e-02, 8.5147e-02,\n",
      "        8.4279e-02, 8.3415e-02, 8.2555e-02, 8.1699e-02, 8.0848e-02, 8.0000e-02,\n",
      "        7.9156e-02, 7.8317e-02, 7.7482e-02, 7.6650e-02, 7.5823e-02, 7.5000e-02,\n",
      "        7.4181e-02, 7.3366e-02, 7.2556e-02, 7.1749e-02, 7.0947e-02, 7.0149e-02,\n",
      "        6.9355e-02, 6.8565e-02, 6.7780e-02, 6.6998e-02, 6.6221e-02, 6.5448e-02,\n",
      "        6.4679e-02, 6.3915e-02, 6.3154e-02, 6.2398e-02, 6.1647e-02, 6.0899e-02,\n",
      "        6.0156e-02, 5.9417e-02, 5.8682e-02, 5.7952e-02, 5.7226e-02, 5.6504e-02,\n",
      "        5.5786e-02, 5.5073e-02, 5.4364e-02, 5.3660e-02, 5.2959e-02, 5.2264e-02,\n",
      "        5.1572e-02, 5.0885e-02, 5.0202e-02, 4.9524e-02, 4.8850e-02, 4.8180e-02,\n",
      "        4.7515e-02, 4.6854e-02, 4.6197e-02, 4.5545e-02, 4.4898e-02, 4.4254e-02,\n",
      "        4.3616e-02, 4.2981e-02, 4.2351e-02, 4.1726e-02, 4.1105e-02, 4.0488e-02,\n",
      "        3.9876e-02, 3.9268e-02, 3.8665e-02, 3.8067e-02, 3.7472e-02, 3.6883e-02,\n",
      "        3.6297e-02, 3.5717e-02, 3.5141e-02, 3.4569e-02, 3.4002e-02, 3.3439e-02,\n",
      "        3.2881e-02, 3.2327e-02, 3.1778e-02, 3.1234e-02, 3.0694e-02, 3.0159e-02,\n",
      "        2.9628e-02, 2.9102e-02, 2.8580e-02, 2.8063e-02, 2.7551e-02, 2.7043e-02,\n",
      "        2.6539e-02, 2.6041e-02, 2.5547e-02, 2.5057e-02, 2.4572e-02, 2.4092e-02,\n",
      "        2.3616e-02, 2.3145e-02, 2.2679e-02, 2.2217e-02, 2.1760e-02, 2.1308e-02,\n",
      "        2.0860e-02, 2.0417e-02, 1.9978e-02, 1.9545e-02, 1.9116e-02, 1.8691e-02,\n",
      "        1.8271e-02, 1.7856e-02, 1.7446e-02, 1.7040e-02, 1.6639e-02, 1.6243e-02,\n",
      "        1.5851e-02, 1.5464e-02, 1.5082e-02, 1.4704e-02, 1.4331e-02, 1.3963e-02,\n",
      "        1.3600e-02, 1.3241e-02, 1.2887e-02, 1.2538e-02, 1.2194e-02, 1.1854e-02,\n",
      "        1.1519e-02, 1.1189e-02, 1.0863e-02, 1.0543e-02, 1.0227e-02, 9.9155e-03,\n",
      "        9.6091e-03, 9.3074e-03, 9.0105e-03, 8.7183e-03, 8.4310e-03, 8.1484e-03,\n",
      "        7.8705e-03, 7.5975e-03, 7.3293e-03, 7.0658e-03, 6.8071e-03, 6.5532e-03,\n",
      "        6.3041e-03, 6.0598e-03, 5.8203e-03, 5.5856e-03, 5.3557e-03, 5.1307e-03,\n",
      "        4.9104e-03, 4.6949e-03, 4.4842e-03, 4.2784e-03, 4.0774e-03, 3.8812e-03,\n",
      "        3.6898e-03, 3.5032e-03, 3.3214e-03, 3.1445e-03, 2.9724e-03, 2.8052e-03,\n",
      "        2.6427e-03, 2.4851e-03, 2.3323e-03, 2.1844e-03, 2.0413e-03, 1.9031e-03,\n",
      "        1.7696e-03, 1.6411e-03, 1.5173e-03, 1.3984e-03, 1.2843e-03, 1.1752e-03,\n",
      "        1.0708e-03, 9.7127e-04, 8.7660e-04, 7.8679e-04, 7.0181e-04, 6.2170e-04,\n",
      "        5.4643e-04, 4.7602e-04, 4.1046e-04, 3.4975e-04, 2.9389e-04, 2.4290e-04,\n",
      "        1.9675e-04, 1.5547e-04, 1.1904e-04, 8.7458e-05, 6.0738e-05, 3.8876e-05,\n",
      "        2.1871e-05, 9.7226e-06, 2.4330e-06, 2.4330e-09])\n",
      "range of bars tensor(2.4330e-09) tensor(1.0000)\n",
      "range of sigmas, tensor(0.0047) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                    | 0/188 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number parameters of the model: 5349376\n",
      "Model strcuture: DiT(\n",
      "  (embedding): Linear(in_features=16, out_features=256, bias=False)\n",
      "  (pos_embedding): SinousEmbedding()\n",
      "  (t_embedding): Sequential(\n",
      "    (0): SinousEmbedding()\n",
      "    (1): Linear(in_features=64, out_features=256, bias=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=256, out_features=512, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out_norm): AdaptiveLayerNorm(\n",
      "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
      "    (beta_gamma): Sequential(\n",
      "      (0): SiLU()\n",
      "      (1): Linear(in_features=256, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out_proj): Linear(in_features=256, out_features=16, bias=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.4645: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 0, MSE 0.1647, [Valid] 0.1647: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DiT. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SinousEmbedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SiLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attn. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GELU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type AdaptiveLayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "epoch 1, loss 0.1469: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 1, MSE 0.1467, [Valid] 0.1467: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:03<00:00, 11.75it/s]\n",
      "epoch 2, loss 0.1323: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 2, MSE 0.1292, [Valid] 0.1292: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 11.73it/s]\n",
      "epoch 3, loss 0.1237: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 3, MSE 0.1178, [Valid] 0.1178: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 11.75it/s]\n",
      "epoch 4, loss 0.1208: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 4, MSE 0.1176, [Valid] 0.1176: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 11.73it/s]\n",
      "epoch 5, loss 0.1151: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.50it/s]\n",
      "epoch 5, MSE 0.1079, [Valid] 0.1079: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 11.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 0.1101: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 6, MSE 0.1074, [Valid] 0.1074: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 11.74it/s]\n",
      "epoch 7, loss 0.1100: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 7, MSE 0.1047, [Valid] 0.1047: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 11.71it/s]\n",
      "epoch 8, loss 0.1048: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 8, MSE 0.1018, [Valid] 0.1018: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:03<00:00, 11.76it/s]\n",
      "epoch 9, loss 0.1005: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 9, MSE 0.0962, [Valid] 0.0962: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 11.75it/s]\n",
      "epoch 10, loss 0.0998: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 10, MSE 0.0995, [Valid] 0.0995: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:03<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11, loss 0.0987: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 11, MSE 0.0949, [Valid] 0.0949: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:03<00:00, 11.77it/s]\n",
      "epoch 12, loss 0.0957: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 12, MSE 0.0907, [Valid] 0.0907: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 11.63it/s]\n",
      "epoch 13, loss 0.0938: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:07<00:00,  1.51it/s]\n",
      "epoch 13, MSE 0.0936, [Valid] 0.0936: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:03<00:00, 11.76it/s]\n",
      "epoch 14, loss 0.0929:  48%|████████████████████████████████████████████████▍                                                   | 91/188 [01:01<01:06,  1.47it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8080/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# parent_dir = os.path.abspath('/root/DeepLearning')\n",
    "parent_dir = os.path.abspath('/home/zhh24/DeepLearning')\n",
    "# parent_dir = os.path.abspath('..')\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "print('appended',parent_dir)\n",
    "\n",
    "import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "mnist = utils.MNIST(batch_size=256)\n",
    "train_loader = mnist.train_dataloader\n",
    "valid_loader = mnist.valid_dataloader\n",
    "T=1000\n",
    "# beta1=1e-4 # variance of lowest temperature\n",
    "# betaT=6e-2 # variance of highest temperature\n",
    "eps = 8e-3\n",
    "steps=torch.linspace(0,T,steps=T+1,dtype=torch.float)\n",
    "f_t=torch.cos(((steps/T+eps)/(1.0+eps))*3.14159*0.5)**2\n",
    "betas=torch.clamp(1.0-f_t[1:]/f_t[:T],0.0,0.999)\n",
    "\n",
    "# step = torch.log(torch.tensor(betaT/beta1))/(T-1)\n",
    "# betas = beta1 * torch.exp(step*torch.arange(T,dtype=torch.float).to(device))\n",
    "# step = (betaT-beta1)/(T-1)\n",
    "# betas = torch.arange(T,dtype=torch.float,device=device) * step + beta1\n",
    "\n",
    "\n",
    "alphas = 1-betas\n",
    "alpha_bars = alphas.clone()\n",
    "for i in range(1,T):\n",
    "    alpha_bars[i] *= alpha_bars[i-1]\n",
    "\n",
    "print(alpha_bars)\n",
    "print('range of bars',alpha_bars.min(),alpha_bars.max())\n",
    "# print(alphas)\n",
    "\n",
    "sqrt = torch.sqrt\n",
    "sigmas = sqrt(betas * (1-alpha_bars / alphas)/(1-alpha_bars))\n",
    "sigmas[0] = 1\n",
    "print('range of sigmas,',sigmas.min(),sigmas.max())\n",
    "alphas = alphas.to(device)\n",
    "alpha_bars = alpha_bars.to(device)\n",
    "betas = betas.to(device)\n",
    "sigmas = sigmas.to(device)\n",
    "weights = torch.ones(T,dtype=torch.float,device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model:DiT,save_dir):\n",
    "    x = torch.randn([100,784]).to(device)\n",
    "    for t in range(T-1,-1,-1):\n",
    "        sigmaz = torch.randn_like(x)*sigmas[t]\n",
    "        if t==0:\n",
    "            sigmaz = 0\n",
    "        noise_pred = model(x,t*torch.ones(x.shape[0],dtype=torch.long,device=device))\n",
    "        x0_pred = (x - noise_pred * sqrt(1 - alpha_bars[t])) / sqrt(alpha_bars[t]).clamp(-1,1)\n",
    "        mean_pred = (sqrt(alphas[t]) * (1-alpha_bars[t]/alphas[t]) * x + sqrt(alpha_bars[t]/alphas[t]) * (1-alphas[t]) * x0_pred) / (1-alpha_bars[t])\n",
    "        x = mean_pred + sigmaz\n",
    "        # x = torch.clamp(x,0,1)\n",
    "    grid = torchvision.utils.make_grid(post_process(x).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize(model,save_dir):\n",
    "    interval = (T-1) // 20\n",
    "    x = torch.randn([10,784]).to(device)\n",
    "    x_history = []\n",
    "    for t in range(T-1,-1,-1):\n",
    "        sigmaz = torch.randn_like(x)*((betas[t])**0.5).to(device)\n",
    "        if t==0:\n",
    "            sigmaz = 0\n",
    "        x = (x-(1-alphas[t])/(sqrt(1-alpha_bars[t]))*model(x,t*torch.ones(x.shape[0],dtype=torch.long,device=device)))/(sqrt(alphas[t]))+sigmaz\n",
    "        # x = torch.clamp(x,0,1)\n",
    "        x_history.append(x)\n",
    "    # print('cat.shape',torch.cat(x_history,dim=0).shape)\n",
    "    grid = torchvision.utils.make_grid(post_process(torch.stack(x_history,dim=0)[::interval,...]).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "    print('Saved visualize to',os.path.abspath(save_dir))\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_denoise(model,save_dir):\n",
    "    # get 10 images from the dataset\n",
    "    x,_ = next(iter(valid_loader))\n",
    "    x = x[:20,...].reshape(20,784).to(device)\n",
    "    x = pre_process(x)\n",
    "    t = torch.tensor([i * T // 20 for i in range(20)],dtype=torch.long,device=device)\n",
    "    noise = torch.randn_like(x).reshape(-1,784)\n",
    "    v1 = (sqrt(alpha_bars[t]).reshape(-1,1)*x).reshape(-1,784)\n",
    "    v2 = sqrt(1-alpha_bars[t]).reshape(-1,1)*noise\n",
    "    x_corr = v1+v2\n",
    "    est = model(x_corr,t)\n",
    "    x_rec = (x_corr - sqrt(1-alpha_bars[t]).reshape(-1,1)*est)/(sqrt(alpha_bars[t])).reshape(-1,1)\n",
    "    grid_orig = torchvision.utils.make_grid(post_process(x).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    grid_corr = torchvision.utils.make_grid(post_process(x_corr).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    grid_rec = torchvision.utils.make_grid(post_process(x_rec).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    # add noise level infomation to the image\n",
    "    noise_level = (1-alpha_bars[t]).reshape(-1).tolist()\n",
    "    ori_mse = noise.pow(2).mean(dim=1).reshape(-1).tolist()\n",
    "    mse = ((est-noise)**2).mean(dim=1).reshape(-1).tolist()\n",
    "    print(noise_level)\n",
    "    print(ori_mse)\n",
    "    print(mse)\n",
    "    grid = torch.cat([grid_orig,grid_corr,grid_rec],dim=1)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "    print('Saved denoise to',os.path.abspath(save_dir))\n",
    "\n",
    "def plot_loss(losses,save_dir):\n",
    "    losses_vals, t_vals = zip(*losses)\n",
    "    losses_vals = torch.cat(losses_vals,dim=0)\n",
    "    t_vals = torch.cat(t_vals,dim=0)\n",
    "\n",
    "    results = []\n",
    "    for t in range(T):\n",
    "        this_t = abs(t_vals.float()-float(t))<0.5\n",
    "        results.append(torch.sum(torch.where(this_t,losses_vals,torch.tensor(0.,device=device))).item() / (torch.sum(this_t.float())+1e-3).item())\n",
    "    plt.plot(results)\n",
    "    plt.ylim(0,max(results)* 1.2)\n",
    "    plt.savefig(save_dir)\n",
    "    plt.close()\n",
    "    # weights = (torch.tensor(results,device=device)) # weights\n",
    "    weights = torch.ones(T,dtype=torch.float,device=device)\n",
    "    # weights[:10]=0\n",
    "    # weights[10:80] /= 100\n",
    "    return weights\n",
    "\n",
    "def pre_process(x):\n",
    "    return x*2-1\n",
    "\n",
    "def post_process(x):\n",
    "    return (x+1)/2\n",
    "\n",
    "def train(epochs,model:DiT,optimizer,eval_interval=5):\n",
    "    global weights\n",
    "    for epoch in range(epochs):\n",
    "        # print('weights normalized:',weights/weights.sum())\n",
    "        all_ts = torch.distributions.Categorical(weights).sample((50000,))\n",
    "        cnt = 0\n",
    "        model.train()\n",
    "        with tqdm(train_loader) as bar:\n",
    "            losses = []\n",
    "            for x,_ in bar:\n",
    "                cnt += x.shape[0]\n",
    "                x = pre_process(x.to(device))\n",
    "                epss = torch.randn_like(x).reshape(-1,784).to(device)\n",
    "                # ts = torch.randint(0,T,(x.shape[0],),device=device,dtype=torch.long)\n",
    "                ts = all_ts[cnt-x.shape[0]:cnt]\n",
    "                alpha_tbars = alpha_bars[ts]\n",
    "                value = (sqrt(alpha_tbars).reshape(-1,1,1,1)*x).reshape(-1,784)+sqrt(1-alpha_tbars).reshape(-1,1)*epss\n",
    "                out = model(value,ts) # [batch,784]\n",
    "                # loss = ((epss-out).pow(2).mean(dim=-1) * (betas[ts])/(2*alphas[ts]*(1-alpha_tbars))).sum(dim=0)\n",
    "                loss = ((epss-out).pow(2).mean(dim=-1)).mean(dim=0)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "                bar.set_description('epoch {}, loss {:.4f}'.format(epoch,sum(losses)/len(losses)))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(valid_loader) as bar:\n",
    "                mses = []\n",
    "                losses = []\n",
    "                losses_for_t = []\n",
    "                for x,_ in bar:\n",
    "                    x = pre_process(x.to(device))\n",
    "                    epss = torch.randn_like(x).reshape(-1,784).to(device)\n",
    "                    ts = torch.randint(0,T,(x.shape[0],),device=device,dtype=torch.long)\n",
    "                    # print(ts)\n",
    "                    alpha_tbars = alpha_bars[ts]\n",
    "                    value = (sqrt(alpha_tbars).reshape(-1,1,1,1)*x).reshape(-1,784)+sqrt(1-alpha_tbars).reshape(-1,1)*epss\n",
    "                    out = model(value,ts)\n",
    "                    mse = F.mse_loss(epss,out)\n",
    "                    mses.append(mse.item())\n",
    "                    loss = ((epss-out).pow(2).mean(dim=-1))\n",
    "                    # loss = (epss-out).pow(2).mean(dim=-1)\n",
    "                    losses_for_t.append((loss.clone().detach(),ts))\n",
    "                    loss = (loss).mean(dim=0)\n",
    "                    losses.append(loss.item())\n",
    "                    bar.set_description('epoch {}, MSE {:.4f}, [Valid] {:.4f}'.format(epoch,sum(mses)/len(mses),sum(losses)/len(losses)))\n",
    "                    \n",
    "        if epoch % eval_interval == 0:\n",
    "            visualize(model,save_dir=os.path.join('./samples',f'diffuse_epoch_{epoch}.png'))\n",
    "            sample(model,save_dir=os.path.join('./samples',f'sample_epoch_{epoch}.png'))\n",
    "            # visualize_denoise(model,save_dir=os.path.join('./samples',f'denoise_epoch_{epoch}.png'))\n",
    "            weights = plot_loss(losses_for_t,save_dir=os.path.join('./samples',f'loss_epoch_{epoch}.png'))\n",
    "            torch.save(model,os.path.join('./samples',f'epoch_{epoch}.pt'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = DiT().to(device)\n",
    "    print('Number parameters of the model:', sum(p.numel() for p in model.parameters()))\n",
    "    print('Model strcuture:',model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "    os.makedirs('./samples',exist_ok=True)\n",
    "    # sample(model,save_dir=os.path.join('./samples',f'init.png'))\n",
    "    # visualize(model,save_dir=os.path.join('./samples',f'init_visualize.png'))\n",
    "    train(200,model,optimizer,eval_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
