{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 19 11:06:29 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              40W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/bin/python\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!which python | grep DYY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class SinousEmbedding(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        assert dim%2==0,NotImplementedError()\n",
    "        self.angles = (10000.**(-2/dim))**torch.arange(1,dim//2+1,1,dtype=torch.float).cuda()\n",
    "        self.angles.requires_grad_(False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        angles = torch.einsum('m,i->im',self.angles,x.float())\n",
    "        return torch.cat((torch.sin(angles),torch.cos(angles)),dim=1)\n",
    "    \n",
    "class Attn(nn.Module):\n",
    "    def __init__(self,dim,head):\n",
    "        super().__init__()\n",
    "        assert dim%head==0,NotImplementedError()\n",
    "        self.head = head\n",
    "        self.head_dim = dim // head\n",
    "        self.Q = nn.Linear(dim,dim)\n",
    "        self.K = nn.Linear(dim,dim)\n",
    "        self.V = nn.Linear(dim,dim)\n",
    "        self.out_proj = nn.Linear(dim,dim)\n",
    "        self.apply_init()\n",
    "\n",
    "    def apply_init(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,query,context):\n",
    "        q = self.Q(query).reshape(*query.shape[:2],self.head,self.head_dim)\n",
    "        k = self.K(context).reshape(*context.shape[:2],self.head,self.head_dim)\n",
    "        v = self.V(context).reshape(*context.shape[:2],self.head,self.head_dim)\n",
    "        score = torch.einsum('bihd,bjhd->bijh',q,k) / (self.head_dim**0.5)\n",
    "        score = F.softmax(score,dim=2)\n",
    "        out = torch.einsum('bijh,bjhd->bihd',score,v).reshape_as(query)\n",
    "        return self.out_proj(out)\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return 0.5 * x * (1 + torch.tanh(0.7978845608 * (x + 0.044715 * x**3)))\n",
    "    \n",
    "class SiLU(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "class AdaptiveLayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim,elementwise_affine=False)\n",
    "        self.beta_gamma = nn.Sequential(\n",
    "            SiLU(),\n",
    "            nn.Linear(dim,2*dim)\n",
    "        )\n",
    "        self.beta_gamma[-1].weight.data.zero_()\n",
    "        self.beta_gamma[-1].bias.data.zero_()\n",
    "\n",
    "    def forward(self, x, conditioned):\n",
    "        x = self.norm(x)\n",
    "        beta,gamma = self.beta_gamma(conditioned).unsqueeze(1).chunk(2,dim=-1)\n",
    "        return x * (1+gamma) + beta\n",
    "\n",
    "class Layer(nn.Module):\n",
    "\n",
    "    def __init__(self,dim,head):\n",
    "        super().__init__()\n",
    "        self.attn = Attn(dim,head)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            GELU(),\n",
    "            nn.Linear(dim,dim)\n",
    "        )\n",
    "        self.condition_mlp = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            SiLU(),\n",
    "            nn.Linear(dim,2*dim)\n",
    "        )\n",
    "        self.norm1 = AdaptiveLayerNorm(dim)\n",
    "        self.norm2 = AdaptiveLayerNorm(dim)\n",
    "        self.apply_init()\n",
    "\n",
    "    def apply_init(self):\n",
    "        self.condition_mlp[2].weight.data.zero_()\n",
    "        self.condition_mlp[2].bias.data.zero_()\n",
    "\n",
    "    def forward(self,x,condition):\n",
    "        alpha1,alpha2 = self.condition_mlp(condition).unsqueeze(1).chunk(2,dim=-1)\n",
    "\n",
    "        # first half\n",
    "        xc = x.clone()\n",
    "        x = self.norm1(x,condition)\n",
    "        x = self.attn(x,x)\n",
    "        x = x * alpha1\n",
    "        x = x + xc\n",
    "\n",
    "        # second half\n",
    "        xc = x.clone()\n",
    "        x = self.norm2(x,condition)\n",
    "        x = self.mlp(x)\n",
    "        x = x * alpha2\n",
    "        x = x + xc\n",
    "\n",
    "        return x\n",
    "\n",
    "class DiT(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 patch_size=4,\n",
    "                 hidden_dim=128,\n",
    "                 num_layers=3,\n",
    "                 image_size = 28*28,\n",
    "                 num_heads=4\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.num_patches = image_size // (patch_size * patch_size)\n",
    "        self.embedding = nn.Linear(patch_size * patch_size, hidden_dim)\n",
    "        self.pos_embedding = SinousEmbedding(hidden_dim)\n",
    "        self.t_embedding = nn.Sequential(\n",
    "            SinousEmbedding(128),\n",
    "            nn.Linear(128,hidden_dim),\n",
    "            SiLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim)\n",
    "        )\n",
    "        self.patch_size = patch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.layers = nn.ModuleList([Layer(hidden_dim,num_heads) for _ in range(num_layers)])\n",
    "        self.out_norm = AdaptiveLayerNorm(hidden_dim)\n",
    "        self.out_proj = nn.Linear(hidden_dim, patch_size * patch_size)\n",
    "        self.apply_init()\n",
    "\n",
    "    def apply_init(self):\n",
    "        self.out_proj.weight.data.zero_()\n",
    "        self.out_proj.bias.data.zero_()\n",
    "        # pass\n",
    "\n",
    "    def first(self,x,t):\n",
    "        t_embed = self.t_embedding(t)\n",
    "        # patchify the image x\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size) # [B, 1, H/4, W/4, 4, 4]\n",
    "        x_embed = self.embedding(x.reshape(x.shape[0],-1,self.patch_size*self.patch_size))\n",
    "        x_embed += self.pos_embedding(torch.arange(x_embed.shape[1],device=device))\n",
    "        data = torch.cat((x_embed,t_embed.unsqueeze(1)),dim=1)\n",
    "        # print('position embedding range:',pos_embed.min(),pos_embed.max(),pos_embed.std())\n",
    "        # print('x range:',data.min(),data.max(),data.std())\n",
    "        return data, t_embed\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        x = x.reshape(x.shape[0],1,28,28)\n",
    "        inputs, conditioned = self.first(x,t)\n",
    "        for i,ly in enumerate(self.layers):\n",
    "            inputs = ly(inputs,conditioned)\n",
    "            # print('layer',i,'input.range:',inputs.min(),inputs.max())\n",
    "        # remove t token\n",
    "        inputs = inputs[:,:-1,:]\n",
    "        inputs = self.out_proj(self.out_norm(inputs,conditioned))\n",
    "        # patchify the image x\n",
    "        length = 28 // self.patch_size\n",
    "        inputs = inputs.reshape(inputs.shape[0],length,length,self.patch_size,self.patch_size).permute(0,3,4,1,2).reshape(inputs.shape[0],self.patch_size*self.patch_size,length*length)\n",
    "        x = F.fold(inputs, (28,28), self.patch_size, stride=self.patch_size)\n",
    "        return x.reshape(x.shape[0],-1)\n",
    "\n",
    "# model = DiT().to(device)\n",
    "# x = torch.randn(7,1,28,28).to(device)\n",
    "# t = torch.randint(0,10,(7,)).to(device)\n",
    "# model(x,t).shape\n",
    "\n",
    "# img = torch.arange(12*12).reshape(1,1,12,12).float()\n",
    "# img\n",
    "# # separate to 7x7 4x4 patches\n",
    "# patch_size = 4\n",
    "# p = img.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "# good = p.squeeze(1).permute(0,3,4,1,2)\n",
    "# good.shape\n",
    "# # change patch back to 28x28\n",
    "\n",
    "# back = F.fold(good.reshape(img.shape[0],16,9), (12,12), 4, stride=4)\n",
    "# back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended /home/zhh24/DeepLearning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                    | 0/188 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.9990e-01, 9.9978e-01, 9.9964e-01, 9.9948e-01, 9.9930e-01, 9.9910e-01,\n",
      "        9.9888e-01, 9.9864e-01, 9.9838e-01, 9.9811e-01, 9.9781e-01, 9.9749e-01,\n",
      "        9.9715e-01, 9.9679e-01, 9.9641e-01, 9.9602e-01, 9.9560e-01, 9.9516e-01,\n",
      "        9.9471e-01, 9.9423e-01, 9.9374e-01, 9.9322e-01, 9.9269e-01, 9.9213e-01,\n",
      "        9.9156e-01, 9.9096e-01, 9.9035e-01, 9.8972e-01, 9.8907e-01, 9.8840e-01,\n",
      "        9.8771e-01, 9.8700e-01, 9.8627e-01, 9.8553e-01, 9.8476e-01, 9.8398e-01,\n",
      "        9.8317e-01, 9.8235e-01, 9.8151e-01, 9.8065e-01, 9.7977e-01, 9.7887e-01,\n",
      "        9.7795e-01, 9.7702e-01, 9.7606e-01, 9.7509e-01, 9.7410e-01, 9.7309e-01,\n",
      "        9.7206e-01, 9.7102e-01, 9.6995e-01, 9.6887e-01, 9.6777e-01, 9.6665e-01,\n",
      "        9.6551e-01, 9.6436e-01, 9.6319e-01, 9.6200e-01, 9.6079e-01, 9.5956e-01,\n",
      "        9.5832e-01, 9.5706e-01, 9.5578e-01, 9.5449e-01, 9.5318e-01, 9.5185e-01,\n",
      "        9.5050e-01, 9.4914e-01, 9.4776e-01, 9.4636e-01, 9.4494e-01, 9.4351e-01,\n",
      "        9.4207e-01, 9.4060e-01, 9.3912e-01, 9.3762e-01, 9.3611e-01, 9.3458e-01,\n",
      "        9.3304e-01, 9.3147e-01, 9.2990e-01, 9.2830e-01, 9.2669e-01, 9.2507e-01,\n",
      "        9.2343e-01, 9.2177e-01, 9.2010e-01, 9.1841e-01, 9.1671e-01, 9.1500e-01,\n",
      "        9.1326e-01, 9.1152e-01, 9.0976e-01, 9.0798e-01, 9.0619e-01, 9.0438e-01,\n",
      "        9.0256e-01, 9.0073e-01, 8.9888e-01, 8.9702e-01, 8.9514e-01, 8.9325e-01,\n",
      "        8.9135e-01, 8.8943e-01, 8.8750e-01, 8.8555e-01, 8.8359e-01, 8.8162e-01,\n",
      "        8.7964e-01, 8.7764e-01, 8.7563e-01, 8.7360e-01, 8.7157e-01, 8.6952e-01,\n",
      "        8.6746e-01, 8.6538e-01, 8.6330e-01, 8.6120e-01, 8.5909e-01, 8.5697e-01,\n",
      "        8.5483e-01, 8.5269e-01, 8.5053e-01, 8.4836e-01, 8.4618e-01, 8.4399e-01,\n",
      "        8.4179e-01, 8.3957e-01, 8.3735e-01, 8.3511e-01, 8.3287e-01, 8.3061e-01,\n",
      "        8.2834e-01, 8.2606e-01, 8.2378e-01, 8.2148e-01, 8.1917e-01, 8.1685e-01,\n",
      "        8.1453e-01, 8.1219e-01, 8.0984e-01, 8.0749e-01, 8.0512e-01, 8.0275e-01,\n",
      "        8.0037e-01, 7.9797e-01, 7.9557e-01, 7.9316e-01, 7.9075e-01, 7.8832e-01,\n",
      "        7.8589e-01, 7.8344e-01, 7.8099e-01, 7.7854e-01, 7.7607e-01, 7.7360e-01,\n",
      "        7.7111e-01, 7.6863e-01, 7.6613e-01, 7.6363e-01, 7.6112e-01, 7.5860e-01,\n",
      "        7.5608e-01, 7.5354e-01, 7.5101e-01, 7.4846e-01, 7.4591e-01, 7.4336e-01,\n",
      "        7.4080e-01, 7.3823e-01, 7.3565e-01, 7.3308e-01, 7.3049e-01, 7.2790e-01,\n",
      "        7.2530e-01, 7.2270e-01, 7.2010e-01, 7.1749e-01, 7.1487e-01, 7.1225e-01,\n",
      "        7.0963e-01, 7.0700e-01, 7.0436e-01, 7.0172e-01, 6.9908e-01, 6.9644e-01,\n",
      "        6.9379e-01, 6.9113e-01, 6.8847e-01, 6.8581e-01, 6.8315e-01, 6.8048e-01,\n",
      "        6.7781e-01, 6.7514e-01, 6.7246e-01, 6.6978e-01, 6.6710e-01, 6.6441e-01,\n",
      "        6.6173e-01, 6.5904e-01, 6.5635e-01, 6.5365e-01, 6.5096e-01, 6.4826e-01,\n",
      "        6.4556e-01, 6.4286e-01, 6.4016e-01, 6.3745e-01, 6.3475e-01, 6.3204e-01,\n",
      "        6.2934e-01, 6.2663e-01, 6.2392e-01, 6.2121e-01, 6.1850e-01, 6.1579e-01,\n",
      "        6.1308e-01, 6.1037e-01, 6.0765e-01, 6.0494e-01, 6.0223e-01, 5.9952e-01,\n",
      "        5.9681e-01, 5.9410e-01, 5.9139e-01, 5.8868e-01, 5.8597e-01, 5.8326e-01,\n",
      "        5.8055e-01, 5.7785e-01, 5.7514e-01, 5.7244e-01, 5.6974e-01, 5.6703e-01,\n",
      "        5.6433e-01, 5.6164e-01, 5.5894e-01, 5.5624e-01, 5.5355e-01, 5.5086e-01,\n",
      "        5.4817e-01, 5.4549e-01, 5.4280e-01, 5.4012e-01, 5.3744e-01, 5.3476e-01,\n",
      "        5.3209e-01, 5.2942e-01, 5.2675e-01, 5.2409e-01, 5.2142e-01, 5.1876e-01,\n",
      "        5.1611e-01, 5.1346e-01, 5.1081e-01, 5.0816e-01, 5.0552e-01, 5.0288e-01,\n",
      "        5.0024e-01, 4.9761e-01, 4.9499e-01, 4.9236e-01, 4.8974e-01, 4.8713e-01,\n",
      "        4.8452e-01, 4.8191e-01, 4.7931e-01, 4.7671e-01, 4.7412e-01, 4.7153e-01,\n",
      "        4.6895e-01, 4.6637e-01, 4.6380e-01, 4.6123e-01, 4.5867e-01, 4.5611e-01,\n",
      "        4.5355e-01, 4.5101e-01, 4.4846e-01, 4.4593e-01, 4.4340e-01, 4.4087e-01,\n",
      "        4.3835e-01, 4.3583e-01, 4.3332e-01, 4.3082e-01, 4.2832e-01, 4.2583e-01,\n",
      "        4.2335e-01, 4.2087e-01, 4.1839e-01, 4.1593e-01, 4.1347e-01, 4.1101e-01,\n",
      "        4.0856e-01, 4.0612e-01, 4.0369e-01, 4.0126e-01, 3.9883e-01, 3.9642e-01,\n",
      "        3.9401e-01, 3.9161e-01, 3.8921e-01, 3.8683e-01, 3.8444e-01, 3.8207e-01,\n",
      "        3.7970e-01, 3.7734e-01, 3.7499e-01, 3.7264e-01, 3.7031e-01, 3.6798e-01,\n",
      "        3.6565e-01, 3.6334e-01, 3.6103e-01, 3.5872e-01, 3.5643e-01, 3.5414e-01,\n",
      "        3.5187e-01, 3.4959e-01, 3.4733e-01, 3.4508e-01, 3.4283e-01, 3.4059e-01,\n",
      "        3.3836e-01, 3.3613e-01, 3.3391e-01, 3.3171e-01, 3.2951e-01, 3.2731e-01,\n",
      "        3.2513e-01, 3.2295e-01, 3.2078e-01, 3.1862e-01, 3.1647e-01, 3.1433e-01,\n",
      "        3.1219e-01, 3.1007e-01, 3.0795e-01, 3.0584e-01, 3.0374e-01, 3.0164e-01,\n",
      "        2.9956e-01, 2.9748e-01, 2.9541e-01, 2.9335e-01, 2.9130e-01, 2.8926e-01,\n",
      "        2.8722e-01, 2.8520e-01, 2.8318e-01, 2.8117e-01, 2.7917e-01, 2.7718e-01,\n",
      "        2.7520e-01, 2.7323e-01, 2.7126e-01, 2.6931e-01, 2.6736e-01, 2.6542e-01,\n",
      "        2.6349e-01, 2.6157e-01, 2.5966e-01, 2.5775e-01, 2.5586e-01, 2.5397e-01,\n",
      "        2.5210e-01, 2.5023e-01, 2.4837e-01, 2.4652e-01, 2.4468e-01, 2.4284e-01,\n",
      "        2.4102e-01, 2.3920e-01, 2.3740e-01, 2.3560e-01, 2.3381e-01, 2.3203e-01,\n",
      "        2.3026e-01, 2.2850e-01, 2.2675e-01, 2.2501e-01, 2.2327e-01, 2.2155e-01,\n",
      "        2.1983e-01, 2.1812e-01, 2.1642e-01, 2.1473e-01, 2.1305e-01, 2.1138e-01,\n",
      "        2.0972e-01, 2.0806e-01, 2.0642e-01, 2.0478e-01, 2.0315e-01, 2.0153e-01,\n",
      "        1.9992e-01, 1.9832e-01, 1.9673e-01, 1.9515e-01, 1.9357e-01, 1.9201e-01,\n",
      "        1.9045e-01, 1.8890e-01, 1.8736e-01, 1.8583e-01, 1.8431e-01, 1.8280e-01,\n",
      "        1.8129e-01, 1.7980e-01, 1.7831e-01, 1.7683e-01, 1.7537e-01, 1.7391e-01,\n",
      "        1.7245e-01, 1.7101e-01, 1.6958e-01, 1.6815e-01, 1.6673e-01, 1.6533e-01,\n",
      "        1.6393e-01, 1.6254e-01, 1.6115e-01, 1.5978e-01, 1.5841e-01, 1.5706e-01,\n",
      "        1.5571e-01, 1.5437e-01, 1.5304e-01, 1.5171e-01, 1.5040e-01, 1.4909e-01,\n",
      "        1.4779e-01, 1.4650e-01, 1.4522e-01, 1.4395e-01, 1.4269e-01, 1.4143e-01,\n",
      "        1.4018e-01, 1.3894e-01, 1.3771e-01, 1.3649e-01, 1.3527e-01, 1.3406e-01,\n",
      "        1.3286e-01, 1.3167e-01, 1.3049e-01, 1.2932e-01, 1.2815e-01, 1.2699e-01,\n",
      "        1.2584e-01, 1.2470e-01, 1.2356e-01, 1.2243e-01, 1.2131e-01, 1.2020e-01,\n",
      "        1.1910e-01, 1.1800e-01, 1.1691e-01, 1.1583e-01, 1.1476e-01, 1.1369e-01,\n",
      "        1.1264e-01, 1.1159e-01, 1.1054e-01, 1.0951e-01, 1.0848e-01, 1.0746e-01,\n",
      "        1.0645e-01, 1.0544e-01, 1.0445e-01, 1.0346e-01, 1.0247e-01, 1.0150e-01,\n",
      "        1.0053e-01, 9.9567e-02, 9.8613e-02, 9.7667e-02, 9.6727e-02, 9.5794e-02,\n",
      "        9.4869e-02, 9.3950e-02, 9.3039e-02, 9.2134e-02, 9.1237e-02, 9.0346e-02,\n",
      "        8.9463e-02, 8.8586e-02, 8.7716e-02, 8.6853e-02, 8.5996e-02, 8.5146e-02,\n",
      "        8.4303e-02, 8.3467e-02, 8.2637e-02, 8.1814e-02, 8.0998e-02, 8.0188e-02,\n",
      "        7.9384e-02, 7.8587e-02, 7.7797e-02, 7.7012e-02, 7.6235e-02, 7.5463e-02,\n",
      "        7.4698e-02, 7.3939e-02, 7.3186e-02, 7.2440e-02, 7.1700e-02, 7.0966e-02,\n",
      "        7.0238e-02, 6.9516e-02, 6.8800e-02, 6.8090e-02, 6.7386e-02, 6.6688e-02,\n",
      "        6.5996e-02, 6.5309e-02, 6.4629e-02, 6.3954e-02, 6.3285e-02, 6.2622e-02,\n",
      "        6.1965e-02, 6.1313e-02, 6.0667e-02, 6.0026e-02, 5.9391e-02, 5.8762e-02,\n",
      "        5.8138e-02, 5.7520e-02, 5.6907e-02, 5.6299e-02, 5.5697e-02, 5.5100e-02,\n",
      "        5.4508e-02, 5.3922e-02, 5.3341e-02, 5.2765e-02, 5.2194e-02, 5.1628e-02,\n",
      "        5.1068e-02, 5.0513e-02, 4.9962e-02, 4.9417e-02, 4.8876e-02, 4.8341e-02,\n",
      "        4.7810e-02, 4.7284e-02, 4.6764e-02, 4.6247e-02, 4.5736e-02, 4.5230e-02,\n",
      "        4.4728e-02, 4.4231e-02, 4.3738e-02, 4.3250e-02, 4.2767e-02, 4.2288e-02,\n",
      "        4.1814e-02, 4.1344e-02, 4.0879e-02, 4.0418e-02, 3.9961e-02, 3.9509e-02,\n",
      "        3.9061e-02, 3.8618e-02, 3.8178e-02, 3.7743e-02, 3.7312e-02, 3.6886e-02,\n",
      "        3.6463e-02, 3.6045e-02, 3.5631e-02, 3.5220e-02, 3.4814e-02, 3.4412e-02,\n",
      "        3.4014e-02, 3.3619e-02, 3.3229e-02, 3.2842e-02, 3.2460e-02, 3.2081e-02,\n",
      "        3.1705e-02, 3.1334e-02, 3.0966e-02, 3.0603e-02, 3.0242e-02, 2.9886e-02,\n",
      "        2.9533e-02, 2.9183e-02, 2.8837e-02, 2.8495e-02, 2.8156e-02, 2.7821e-02,\n",
      "        2.7489e-02, 2.7160e-02, 2.6835e-02, 2.6513e-02, 2.6195e-02, 2.5879e-02,\n",
      "        2.5567e-02, 2.5259e-02, 2.4953e-02, 2.4651e-02, 2.4352e-02, 2.4056e-02,\n",
      "        2.3763e-02, 2.3474e-02, 2.3187e-02, 2.2903e-02, 2.2623e-02, 2.2345e-02,\n",
      "        2.2071e-02, 2.1799e-02, 2.1530e-02, 2.1264e-02, 2.1001e-02, 2.0741e-02,\n",
      "        2.0484e-02, 2.0229e-02, 1.9977e-02, 1.9728e-02, 1.9482e-02, 1.9238e-02,\n",
      "        1.8997e-02, 1.8758e-02, 1.8523e-02, 1.8289e-02, 1.8059e-02, 1.7831e-02,\n",
      "        1.7605e-02, 1.7382e-02, 1.7161e-02, 1.6943e-02, 1.6728e-02, 1.6514e-02,\n",
      "        1.6304e-02, 1.6095e-02, 1.5889e-02, 1.5685e-02, 1.5484e-02, 1.5284e-02,\n",
      "        1.5087e-02, 1.4893e-02, 1.4700e-02, 1.4510e-02, 1.4321e-02, 1.4135e-02,\n",
      "        1.3952e-02, 1.3770e-02, 1.3590e-02, 1.3413e-02, 1.3237e-02, 1.3064e-02,\n",
      "        1.2892e-02, 1.2723e-02, 1.2555e-02, 1.2389e-02, 1.2226e-02, 1.2064e-02,\n",
      "        1.1904e-02, 1.1746e-02, 1.1590e-02, 1.1436e-02, 1.1284e-02, 1.1133e-02,\n",
      "        1.0984e-02, 1.0837e-02, 1.0692e-02, 1.0548e-02, 1.0407e-02, 1.0266e-02,\n",
      "        1.0128e-02, 9.9911e-03, 9.8560e-03, 9.7225e-03, 9.5906e-03, 9.4603e-03,\n",
      "        9.3316e-03, 9.2044e-03, 9.0788e-03, 8.9548e-03, 8.8322e-03, 8.7112e-03,\n",
      "        8.5916e-03, 8.4735e-03, 8.3569e-03, 8.2417e-03, 8.1279e-03, 8.0155e-03,\n",
      "        7.9046e-03, 7.7950e-03, 7.6867e-03, 7.5799e-03, 7.4743e-03, 7.3701e-03,\n",
      "        7.2672e-03, 7.1655e-03, 7.0652e-03, 6.9661e-03, 6.8683e-03, 6.7717e-03,\n",
      "        6.6763e-03, 6.5822e-03, 6.4892e-03, 6.3974e-03, 6.3068e-03, 6.2173e-03,\n",
      "        6.1290e-03, 6.0419e-03, 5.9558e-03, 5.8709e-03, 5.7870e-03, 5.7042e-03,\n",
      "        5.6225e-03, 5.5419e-03, 5.4623e-03, 5.3837e-03, 5.3062e-03, 5.2297e-03,\n",
      "        5.1541e-03, 5.0796e-03, 5.0060e-03, 4.9334e-03, 4.8618e-03, 4.7911e-03,\n",
      "        4.7213e-03, 4.6525e-03, 4.5845e-03, 4.5175e-03, 4.4514e-03, 4.3861e-03,\n",
      "        4.3217e-03, 4.2582e-03, 4.1955e-03, 4.1336e-03, 4.0726e-03, 4.0124e-03,\n",
      "        3.9530e-03, 3.8945e-03, 3.8367e-03, 3.7796e-03, 3.7234e-03, 3.6679e-03,\n",
      "        3.6132e-03, 3.5592e-03, 3.5060e-03, 3.4534e-03, 3.4016e-03, 3.3506e-03,\n",
      "        3.3002e-03, 3.2505e-03, 3.2014e-03, 3.1531e-03, 3.1054e-03, 3.0584e-03,\n",
      "        3.0120e-03, 2.9663e-03, 2.9212e-03, 2.8768e-03, 2.8329e-03, 2.7897e-03,\n",
      "        2.7471e-03, 2.7051e-03, 2.6636e-03, 2.6228e-03, 2.5825e-03, 2.5428e-03,\n",
      "        2.5036e-03, 2.4650e-03, 2.4270e-03, 2.3894e-03, 2.3525e-03, 2.3160e-03,\n",
      "        2.2801e-03, 2.2446e-03, 2.2097e-03, 2.1753e-03, 2.1414e-03, 2.1079e-03,\n",
      "        2.0750e-03, 2.0425e-03, 2.0104e-03, 1.9789e-03, 1.9478e-03, 1.9171e-03,\n",
      "        1.8869e-03, 1.8572e-03, 1.8278e-03, 1.7989e-03, 1.7704e-03, 1.7423e-03,\n",
      "        1.7147e-03, 1.6874e-03, 1.6606e-03, 1.6341e-03, 1.6080e-03, 1.5823e-03,\n",
      "        1.5570e-03, 1.5321e-03, 1.5075e-03, 1.4833e-03, 1.4595e-03, 1.4360e-03,\n",
      "        1.4128e-03, 1.3900e-03, 1.3676e-03, 1.3455e-03, 1.3237e-03, 1.3022e-03,\n",
      "        1.2811e-03, 1.2602e-03, 1.2397e-03, 1.2195e-03, 1.1996e-03, 1.1800e-03,\n",
      "        1.1607e-03, 1.1417e-03, 1.1230e-03, 1.1046e-03, 1.0864e-03, 1.0686e-03,\n",
      "        1.0509e-03, 1.0336e-03, 1.0165e-03, 9.9974e-04, 9.8319e-04, 9.6689e-04,\n",
      "        9.5085e-04, 9.3505e-04, 9.1950e-04, 9.0419e-04, 8.8911e-04, 8.7427e-04,\n",
      "        8.5966e-04, 8.4527e-04, 8.3111e-04, 8.1717e-04, 8.0345e-04, 7.8994e-04,\n",
      "        7.7664e-04, 7.6355e-04, 7.5067e-04, 7.3799e-04, 7.2551e-04, 7.1322e-04,\n",
      "        7.0113e-04, 6.8923e-04, 6.7752e-04, 6.6600e-04, 6.5465e-04, 6.4349e-04,\n",
      "        6.3250e-04, 6.2169e-04, 6.1106e-04, 6.0059e-04, 5.9029e-04, 5.8015e-04,\n",
      "        5.7018e-04, 5.6036e-04, 5.5071e-04, 5.4121e-04, 5.3186e-04, 5.2266e-04,\n",
      "        5.1362e-04, 5.0472e-04, 4.9596e-04, 4.8734e-04, 4.7887e-04, 4.7053e-04,\n",
      "        4.6233e-04, 4.5426e-04, 4.4633e-04, 4.3852e-04, 4.3084e-04, 4.2329e-04,\n",
      "        4.1586e-04, 4.0855e-04, 4.0137e-04, 3.9430e-04, 3.8735e-04, 3.8051e-04,\n",
      "        3.7379e-04, 3.6718e-04, 3.6067e-04, 3.5428e-04, 3.4799e-04, 3.4181e-04,\n",
      "        3.3573e-04, 3.2975e-04, 3.2387e-04, 3.1809e-04, 3.1240e-04, 3.0682e-04,\n",
      "        3.0132e-04, 2.9592e-04, 2.9061e-04, 2.8539e-04, 2.8025e-04, 2.7521e-04,\n",
      "        2.7024e-04, 2.6537e-04, 2.6057e-04, 2.5586e-04, 2.5123e-04, 2.4667e-04,\n",
      "        2.4220e-04, 2.3780e-04, 2.3347e-04, 2.2922e-04, 2.2504e-04, 2.2094e-04,\n",
      "        2.1690e-04, 2.1293e-04, 2.0904e-04, 2.0520e-04, 2.0144e-04, 1.9774e-04,\n",
      "        1.9410e-04, 1.9053e-04, 1.8702e-04, 1.8357e-04, 1.8018e-04, 1.7685e-04,\n",
      "        1.7358e-04, 1.7036e-04, 1.6720e-04, 1.6410e-04, 1.6105e-04, 1.5805e-04,\n",
      "        1.5511e-04, 1.5222e-04, 1.4937e-04, 1.4658e-04, 1.4384e-04, 1.4115e-04,\n",
      "        1.3850e-04, 1.3590e-04, 1.3335e-04, 1.3084e-04, 1.2838e-04, 1.2596e-04,\n",
      "        1.2358e-04, 1.2125e-04, 1.1896e-04, 1.1671e-04, 1.1450e-04, 1.1232e-04,\n",
      "        1.1019e-04, 1.0810e-04, 1.0604e-04, 1.0402e-04, 1.0204e-04, 1.0009e-04,\n",
      "        9.8180e-05, 9.6302e-05, 9.4459e-05, 9.2649e-05, 9.0871e-05, 8.9126e-05,\n",
      "        8.7413e-05, 8.5731e-05, 8.4080e-05, 8.2458e-05, 8.0867e-05, 7.9304e-05,\n",
      "        7.7770e-05, 7.6264e-05, 7.4786e-05, 7.3335e-05, 7.1911e-05, 7.0513e-05,\n",
      "        6.9140e-05, 6.7793e-05, 6.6471e-05, 6.5174e-05, 6.3900e-05, 6.2650e-05,\n",
      "        6.1423e-05, 6.0219e-05, 5.9038e-05, 5.7878e-05, 5.6740e-05, 5.5623e-05,\n",
      "        5.4527e-05, 5.3452e-05, 5.2397e-05, 5.1361e-05, 5.0346e-05, 4.9349e-05,\n",
      "        4.8371e-05, 4.7411e-05, 4.6469e-05, 4.5545e-05, 4.4639e-05, 4.3750e-05,\n",
      "        4.2877e-05, 4.2022e-05, 4.1182e-05, 4.0358e-05], device='cuda:0')\n",
      "range of bars tensor(4.0358e-05, device='cuda:0') tensor(0.9999, device='cuda:0')\n",
      "range of sigmas, tensor(0.0074, device='cuda:0') tensor(1., device='cuda:0')\n",
      "Number parameters of the model: 345796\n",
      "Model strcuture: DiT(\n",
      "  (embedding): Linear(in_features=4, out_features=64, bias=True)\n",
      "  (pos_embedding): SinousEmbedding()\n",
      "  (t_embedding): Sequential(\n",
      "    (0): SinousEmbedding()\n",
      "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (layers): ModuleList(\n",
      "    (0): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (K): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (V): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (K): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (V): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (K): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (V): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (K): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (V): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (K): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (V): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Layer(\n",
      "      (attn): Attn(\n",
      "        (Q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (K): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (V): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): GELU()\n",
      "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (condition_mlp): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): SiLU()\n",
      "        (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm1): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm2): AdaptiveLayerNorm(\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "        (beta_gamma): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out_norm): AdaptiveLayerNorm(\n",
      "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "    (beta_gamma): Sequential(\n",
      "      (0): SiLU()\n",
      "      (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (out_proj): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.5560: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:13<00:00,  1.49it/s]\n",
      "epoch 0, MSE 0.1336, [Valid] 0.1336: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DiT. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SinousEmbedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SiLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attn. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GELU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type AdaptiveLayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "epoch 1, loss 0.0983: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 1, MSE 0.0826, [Valid] 0.0826: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.51it/s]\n",
      "epoch 2, loss 0.0772: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 2, MSE 0.0733, [Valid] 0.0733: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.62it/s]\n",
      "epoch 3, loss 0.0717: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 3, MSE 0.0700, [Valid] 0.0700: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.53it/s]\n",
      "epoch 4, loss 0.0696: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 4, MSE 0.0672, [Valid] 0.0672: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.56it/s]\n",
      "epoch 5, loss 0.0672: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 5, MSE 0.0642, [Valid] 0.0642: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 0.0632: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 6, MSE 0.0648, [Valid] 0.0648: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.50it/s]\n",
      "epoch 7, loss 0.0630: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 7, MSE 0.0616, [Valid] 0.0616: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.49it/s]\n",
      "epoch 8, loss 0.0603: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 8, MSE 0.0605, [Valid] 0.0605: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.49it/s]\n",
      "epoch 9, loss 0.0587: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.48it/s]\n",
      "epoch 9, MSE 0.0579, [Valid] 0.0579: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.49it/s]\n",
      "epoch 10, loss 0.0579: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.48it/s]\n",
      "epoch 10, MSE 0.0575, [Valid] 0.0575: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11, loss 0.0565: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 11, MSE 0.0574, [Valid] 0.0574: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.52it/s]\n",
      "epoch 12, loss 0.0559: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 12, MSE 0.0557, [Valid] 0.0557: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.49it/s]\n",
      "epoch 13, loss 0.0553: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 13, MSE 0.0549, [Valid] 0.0549: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.51it/s]\n",
      "epoch 14, loss 0.0543: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 14, MSE 0.0541, [Valid] 0.0541: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.57it/s]\n",
      "epoch 15, loss 0.0533: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.48it/s]\n",
      "epoch 15, MSE 0.0531, [Valid] 0.0531: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_15.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 16, loss 0.0536: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 16, MSE 0.0535, [Valid] 0.0535: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.52it/s]\n",
      "epoch 17, loss 0.0530: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 17, MSE 0.0528, [Valid] 0.0528: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.55it/s]\n",
      "epoch 18, loss 0.0535: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 18, MSE 0.0526, [Valid] 0.0526: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.51it/s]\n",
      "epoch 19, loss 0.0532: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 19, MSE 0.0510, [Valid] 0.0510: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.54it/s]\n",
      "epoch 20, loss 0.0520: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 20, MSE 0.0524, [Valid] 0.0524: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_20.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 21, loss 0.0520: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 21, MSE 0.0518, [Valid] 0.0518: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.60it/s]\n",
      "epoch 22, loss 0.0515: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 22, MSE 0.0523, [Valid] 0.0523: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.53it/s]\n",
      "epoch 23, loss 0.0510: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 23, MSE 0.0518, [Valid] 0.0518: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.53it/s]\n",
      "epoch 24, loss 0.0514: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 24, MSE 0.0517, [Valid] 0.0517: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.58it/s]\n",
      "epoch 25, loss 0.0515: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:12<00:00,  1.49it/s]\n",
      "epoch 25, MSE 0.0506, [Valid] 0.0506: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_25.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 26, loss 0.0513:  10%|█████████▌                                                                                          | 18/188 [00:12<01:59,  1.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3838199/102147449.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# sample(model,save_dir=os.path.join('./samples',f'init.png'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;31m# visualize(model,save_dir=os.path.join('./samples',f'init_visualize.png'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3838199/102147449.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, model, optimizer, eval_interval)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# parent_dir = os.path.abspath('/root/DeepLearning')\n",
    "parent_dir = os.path.abspath('/home/zhh24/DeepLearning')\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "print('appended',parent_dir)\n",
    "\n",
    "import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "mnist = utils.MNIST(batch_size=256)\n",
    "train_loader = mnist.train_dataloader\n",
    "valid_loader = mnist.valid_dataloader\n",
    "\n",
    "T=1000\n",
    "beta1=1e-4 # variance of lowest temperature\n",
    "betaT=2e-2 # variance of highest temperature\n",
    "\n",
    "# step = torch.log(torch.tensor(betaT/beta1))/(T-1)\n",
    "# betas = beta1 * torch.exp(step*torch.arange(T,dtype=torch.float).to(device))\n",
    "step = (betaT-beta1)/(T-1)\n",
    "betas = torch.arange(T,dtype=torch.float,device=device) * step + beta1\n",
    "\n",
    "\n",
    "alphas = 1-betas\n",
    "alpha_bars = alphas.clone()\n",
    "for i in range(1,T):\n",
    "    alpha_bars[i] *= alpha_bars[i-1]\n",
    "\n",
    "print(alpha_bars)\n",
    "print('range of bars',alpha_bars.min(),alpha_bars.max())\n",
    "# print(alphas)\n",
    "# assert False\n",
    "\n",
    "sqrt = torch.sqrt\n",
    "sigmas = sqrt(betas * (1-alpha_bars / alphas)/(1-alpha_bars))\n",
    "sigmas[0] = 1\n",
    "print('range of sigmas,',sigmas.min(),sigmas.max())\n",
    "alphas = alphas.to(device)\n",
    "alpha_bars = alpha_bars.to(device)\n",
    "betas = betas.to(device)\n",
    "sigmas = sigmas.to(device)\n",
    "weights = torch.ones(T,dtype=torch.float,device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model:DiT,save_dir):\n",
    "    x = torch.randn([100,784]).to(device)\n",
    "    for t in range(T-1,-1,-1):\n",
    "        sigmaz = torch.randn_like(x)*sigmas[t]\n",
    "        if t==0:\n",
    "            sigmaz = 0\n",
    "        x = (x-(1-alphas[t])/(sqrt(1-alpha_bars[t]))*model(x,t*torch.ones(x.shape[0],dtype=torch.long,device=device)))/(sqrt(alphas[t]))+sigmaz\n",
    "        # x = torch.clamp(x,0,1)\n",
    "    grid = torchvision.utils.make_grid(post_process(x).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize(model,save_dir):\n",
    "    interval = (T-1) // 20\n",
    "    x = torch.randn([10,784]).to(device)\n",
    "    x_history = []\n",
    "    for t in range(T-1,-1,-1):\n",
    "        sigmaz = torch.randn_like(x)*((betas[t])**0.5).to(device)\n",
    "        if t==0:\n",
    "            sigmaz = 0\n",
    "        x = (x-(1-alphas[t])/(sqrt(1-alpha_bars[t]))*model(x,t*torch.ones(x.shape[0],dtype=torch.long,device=device)))/(sqrt(alphas[t]))+sigmaz\n",
    "        # x = torch.clamp(x,0,1)\n",
    "        x_history.append(x)\n",
    "    # print('cat.shape',torch.cat(x_history,dim=0).shape)\n",
    "    grid = torchvision.utils.make_grid(post_process(torch.stack(x_history,dim=0)[::interval,...]).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "    print('Saved visualize to',os.path.abspath(save_dir))\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_denoise(model,save_dir):\n",
    "    # get 10 images from the dataset\n",
    "    x,_ = next(iter(valid_loader))\n",
    "    x = x[:20,...].reshape(20,784).to(device)\n",
    "    x = pre_process(x)\n",
    "    t = torch.tensor([i * T // 20 for i in range(20)],dtype=torch.long,device=device)\n",
    "    noise = torch.randn_like(x).reshape(-1,784)\n",
    "    v1 = (sqrt(alpha_bars[t]).reshape(-1,1)*x).reshape(-1,784)\n",
    "    v2 = sqrt(1-alpha_bars[t]).reshape(-1,1)*noise\n",
    "    x_corr = v1+v2\n",
    "    est = model(x_corr,t)\n",
    "    x_rec = (x_corr - sqrt(1-alpha_bars[t]).reshape(-1,1)*est)/(sqrt(alpha_bars[t])).reshape(-1,1)\n",
    "    grid_orig = torchvision.utils.make_grid(post_process(x).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    grid_corr = torchvision.utils.make_grid(post_process(x_corr).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    grid_rec = torchvision.utils.make_grid(post_process(x_rec).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    # add noise level infomation to the image\n",
    "    noise_level = (1-alpha_bars[t]).reshape(-1).tolist()\n",
    "    ori_mse = noise.pow(2).mean(dim=1).reshape(-1).tolist()\n",
    "    mse = ((est-noise)**2).mean(dim=1).reshape(-1).tolist()\n",
    "    print(noise_level)\n",
    "    print(ori_mse)\n",
    "    print(mse)\n",
    "    grid = torch.cat([grid_orig,grid_corr,grid_rec],dim=1)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "    print('Saved denoise to',os.path.abspath(save_dir))\n",
    "\n",
    "def plot_loss(losses,save_dir):\n",
    "    losses_vals, t_vals = zip(*losses)\n",
    "    losses_vals = torch.cat(losses_vals,dim=0)\n",
    "    t_vals = torch.cat(t_vals,dim=0)\n",
    "    # print('t_vals',t_vals)\n",
    "    # print('losses_vals',losses_vals)\n",
    "\n",
    "    results = []\n",
    "    for t in range(T):\n",
    "        this_t = abs(t_vals.float()-float(t))<0.5\n",
    "        results.append(torch.sum(torch.where(this_t,losses_vals,torch.tensor(0.,device=device))).item() / (torch.sum(this_t.float())+1e-3).item())\n",
    "    plt.plot(results)\n",
    "    plt.ylim(0,max(results)* 1.2)\n",
    "    plt.savefig(save_dir)\n",
    "    plt.close()\n",
    "    # weights = (torch.tensor(results,device=device)) # weights\n",
    "    weights = torch.ones(T,dtype=torch.float,device=device)\n",
    "    # weights[:10]=0\n",
    "    # weights[10:80] /= 100\n",
    "    return weights\n",
    "\n",
    "def pre_process(x):\n",
    "    # do the logit transform\n",
    "    # return (torch.log(x+1e-3)-torch.log(1-x+1e-3))\n",
    "    return x*2-1 #MODIFIED\n",
    "    return (x+1)/2\n",
    "\n",
    "def post_process(x):\n",
    "    # return torch.sigmoid(x)\n",
    "    return (x+1)/2 #MODIFIED\n",
    "    return x*2-1\n",
    "\n",
    "def train(epochs,model:DiT,optimizer,eval_interval=5):\n",
    "    global weights\n",
    "    for epoch in range(epochs):\n",
    "        # print('weights normalized:',weights/weights.sum())\n",
    "        all_ts = torch.distributions.Categorical(weights).sample((50000,))\n",
    "        cnt = 0\n",
    "        model.train()\n",
    "        with tqdm(train_loader) as bar:\n",
    "            losses = []\n",
    "            for x,_ in bar:\n",
    "                cnt += x.shape[0]\n",
    "                x = pre_process(x.to(device))\n",
    "                epss = torch.randn_like(x).reshape(-1,784).to(device)\n",
    "                # ts = torch.randint(0,T,(x.shape[0],),device=device,dtype=torch.long)\n",
    "                ts = all_ts[cnt-x.shape[0]:cnt]\n",
    "                alpha_tbars = alpha_bars[ts]\n",
    "                value = (sqrt(alpha_tbars).reshape(-1,1,1,1)*x).reshape(-1,784)+sqrt(1-alpha_tbars).reshape(-1,1)*epss\n",
    "                out = model(value,ts) # [batch,784]\n",
    "\n",
    "                # loss = ((epss-out).pow(2).mean(dim=-1) * (betas[ts])/(2*alphas[ts]*(1-alpha_tbars))).sum(dim=0)\n",
    "                loss = ((epss-out).pow(2).mean(dim=-1)).mean(dim=0)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "                bar.set_description('epoch {}, loss {:.4f}'.format(epoch,sum(losses)/len(losses)))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(valid_loader) as bar:\n",
    "                mses = []\n",
    "                losses = []\n",
    "                losses_for_t = []\n",
    "                for x,_ in bar:\n",
    "                    x = pre_process(x.to(device))\n",
    "                    epss = torch.randn_like(x).reshape(-1,784).to(device)\n",
    "                    ts = torch.randint(0,T,(x.shape[0],),device=device,dtype=torch.long)\n",
    "                    # print(ts)\n",
    "                    alpha_tbars = alpha_bars[ts]\n",
    "                    value = (sqrt(alpha_tbars).reshape(-1,1,1,1)*x).reshape(-1,784)+sqrt(1-alpha_tbars).reshape(-1,1)*epss\n",
    "                    out = model(value,ts)\n",
    "                    mse = F.mse_loss(epss,out)\n",
    "                    mses.append(mse.item())\n",
    "                    loss = ((epss-out).pow(2).mean(dim=-1))\n",
    "                    # loss = (epss-out).pow(2).mean(dim=-1)\n",
    "                    losses_for_t.append((loss.clone().detach(),ts))\n",
    "                    loss = (loss).mean(dim=0)\n",
    "                    losses.append(loss.item())\n",
    "                    bar.set_description('epoch {}, MSE {:.4f}, [Valid] {:.4f}'.format(epoch,sum(mses)/len(mses),sum(losses)/len(losses)))\n",
    "                    \n",
    "        if epoch % eval_interval == 0:\n",
    "            visualize(model,save_dir=os.path.join('./samples',f'diffuse_epoch_{epoch}.png'))\n",
    "            sample(model,save_dir=os.path.join('./samples',f'sample_epoch_{epoch}.png'))\n",
    "            # visualize_denoise(model,save_dir=os.path.join('./samples',f'denoise_epoch_{epoch}.png'))\n",
    "            weights = plot_loss(losses_for_t,save_dir=os.path.join('./samples',f'loss_epoch_{epoch}.png'))\n",
    "            torch.save(model,os.path.join('./samples',f'epoch_{epoch}.pt'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = DiT(\n",
    "        num_layers=6,\n",
    "        hidden_dim=64,\n",
    "        patch_size=2,\n",
    "        num_heads=8\n",
    "    ).to(device)\n",
    "    print('Number parameters of the model:', sum(p.numel() for p in model.parameters()))\n",
    "    print('Model strcuture:',model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "    os.makedirs('./samples',exist_ok=True)\n",
    "    # sample(model,save_dir=os.path.join('./samples',f'init.png'))\n",
    "    # visualize(model,save_dir=os.path.join('./samples',f'init_visualize.png'))\n",
    "    train(200,model,optimizer,eval_interval=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
