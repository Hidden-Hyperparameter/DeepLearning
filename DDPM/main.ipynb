{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 19 12:13:05 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              54W / 300W |   8716MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   3838199      C   ...zhh24/anaconda3/envs/DYY/bin/python     8500MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/bin/python\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!which python | grep DYY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(3407)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class SinousEmbedding(nn.Module):\n",
    "#     def __init__(self, dim) -> None:\n",
    "#         super().__init__()\n",
    "#         assert dim%2==0,NotImplementedError()\n",
    "#         self.angles = (1000.**(-2/dim))**torch.arange(1,dim//2+1,1,dtype=torch.float).cuda()\n",
    "#         self.angles.requires_grad_(False)\n",
    "#     def forward(self,x):\n",
    "#         angles = torch.einsum('m,i->im',self.angles,x.float())\n",
    "#         return torch.cat((torch.sin(angles),torch.cos(angles)),dim=1)\n",
    "\n",
    "# class DDPM(nn.Module):\n",
    "#     def __init__(self, *args, **kwargs) -> None:\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.in_size = 28 * 28\n",
    "#         self.t_embedding_dim = 256\n",
    "#         self.t_embedding = SinousEmbedding(dim=self.t_embedding_dim)\n",
    "#         self.up = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(784+self.t_embedding_dim,64),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(64,32),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#             # nn.Sequential(\n",
    "#             #     nn.Linear(256,256),\n",
    "#             #     # nn.LeakyReLU(0.1),\n",
    "#             # ),\n",
    "#         ])\n",
    "#         self.middle = nn.ModuleList([\n",
    "#             nn.Linear(32,32),\n",
    "#             # nn.LeakyReLU(0.1),\n",
    "#         ])\n",
    "#         self.down= nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(32,32),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#             # nn.Sequential(\n",
    "#             #     nn.Linear(256,256),\n",
    "#             #     # nn.LeakyReLU(0.1),\n",
    "#             # ),\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(32,64),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#         ])\n",
    "#         self.end_mlp = nn.Linear(64,784)\n",
    "#         self.apply_init()\n",
    "\n",
    "#     def apply_init(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.xavier_normal_(m.weight)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     def forward(self,x,t):\n",
    "#         x = x.reshape(-1,784)\n",
    "#         ttensor = self.t_embedding(t) # [batch, 256]\n",
    "#         batch = x.shape[0]\n",
    "#         xc = x.clone()\n",
    "#         ups = []\n",
    "#         x = torch.cat((x,ttensor),dim=-1)\n",
    "#         for ly in self.up:\n",
    "#             x = ly(x)\n",
    "#             ups.append(x.clone())\n",
    "#         for ly in self.middle:\n",
    "#             x = ly(x)\n",
    "#         for ly in self.down:\n",
    "#             x = ly(x) + ups.pop()\n",
    "\n",
    "#         x = self.end_mlp(x)\n",
    "#         x = (x + xc)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SinousEmbedding(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        assert dim%2==0,NotImplementedError()\n",
    "        self.angles = (10000.**(-2/dim))**torch.arange(1,dim//2+1,1,dtype=torch.float).cuda()\n",
    "        self.angles.requires_grad_(False)\n",
    "    def forward(self,x):\n",
    "        angles = torch.einsum('m,i->im',self.angles,x.float())\n",
    "        return torch.cat((torch.sin(angles),torch.cos(angles)),dim=1)\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,channels=128,kernel_size=3,t_dim=64) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels,channels,kernel_size=kernel_size,padding=kernel_size//2)\n",
    "        self.t_net = nn.Linear(t_dim,channels)\n",
    "        self.conv2 = nn.Conv2d(channels,channels,kernel_size=kernel_size,padding=kernel_size//2)\n",
    "        self.conv1.weight.data.fill_(0)\n",
    "        self.conv2.weight.data.fill_(0)\n",
    "        self.t_net.weight.data.fill_(0)\n",
    "        self.conv1.bias.data.fill_(0)\n",
    "        self.conv2.bias.data.fill_(0)\n",
    "        self.t_net.bias.data.fill_(0)\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        xc = x.clone()\n",
    "        x = self.conv1(x.relu())\n",
    "        x = x + self.t_net(t).unsqueeze(-1).unsqueeze(-1).expand(t.shape[0],x.shape[1],x.shape[2],x.shape[3])\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + xc\n",
    "\n",
    "\n",
    "class F_x_t(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,out_channels,out_size,kernel_size=3,t_shape=64,attn=False,attn_dim=32,residual=True) -> None:\n",
    "        super().__init__()\n",
    "        # self.t_channels = out_channels // 2\n",
    "        # self.conv_channels = out_channels - self.t_channels\n",
    "        self.t_channels = out_channels\n",
    "        self.conv_channels = out_channels\n",
    "        self.conv = nn.Conv2d(in_channels, self.conv_channels, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        self.out_size = out_size\n",
    "        self.fc = nn.Linear(t_shape, self.t_channels)\n",
    "        self.attn = attn\n",
    "        self.residual = residual\n",
    "        if attn:\n",
    "            self.Q  = nn.Conv2d(out_channels, attn_dim, kernel_size=1)\n",
    "            self.K  = nn.Conv2d(out_channels, attn_dim, kernel_size=1)\n",
    "            self.V  = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
    "        if residual:\n",
    "            self.res = ResidualBlock(channels=out_channels,kernel_size=kernel_size,t_dim=t_shape)\n",
    "        # self.fc = nn.Embedding(t_shape, self.t_num)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if self.t_channels == 0:\n",
    "            raise NotImplementedError()\n",
    "            return self.conv(x)\n",
    "        # return torch.cat([self.conv(x),self.fc(t).unsqueeze(-1).unsqueeze(-1).expand(t.shape[0], self.t_channels, self.out_size, self.out_size)],dim=1).relu()\n",
    "        val = self.conv(x) + self.fc(t).unsqueeze(-1).unsqueeze(-1).expand(t.shape[0], self.t_channels, self.out_size, self.out_size)\n",
    "        if self.residual:\n",
    "            val = self.res(val,t)\n",
    "        if self.attn:\n",
    "            q = self.Q(val)\n",
    "            k = self.K(val)\n",
    "            v = self.V(val)\n",
    "            attn_score = torch.einsum('bchw,bcxy->bhwxy',q,k).reshape(q.shape[0],*q.shape[-2:],-1)\n",
    "            attn_score = attn_score.softmax(dim=-1).reshape(q.shape[0],*q.shape[-2:],*k.shape[-2:])\n",
    "            return torch.einsum('bhwxy,bcxy->bchw',attn_score,v).relu()\n",
    "        return val.relu()\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.t_embedding_dim = 32\n",
    "        self.t_embedding = SinousEmbedding(dim=self.t_embedding_dim)\n",
    "        self.up= nn.ModuleList([\n",
    "            F_x_t(in_channels=1,out_channels=32,out_size=32,kernel_size=3,t_shape=self.t_embedding_dim),\n",
    "            F_x_t(in_channels=32,out_channels=64,out_size=16,kernel_size=3,t_shape=self.t_embedding_dim),\n",
    "            F_x_t(in_channels=64,out_channels=128,out_size=8,kernel_size=3,t_shape=self.t_embedding_dim,attn=False),\n",
    "            # ResidualBlock(channels=128,kernel_size=3,t_dim=self.t_embedding_dim),\n",
    "            # F_x_t(in_channels=128,out_channels=128,out_size=4,kernel_size=1,t_shape=self.t_embedding_dim),\n",
    "        ])\n",
    "        self.middle = nn.ModuleList([\n",
    "            nn.Identity()\n",
    "            # ResidualBlock(channels=128,kernel_size=3,t_dim=self.t_embedding_dim),\n",
    "            # F_x_t(in_channels=128,out_channels=128,out_size=4,kernel_size=1,t_shape=self.t_embedding_dim,attn=False),\n",
    "        ])\n",
    "        self.down= nn.ModuleList([\n",
    "            # F_x_t(in_channels=128,out_channels=128,out_size=2,kernel_size=1,t_shape=self.t_embedding_dim),\n",
    "            F_x_t(in_channels=128,out_channels=64,out_size=8,kernel_size=3,t_shape=self.t_embedding_dim,attn=False),\n",
    "            F_x_t(in_channels=64,out_channels=32,out_size=16,kernel_size=3,t_shape=self.t_embedding_dim),\n",
    "            F_x_t(in_channels=32,out_channels=16,out_size=32,kernel_size=3,t_shape=self.t_embedding_dim),\n",
    "        ])\n",
    "        # self.end_mlp = nn.Conv2d(32,1,kernel_size=3,padding=1)\n",
    "        self.end_mlp = nn.Conv2d(16,1,kernel_size=1)\n",
    "\n",
    "    def forward(self,x,t):\n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        x = F.pad(x,(2,2,2,2),mode='constant',value=0)\n",
    "        ttensor = self.t_embedding(t) # [batch, 256]\n",
    "        batch = x.shape[0]\n",
    "        # xc = x.clone()            print(attn_score.shape)\n",
    "\n",
    "        ups = []\n",
    "        for ly in self.up:\n",
    "            x = ly(x,ttensor)\n",
    "            ups.append(x.clone()) # append: 28x28, 14x14\n",
    "            x = nn.AvgPool2d(2)(x)\n",
    "        for ly in self.middle:\n",
    "            # x = ly(x,ttensor)\n",
    "            x = ly(x)\n",
    "        for ly in self.down:\n",
    "            x = nn.Upsample(scale_factor=2)(x) + ups.pop() # 14x14, 28x28\n",
    "            x = ly(x,ttensor)\n",
    "            # x = nn.Upsample(scale_factor=2)(x) + ups.pop()\n",
    "        x = self.end_mlp(x)\n",
    "        x = x[:,:,2:30,2:30]\n",
    "        return x.reshape(batch,28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended /home/zhh24/DeepLearning\n",
      "tensor([9.9990e-01, 9.9978e-01, 9.9964e-01, 9.9948e-01, 9.9930e-01, 9.9910e-01,\n",
      "        9.9888e-01, 9.9864e-01, 9.9838e-01, 9.9811e-01, 9.9781e-01, 9.9749e-01,\n",
      "        9.9715e-01, 9.9679e-01, 9.9641e-01, 9.9602e-01, 9.9560e-01, 9.9516e-01,\n",
      "        9.9471e-01, 9.9423e-01, 9.9374e-01, 9.9322e-01, 9.9269e-01, 9.9213e-01,\n",
      "        9.9156e-01, 9.9096e-01, 9.9035e-01, 9.8972e-01, 9.8907e-01, 9.8840e-01,\n",
      "        9.8771e-01, 9.8700e-01, 9.8627e-01, 9.8553e-01, 9.8476e-01, 9.8398e-01,\n",
      "        9.8317e-01, 9.8235e-01, 9.8151e-01, 9.8065e-01, 9.7977e-01, 9.7887e-01,\n",
      "        9.7795e-01, 9.7702e-01, 9.7606e-01, 9.7509e-01, 9.7410e-01, 9.7309e-01,\n",
      "        9.7206e-01, 9.7102e-01, 9.6995e-01, 9.6887e-01, 9.6777e-01, 9.6665e-01,\n",
      "        9.6551e-01, 9.6436e-01, 9.6319e-01, 9.6200e-01, 9.6079e-01, 9.5956e-01,\n",
      "        9.5832e-01, 9.5706e-01, 9.5578e-01, 9.5449e-01, 9.5318e-01, 9.5185e-01,\n",
      "        9.5050e-01, 9.4914e-01, 9.4776e-01, 9.4636e-01, 9.4494e-01, 9.4351e-01,\n",
      "        9.4207e-01, 9.4060e-01, 9.3912e-01, 9.3762e-01, 9.3611e-01, 9.3458e-01,\n",
      "        9.3304e-01, 9.3147e-01, 9.2990e-01, 9.2830e-01, 9.2669e-01, 9.2507e-01,\n",
      "        9.2343e-01, 9.2177e-01, 9.2010e-01, 9.1841e-01, 9.1671e-01, 9.1500e-01,\n",
      "        9.1326e-01, 9.1152e-01, 9.0976e-01, 9.0798e-01, 9.0619e-01, 9.0438e-01,\n",
      "        9.0256e-01, 9.0073e-01, 8.9888e-01, 8.9702e-01, 8.9514e-01, 8.9325e-01,\n",
      "        8.9135e-01, 8.8943e-01, 8.8750e-01, 8.8555e-01, 8.8359e-01, 8.8162e-01,\n",
      "        8.7964e-01, 8.7764e-01, 8.7563e-01, 8.7360e-01, 8.7157e-01, 8.6952e-01,\n",
      "        8.6746e-01, 8.6538e-01, 8.6330e-01, 8.6120e-01, 8.5909e-01, 8.5697e-01,\n",
      "        8.5483e-01, 8.5269e-01, 8.5053e-01, 8.4836e-01, 8.4618e-01, 8.4399e-01,\n",
      "        8.4179e-01, 8.3957e-01, 8.3735e-01, 8.3511e-01, 8.3287e-01, 8.3061e-01,\n",
      "        8.2834e-01, 8.2606e-01, 8.2378e-01, 8.2148e-01, 8.1917e-01, 8.1685e-01,\n",
      "        8.1453e-01, 8.1219e-01, 8.0984e-01, 8.0749e-01, 8.0512e-01, 8.0275e-01,\n",
      "        8.0037e-01, 7.9797e-01, 7.9557e-01, 7.9316e-01, 7.9075e-01, 7.8832e-01,\n",
      "        7.8589e-01, 7.8344e-01, 7.8099e-01, 7.7854e-01, 7.7607e-01, 7.7360e-01,\n",
      "        7.7111e-01, 7.6863e-01, 7.6613e-01, 7.6363e-01, 7.6112e-01, 7.5860e-01,\n",
      "        7.5608e-01, 7.5354e-01, 7.5101e-01, 7.4846e-01, 7.4591e-01, 7.4336e-01,\n",
      "        7.4080e-01, 7.3823e-01, 7.3565e-01, 7.3308e-01, 7.3049e-01, 7.2790e-01,\n",
      "        7.2530e-01, 7.2270e-01, 7.2010e-01, 7.1749e-01, 7.1487e-01, 7.1225e-01,\n",
      "        7.0963e-01, 7.0700e-01, 7.0436e-01, 7.0172e-01, 6.9908e-01, 6.9644e-01,\n",
      "        6.9379e-01, 6.9113e-01, 6.8847e-01, 6.8581e-01, 6.8315e-01, 6.8048e-01,\n",
      "        6.7781e-01, 6.7514e-01, 6.7246e-01, 6.6978e-01, 6.6710e-01, 6.6441e-01,\n",
      "        6.6173e-01, 6.5904e-01, 6.5635e-01, 6.5365e-01, 6.5096e-01, 6.4826e-01,\n",
      "        6.4556e-01, 6.4286e-01, 6.4016e-01, 6.3745e-01, 6.3475e-01, 6.3204e-01,\n",
      "        6.2934e-01, 6.2663e-01, 6.2392e-01, 6.2121e-01, 6.1850e-01, 6.1579e-01,\n",
      "        6.1308e-01, 6.1037e-01, 6.0765e-01, 6.0494e-01, 6.0223e-01, 5.9952e-01,\n",
      "        5.9681e-01, 5.9410e-01, 5.9139e-01, 5.8868e-01, 5.8597e-01, 5.8326e-01,\n",
      "        5.8055e-01, 5.7785e-01, 5.7514e-01, 5.7244e-01, 5.6974e-01, 5.6703e-01,\n",
      "        5.6433e-01, 5.6164e-01, 5.5894e-01, 5.5624e-01, 5.5355e-01, 5.5086e-01,\n",
      "        5.4817e-01, 5.4549e-01, 5.4280e-01, 5.4012e-01, 5.3744e-01, 5.3476e-01,\n",
      "        5.3209e-01, 5.2942e-01, 5.2675e-01, 5.2409e-01, 5.2142e-01, 5.1876e-01,\n",
      "        5.1611e-01, 5.1346e-01, 5.1081e-01, 5.0816e-01, 5.0552e-01, 5.0288e-01,\n",
      "        5.0024e-01, 4.9761e-01, 4.9499e-01, 4.9236e-01, 4.8974e-01, 4.8713e-01,\n",
      "        4.8452e-01, 4.8191e-01, 4.7931e-01, 4.7671e-01, 4.7412e-01, 4.7153e-01,\n",
      "        4.6895e-01, 4.6637e-01, 4.6380e-01, 4.6123e-01, 4.5867e-01, 4.5611e-01,\n",
      "        4.5355e-01, 4.5101e-01, 4.4846e-01, 4.4593e-01, 4.4340e-01, 4.4087e-01,\n",
      "        4.3835e-01, 4.3583e-01, 4.3332e-01, 4.3082e-01, 4.2832e-01, 4.2583e-01,\n",
      "        4.2335e-01, 4.2087e-01, 4.1839e-01, 4.1593e-01, 4.1347e-01, 4.1101e-01,\n",
      "        4.0856e-01, 4.0612e-01, 4.0369e-01, 4.0126e-01, 3.9883e-01, 3.9642e-01,\n",
      "        3.9401e-01, 3.9161e-01, 3.8921e-01, 3.8683e-01, 3.8444e-01, 3.8207e-01,\n",
      "        3.7970e-01, 3.7734e-01, 3.7499e-01, 3.7264e-01, 3.7031e-01, 3.6798e-01,\n",
      "        3.6565e-01, 3.6334e-01, 3.6103e-01, 3.5872e-01, 3.5643e-01, 3.5414e-01,\n",
      "        3.5187e-01, 3.4959e-01, 3.4733e-01, 3.4508e-01, 3.4283e-01, 3.4059e-01,\n",
      "        3.3836e-01, 3.3613e-01, 3.3391e-01, 3.3171e-01, 3.2951e-01, 3.2731e-01,\n",
      "        3.2513e-01, 3.2295e-01, 3.2078e-01, 3.1862e-01, 3.1647e-01, 3.1433e-01,\n",
      "        3.1219e-01, 3.1007e-01, 3.0795e-01, 3.0584e-01, 3.0374e-01, 3.0164e-01,\n",
      "        2.9956e-01, 2.9748e-01, 2.9541e-01, 2.9335e-01, 2.9130e-01, 2.8926e-01,\n",
      "        2.8722e-01, 2.8520e-01, 2.8318e-01, 2.8117e-01, 2.7917e-01, 2.7718e-01,\n",
      "        2.7520e-01, 2.7323e-01, 2.7126e-01, 2.6931e-01, 2.6736e-01, 2.6542e-01,\n",
      "        2.6349e-01, 2.6157e-01, 2.5966e-01, 2.5775e-01, 2.5586e-01, 2.5397e-01,\n",
      "        2.5210e-01, 2.5023e-01, 2.4837e-01, 2.4652e-01, 2.4468e-01, 2.4284e-01,\n",
      "        2.4102e-01, 2.3920e-01, 2.3740e-01, 2.3560e-01, 2.3381e-01, 2.3203e-01,\n",
      "        2.3026e-01, 2.2850e-01, 2.2675e-01, 2.2501e-01, 2.2327e-01, 2.2155e-01,\n",
      "        2.1983e-01, 2.1812e-01, 2.1642e-01, 2.1473e-01, 2.1305e-01, 2.1138e-01,\n",
      "        2.0972e-01, 2.0806e-01, 2.0642e-01, 2.0478e-01, 2.0315e-01, 2.0153e-01,\n",
      "        1.9992e-01, 1.9832e-01, 1.9673e-01, 1.9515e-01, 1.9357e-01, 1.9201e-01,\n",
      "        1.9045e-01, 1.8890e-01, 1.8736e-01, 1.8583e-01, 1.8431e-01, 1.8280e-01,\n",
      "        1.8129e-01, 1.7980e-01, 1.7831e-01, 1.7683e-01, 1.7537e-01, 1.7391e-01,\n",
      "        1.7245e-01, 1.7101e-01, 1.6958e-01, 1.6815e-01, 1.6673e-01, 1.6533e-01,\n",
      "        1.6393e-01, 1.6254e-01, 1.6115e-01, 1.5978e-01, 1.5841e-01, 1.5706e-01,\n",
      "        1.5571e-01, 1.5437e-01, 1.5304e-01, 1.5171e-01, 1.5040e-01, 1.4909e-01,\n",
      "        1.4779e-01, 1.4650e-01, 1.4522e-01, 1.4395e-01, 1.4269e-01, 1.4143e-01,\n",
      "        1.4018e-01, 1.3894e-01, 1.3771e-01, 1.3649e-01, 1.3527e-01, 1.3406e-01,\n",
      "        1.3286e-01, 1.3167e-01, 1.3049e-01, 1.2932e-01, 1.2815e-01, 1.2699e-01,\n",
      "        1.2584e-01, 1.2470e-01, 1.2356e-01, 1.2243e-01, 1.2131e-01, 1.2020e-01,\n",
      "        1.1910e-01, 1.1800e-01, 1.1691e-01, 1.1583e-01, 1.1476e-01, 1.1369e-01,\n",
      "        1.1264e-01, 1.1159e-01, 1.1054e-01, 1.0951e-01, 1.0848e-01, 1.0746e-01,\n",
      "        1.0645e-01, 1.0544e-01, 1.0445e-01, 1.0346e-01, 1.0247e-01, 1.0150e-01,\n",
      "        1.0053e-01, 9.9567e-02, 9.8613e-02, 9.7667e-02, 9.6727e-02, 9.5794e-02,\n",
      "        9.4869e-02, 9.3950e-02, 9.3039e-02, 9.2134e-02, 9.1237e-02, 9.0346e-02,\n",
      "        8.9463e-02, 8.8586e-02, 8.7716e-02, 8.6853e-02, 8.5996e-02, 8.5146e-02,\n",
      "        8.4303e-02, 8.3467e-02, 8.2637e-02, 8.1814e-02, 8.0998e-02, 8.0188e-02,\n",
      "        7.9384e-02, 7.8587e-02, 7.7797e-02, 7.7012e-02, 7.6235e-02, 7.5463e-02,\n",
      "        7.4698e-02, 7.3939e-02, 7.3186e-02, 7.2440e-02, 7.1700e-02, 7.0966e-02,\n",
      "        7.0238e-02, 6.9516e-02, 6.8800e-02, 6.8090e-02, 6.7386e-02, 6.6688e-02,\n",
      "        6.5996e-02, 6.5309e-02, 6.4629e-02, 6.3954e-02, 6.3285e-02, 6.2622e-02,\n",
      "        6.1965e-02, 6.1313e-02, 6.0667e-02, 6.0026e-02, 5.9391e-02, 5.8762e-02,\n",
      "        5.8138e-02, 5.7520e-02, 5.6907e-02, 5.6299e-02, 5.5697e-02, 5.5100e-02,\n",
      "        5.4508e-02, 5.3922e-02, 5.3341e-02, 5.2765e-02, 5.2194e-02, 5.1628e-02,\n",
      "        5.1068e-02, 5.0513e-02, 4.9962e-02, 4.9417e-02, 4.8876e-02, 4.8341e-02,\n",
      "        4.7810e-02, 4.7284e-02, 4.6764e-02, 4.6247e-02, 4.5736e-02, 4.5230e-02,\n",
      "        4.4728e-02, 4.4231e-02, 4.3738e-02, 4.3250e-02, 4.2767e-02, 4.2288e-02,\n",
      "        4.1814e-02, 4.1344e-02, 4.0879e-02, 4.0418e-02, 3.9961e-02, 3.9509e-02,\n",
      "        3.9061e-02, 3.8618e-02, 3.8178e-02, 3.7743e-02, 3.7312e-02, 3.6886e-02,\n",
      "        3.6463e-02, 3.6045e-02, 3.5631e-02, 3.5220e-02, 3.4814e-02, 3.4412e-02,\n",
      "        3.4014e-02, 3.3619e-02, 3.3229e-02, 3.2842e-02, 3.2460e-02, 3.2081e-02,\n",
      "        3.1705e-02, 3.1334e-02, 3.0966e-02, 3.0603e-02, 3.0242e-02, 2.9886e-02,\n",
      "        2.9533e-02, 2.9183e-02, 2.8837e-02, 2.8495e-02, 2.8156e-02, 2.7821e-02,\n",
      "        2.7489e-02, 2.7160e-02, 2.6835e-02, 2.6513e-02, 2.6195e-02, 2.5879e-02,\n",
      "        2.5567e-02, 2.5259e-02, 2.4953e-02, 2.4651e-02, 2.4352e-02, 2.4056e-02,\n",
      "        2.3763e-02, 2.3474e-02, 2.3187e-02, 2.2903e-02, 2.2623e-02, 2.2345e-02,\n",
      "        2.2071e-02, 2.1799e-02, 2.1530e-02, 2.1264e-02, 2.1001e-02, 2.0741e-02,\n",
      "        2.0484e-02, 2.0229e-02, 1.9977e-02, 1.9728e-02, 1.9482e-02, 1.9238e-02,\n",
      "        1.8997e-02, 1.8758e-02, 1.8523e-02, 1.8289e-02, 1.8059e-02, 1.7831e-02,\n",
      "        1.7605e-02, 1.7382e-02, 1.7161e-02, 1.6943e-02, 1.6728e-02, 1.6514e-02,\n",
      "        1.6304e-02, 1.6095e-02, 1.5889e-02, 1.5685e-02, 1.5484e-02, 1.5284e-02,\n",
      "        1.5087e-02, 1.4893e-02, 1.4700e-02, 1.4510e-02, 1.4321e-02, 1.4135e-02,\n",
      "        1.3952e-02, 1.3770e-02, 1.3590e-02, 1.3413e-02, 1.3237e-02, 1.3064e-02,\n",
      "        1.2892e-02, 1.2723e-02, 1.2555e-02, 1.2389e-02, 1.2226e-02, 1.2064e-02,\n",
      "        1.1904e-02, 1.1746e-02, 1.1590e-02, 1.1436e-02, 1.1284e-02, 1.1133e-02,\n",
      "        1.0984e-02, 1.0837e-02, 1.0692e-02, 1.0548e-02, 1.0407e-02, 1.0266e-02,\n",
      "        1.0128e-02, 9.9911e-03, 9.8560e-03, 9.7225e-03, 9.5906e-03, 9.4603e-03,\n",
      "        9.3316e-03, 9.2044e-03, 9.0788e-03, 8.9548e-03, 8.8322e-03, 8.7112e-03,\n",
      "        8.5916e-03, 8.4735e-03, 8.3569e-03, 8.2417e-03, 8.1279e-03, 8.0155e-03,\n",
      "        7.9046e-03, 7.7950e-03, 7.6867e-03, 7.5799e-03, 7.4743e-03, 7.3701e-03,\n",
      "        7.2672e-03, 7.1655e-03, 7.0652e-03, 6.9661e-03, 6.8683e-03, 6.7717e-03,\n",
      "        6.6763e-03, 6.5822e-03, 6.4892e-03, 6.3974e-03, 6.3068e-03, 6.2173e-03,\n",
      "        6.1290e-03, 6.0419e-03, 5.9558e-03, 5.8709e-03, 5.7870e-03, 5.7042e-03,\n",
      "        5.6225e-03, 5.5419e-03, 5.4623e-03, 5.3837e-03, 5.3062e-03, 5.2297e-03,\n",
      "        5.1541e-03, 5.0796e-03, 5.0060e-03, 4.9334e-03, 4.8618e-03, 4.7911e-03,\n",
      "        4.7213e-03, 4.6525e-03, 4.5845e-03, 4.5175e-03, 4.4514e-03, 4.3861e-03,\n",
      "        4.3217e-03, 4.2582e-03, 4.1955e-03, 4.1336e-03, 4.0726e-03, 4.0124e-03,\n",
      "        3.9530e-03, 3.8945e-03, 3.8367e-03, 3.7796e-03, 3.7234e-03, 3.6679e-03,\n",
      "        3.6132e-03, 3.5592e-03, 3.5060e-03, 3.4534e-03, 3.4016e-03, 3.3506e-03,\n",
      "        3.3002e-03, 3.2505e-03, 3.2014e-03, 3.1531e-03, 3.1054e-03, 3.0584e-03,\n",
      "        3.0120e-03, 2.9663e-03, 2.9212e-03, 2.8768e-03, 2.8329e-03, 2.7897e-03,\n",
      "        2.7471e-03, 2.7051e-03, 2.6636e-03, 2.6228e-03, 2.5825e-03, 2.5428e-03,\n",
      "        2.5036e-03, 2.4650e-03, 2.4270e-03, 2.3894e-03, 2.3525e-03, 2.3160e-03,\n",
      "        2.2801e-03, 2.2446e-03, 2.2097e-03, 2.1753e-03, 2.1414e-03, 2.1079e-03,\n",
      "        2.0750e-03, 2.0425e-03, 2.0104e-03, 1.9789e-03, 1.9478e-03, 1.9171e-03,\n",
      "        1.8869e-03, 1.8572e-03, 1.8278e-03, 1.7989e-03, 1.7704e-03, 1.7423e-03,\n",
      "        1.7147e-03, 1.6874e-03, 1.6606e-03, 1.6341e-03, 1.6080e-03, 1.5823e-03,\n",
      "        1.5570e-03, 1.5321e-03, 1.5075e-03, 1.4833e-03, 1.4595e-03, 1.4360e-03,\n",
      "        1.4128e-03, 1.3900e-03, 1.3676e-03, 1.3455e-03, 1.3237e-03, 1.3022e-03,\n",
      "        1.2811e-03, 1.2602e-03, 1.2397e-03, 1.2195e-03, 1.1996e-03, 1.1800e-03,\n",
      "        1.1607e-03, 1.1417e-03, 1.1230e-03, 1.1046e-03, 1.0864e-03, 1.0686e-03,\n",
      "        1.0509e-03, 1.0336e-03, 1.0165e-03, 9.9974e-04, 9.8319e-04, 9.6689e-04,\n",
      "        9.5085e-04, 9.3505e-04, 9.1950e-04, 9.0419e-04, 8.8911e-04, 8.7427e-04,\n",
      "        8.5966e-04, 8.4527e-04, 8.3111e-04, 8.1717e-04, 8.0345e-04, 7.8994e-04,\n",
      "        7.7664e-04, 7.6355e-04, 7.5067e-04, 7.3799e-04, 7.2551e-04, 7.1322e-04,\n",
      "        7.0113e-04, 6.8923e-04, 6.7752e-04, 6.6600e-04, 6.5465e-04, 6.4349e-04,\n",
      "        6.3250e-04, 6.2169e-04, 6.1106e-04, 6.0059e-04, 5.9029e-04, 5.8015e-04,\n",
      "        5.7018e-04, 5.6036e-04, 5.5071e-04, 5.4121e-04, 5.3186e-04, 5.2266e-04,\n",
      "        5.1362e-04, 5.0472e-04, 4.9596e-04, 4.8734e-04, 4.7887e-04, 4.7053e-04,\n",
      "        4.6233e-04, 4.5426e-04, 4.4633e-04, 4.3852e-04, 4.3084e-04, 4.2329e-04,\n",
      "        4.1586e-04, 4.0855e-04, 4.0137e-04, 3.9430e-04, 3.8735e-04, 3.8051e-04,\n",
      "        3.7379e-04, 3.6718e-04, 3.6067e-04, 3.5428e-04, 3.4799e-04, 3.4181e-04,\n",
      "        3.3573e-04, 3.2975e-04, 3.2387e-04, 3.1809e-04, 3.1240e-04, 3.0682e-04,\n",
      "        3.0132e-04, 2.9592e-04, 2.9061e-04, 2.8539e-04, 2.8025e-04, 2.7521e-04,\n",
      "        2.7024e-04, 2.6537e-04, 2.6057e-04, 2.5586e-04, 2.5123e-04, 2.4667e-04,\n",
      "        2.4220e-04, 2.3780e-04, 2.3347e-04, 2.2922e-04, 2.2504e-04, 2.2094e-04,\n",
      "        2.1690e-04, 2.1293e-04, 2.0904e-04, 2.0520e-04, 2.0144e-04, 1.9774e-04,\n",
      "        1.9410e-04, 1.9053e-04, 1.8702e-04, 1.8357e-04, 1.8018e-04, 1.7685e-04,\n",
      "        1.7358e-04, 1.7036e-04, 1.6720e-04, 1.6410e-04, 1.6105e-04, 1.5805e-04,\n",
      "        1.5511e-04, 1.5222e-04, 1.4937e-04, 1.4658e-04, 1.4384e-04, 1.4115e-04,\n",
      "        1.3850e-04, 1.3590e-04, 1.3335e-04, 1.3084e-04, 1.2838e-04, 1.2596e-04,\n",
      "        1.2358e-04, 1.2125e-04, 1.1896e-04, 1.1671e-04, 1.1450e-04, 1.1232e-04,\n",
      "        1.1019e-04, 1.0810e-04, 1.0604e-04, 1.0402e-04, 1.0204e-04, 1.0009e-04,\n",
      "        9.8180e-05, 9.6302e-05, 9.4459e-05, 9.2649e-05, 9.0871e-05, 8.9126e-05,\n",
      "        8.7413e-05, 8.5731e-05, 8.4080e-05, 8.2458e-05, 8.0867e-05, 7.9304e-05,\n",
      "        7.7770e-05, 7.6264e-05, 7.4786e-05, 7.3335e-05, 7.1911e-05, 7.0513e-05,\n",
      "        6.9140e-05, 6.7793e-05, 6.6471e-05, 6.5174e-05, 6.3900e-05, 6.2650e-05,\n",
      "        6.1423e-05, 6.0219e-05, 5.9038e-05, 5.7878e-05, 5.6740e-05, 5.5623e-05,\n",
      "        5.4527e-05, 5.3452e-05, 5.2397e-05, 5.1361e-05, 5.0346e-05, 4.9349e-05,\n",
      "        4.8371e-05, 4.7411e-05, 4.6469e-05, 4.5545e-05, 4.4639e-05, 4.3750e-05,\n",
      "        4.2877e-05, 4.2022e-05, 4.1182e-05, 4.0358e-05], device='cuda:0')\n",
      "range of bars tensor(4.0358e-05, device='cuda:0') tensor(0.9999, device='cuda:0')\n",
      "range of sigmas, tensor(0.0074, device='cuda:0') tensor(1., device='cuda:0')\n",
      "Number parameters of the model: 696257\n",
      "Model strcuture: DDPM(\n",
      "  (t_embedding): SinousEmbedding()\n",
      "  (up): ModuleList(\n",
      "    (0): F_x_t(\n",
      "      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (res): ResidualBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (t_net): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): F_x_t(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (res): ResidualBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (t_net): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): F_x_t(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (res): ResidualBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (t_net): Linear(in_features=32, out_features=128, bias=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (middle): ModuleList(\n",
      "    (0): Identity()\n",
      "  )\n",
      "  (down): ModuleList(\n",
      "    (0): F_x_t(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (res): ResidualBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (t_net): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): F_x_t(\n",
      "      (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (res): ResidualBlock(\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (t_net): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (2): F_x_t(\n",
      "      (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (res): ResidualBlock(\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (t_net): Linear(in_features=32, out_features=16, bias=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (end_mlp): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                     | 0/94 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/init_visualize.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.2792: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.30it/s]\n",
      "epoch 0, MSE 0.1279, [Valid] 0.1279: 100%|███████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DDPM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SinousEmbedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type F_x_t. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ResidualBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "epoch 1, loss 0.1134: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:29<00:00,  3.29it/s]\n",
      "epoch 1, MSE 0.0988, [Valid] 0.0988: 100%|███████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.11it/s]\n",
      "epoch 2, loss 0.0909: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.28it/s]\n",
      "epoch 2, MSE 0.0820, [Valid] 0.0820: 100%|███████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.08it/s]\n",
      "epoch 3, loss 0.0804: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.29it/s]\n",
      "epoch 3, MSE 0.0763, [Valid] 0.0763: 100%|███████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.13it/s]\n",
      "epoch 4, loss 0.0712: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:29<00:00,  3.29it/s]\n",
      "epoch 4, MSE 0.0702, [Valid] 0.0702: 100%|███████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.10it/s]\n",
      "epoch 5, loss 0.0669: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.28it/s]\n",
      "epoch 5, MSE 0.0624, [Valid] 0.0624: 100%|███████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 0.0617: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:29<00:00,  3.30it/s]\n",
      "epoch 6, MSE 0.0608, [Valid] 0.0608: 100%|███████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.09it/s]\n",
      "epoch 7, loss 0.0604: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:29<00:00,  3.28it/s]\n",
      "epoch 7, MSE 0.0574, [Valid] 0.0574: 100%|███████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.11it/s]\n",
      "epoch 8, loss 0.0576: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.29it/s]\n",
      "epoch 8, MSE 0.0562, [Valid] 0.0562: 100%|███████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.11it/s]\n",
      "epoch 9, loss 0.0560: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.29it/s]\n",
      "epoch 9, MSE 0.0554, [Valid] 0.0554: 100%|███████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.11it/s]\n",
      "epoch 10, loss 0.0542: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:29<00:00,  3.29it/s]\n",
      "epoch 10, MSE 0.0534, [Valid] 0.0534: 100%|██████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11, loss 0.0525: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.30it/s]\n",
      "epoch 11, MSE 0.0542, [Valid] 0.0542: 100%|██████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.11it/s]\n",
      "epoch 12, loss 0.0520: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.29it/s]\n",
      "epoch 12, MSE 0.0513, [Valid] 0.0513: 100%|██████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.06it/s]\n",
      "epoch 13, loss 0.0505: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.29it/s]\n",
      "epoch 13, MSE 0.0520, [Valid] 0.0520: 100%|██████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.13it/s]\n",
      "epoch 14, loss 0.0497: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.30it/s]\n",
      "epoch 14, MSE 0.0503, [Valid] 0.0503: 100%|██████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.10it/s]\n",
      "epoch 15, loss 0.0487: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:30<00:00,  3.28it/s]\n",
      "epoch 15, MSE 0.0513, [Valid] 0.0513: 100%|██████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:03<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_15.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 16, loss 0.0475:  91%|████████████████████████████████████████████████████████████████████████████████████████████▍        | 86/94 [00:27<00:02,  3.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3840999/1398644464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./samples'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf'init.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./samples'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34mf'init_visualize.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meval_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_3840999/1398644464.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, model, optimizer, eval_interval)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# parent_dir = os.path.abspath('/root/DeepLearning')\n",
    "parent_dir = os.path.abspath('/home/zhh24/DeepLearning')\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "print('appended',parent_dir)\n",
    "\n",
    "import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "mnist = utils.MNIST(batch_size=512,data_aug=True)\n",
    "train_loader = mnist.train_dataloader\n",
    "valid_loader = mnist.valid_dataloader\n",
    "T=1000\n",
    "beta1=1e-4 # variance of lowest temperature\n",
    "betaT=2e-2 # variance of highest temperature\n",
    "\n",
    "# step = torch.log(torch.tensor(betaT/beta1))/(T-1)\n",
    "# betas = beta1 * torch.exp(step*torch.arange(T,dtype=torch.float).to(device))\n",
    "step = (betaT-beta1)/(T-1)\n",
    "betas = torch.arange(T,dtype=torch.float,device=device) * step + beta1\n",
    "\n",
    "\n",
    "alphas = 1-betas\n",
    "alpha_bars = alphas.clone()\n",
    "for i in range(1,T):\n",
    "    alpha_bars[i] *= alpha_bars[i-1]\n",
    "\n",
    "                    # we re-define a way to generate hyperparameters\n",
    "                    # alpha_bar_0 = .9\n",
    "                    # # alpha_bar_mid = .3\n",
    "                    # alpha_bar_T = 1e-3\n",
    "                    # alpha_bars = torch.zeros(T,dtype=torch.float)\n",
    "                    # # alpha_bars[:T//2] = alpha_bar_0 + (alpha_bar_mid-alpha_bar_0) * torch.arange(T//2,dtype=torch.float,device=device) / (T//2)\n",
    "                    # # alpha_bars[T//2:] = alpha_bar_mid + (alpha_bar_T-alpha_bar_mid) * torch.arange(T//2,dtype=torch.float,device=device) / (T//2)\n",
    "                    # alpha_bars = alpha_bar_0 + (alpha_bar_T-alpha_bar_0) * torch.arange(T,dtype=torch.float,device=device) / T\n",
    "                    # alphas = alpha_bars.clone()\n",
    "                    # for i in range(1,T):\n",
    "                    #     alphas[i] = alpha_bars[i] / alpha_bars[i-1]\n",
    "                    # betas = 1-alphas\n",
    "\n",
    "print(alpha_bars)\n",
    "print('range of bars',alpha_bars.min(),alpha_bars.max())\n",
    "# print(alphas)\n",
    "\n",
    "sqrt = torch.sqrt\n",
    "sigmas = sqrt(betas * (1-alpha_bars / alphas)/(1-alpha_bars))\n",
    "sigmas[0] = 1\n",
    "print('range of sigmas,',sigmas.min(),sigmas.max())\n",
    "alphas = alphas.to(device)\n",
    "alpha_bars = alpha_bars.to(device)\n",
    "betas = betas.to(device)\n",
    "sigmas = sigmas.to(device)\n",
    "weights = torch.ones(T,dtype=torch.float,device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model:DDPM,save_dir):\n",
    "    x = torch.randn([100,784]).to(device)\n",
    "    for t in range(T-1,-1,-1):\n",
    "        sigmaz = torch.randn_like(x)*sigmas[t]\n",
    "        if t==0:\n",
    "            sigmaz = 0\n",
    "        x = (x-(1-alphas[t])/(sqrt(1-alpha_bars[t]))*model(x,t*torch.ones(x.shape[0],dtype=torch.long,device=device)))/(sqrt(alphas[t]))+sigmaz\n",
    "        # x = torch.clamp(x,0,1)\n",
    "    grid = torchvision.utils.make_grid(post_process(x).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize(model,save_dir):\n",
    "    interval = (T-1) // 20\n",
    "    x = torch.randn([10,784]).to(device)\n",
    "    x_history = []\n",
    "    for t in range(T-1,-1,-1):\n",
    "        sigmaz = torch.randn_like(x)*((betas[t])**0.5).to(device)\n",
    "        if t==0:\n",
    "            sigmaz = 0\n",
    "        x = (x-(1-alphas[t])/(sqrt(1-alpha_bars[t]))*model(x,t*torch.ones(x.shape[0],dtype=torch.long,device=device)))/(sqrt(alphas[t]))+sigmaz\n",
    "        # x = torch.clamp(x,0,1)\n",
    "        x_history.append(x)\n",
    "    # print('cat.shape',torch.cat(x_history,dim=0).shape)\n",
    "    grid = torchvision.utils.make_grid(post_process(torch.stack(x_history,dim=0)[::interval,...]).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "    print('Saved visualize to',os.path.abspath(save_dir))\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_denoise(model,save_dir):\n",
    "    # get 10 images from the dataset\n",
    "    x,_ = next(iter(valid_loader))\n",
    "    x = x[:20,...].reshape(20,784).to(device)\n",
    "    x = pre_process(x)\n",
    "    t = torch.tensor([i * T // 20 for i in range(20)],dtype=torch.long,device=device)\n",
    "    noise = torch.randn_like(x).reshape(-1,784)\n",
    "    v1 = (sqrt(alpha_bars[t]).reshape(-1,1)*x).reshape(-1,784)\n",
    "    v2 = sqrt(1-alpha_bars[t]).reshape(-1,1)*noise\n",
    "    x_corr = v1+v2\n",
    "    est = model(x_corr,t)\n",
    "    x_rec = (x_corr - sqrt(1-alpha_bars[t]).reshape(-1,1)*est)/(sqrt(alpha_bars[t])).reshape(-1,1)\n",
    "    grid_orig = torchvision.utils.make_grid(post_process(x).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    grid_corr = torchvision.utils.make_grid(post_process(x_corr).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    grid_rec = torchvision.utils.make_grid(post_process(x_rec).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    # add noise level infomation to the image\n",
    "    noise_level = (1-alpha_bars[t]).reshape(-1).tolist()\n",
    "    ori_mse = noise.pow(2).mean(dim=1).reshape(-1).tolist()\n",
    "    mse = ((est-noise)**2).mean(dim=1).reshape(-1).tolist()\n",
    "    print(noise_level)\n",
    "    print(ori_mse)\n",
    "    print(mse)\n",
    "    grid = torch.cat([grid_orig,grid_corr,grid_rec],dim=1)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "    print('Saved denoise to',os.path.abspath(save_dir))\n",
    "\n",
    "def plot_loss(losses,save_dir):\n",
    "    losses_vals, t_vals = zip(*losses)\n",
    "    losses_vals = torch.cat(losses_vals,dim=0)\n",
    "    t_vals = torch.cat(t_vals,dim=0)\n",
    "    # print('t_vals',t_vals)\n",
    "    # print('losses_vals',losses_vals)\n",
    "\n",
    "    results = []\n",
    "    for t in range(T):\n",
    "        this_t = abs(t_vals.float()-float(t))<0.5\n",
    "        results.append(torch.sum(torch.where(this_t,losses_vals,torch.tensor(0.,device=device))).item() / (torch.sum(this_t.float())+1e-3).item())\n",
    "    plt.plot(results)\n",
    "    plt.ylim(0,max(results)* 1.2)\n",
    "    plt.savefig(save_dir)\n",
    "    plt.close()\n",
    "    # weights = (torch.tensor(results,device=device)) # weights\n",
    "    weights = torch.ones(T,dtype=torch.float,device=device)\n",
    "    # weights[:10]=0\n",
    "    # weights[10:80] /= 100\n",
    "    return weights\n",
    "\n",
    "def pre_process(x):\n",
    "    # do the logit transform\n",
    "    # return (torch.log(x+1e-3)-torch.log(1-x+1e-3))\n",
    "    return x*2-1 #MODIFIED\n",
    "    return (x+1)/2\n",
    "\n",
    "def post_process(x):\n",
    "    # return torch.sigmoid(x)\n",
    "    return (x+1)/2 #MODIFIED\n",
    "    return x*2-1\n",
    "\n",
    "def train(epochs,model:DDPM,optimizer,eval_interval=5):\n",
    "    global weights\n",
    "    for epoch in range(epochs):\n",
    "        # print('weights normalized:',weights/weights.sum())\n",
    "        all_ts = torch.distributions.Categorical(weights).sample((50000,))\n",
    "        cnt = 0\n",
    "        model.train()\n",
    "        with tqdm(train_loader) as bar:\n",
    "            losses = []\n",
    "            for x,_ in bar:\n",
    "                cnt += x.shape[0]\n",
    "                x = pre_process(x.to(device))\n",
    "                epss = torch.randn_like(x).reshape(-1,784).to(device)\n",
    "                # ts = torch.randint(0,T,(x.shape[0],),device=device,dtype=torch.long)\n",
    "                ts = all_ts[cnt-x.shape[0]:cnt]\n",
    "                alpha_tbars = alpha_bars[ts]\n",
    "                value = (sqrt(alpha_tbars).reshape(-1,1,1,1)*x).reshape(-1,784)+sqrt(1-alpha_tbars).reshape(-1,1)*epss\n",
    "                out = model(value,ts) # [batch,784]\n",
    "                # loss = ((epss-out).pow(2).mean(dim=-1) * (betas[ts])/(2*alphas[ts]*(1-alpha_tbars))).sum(dim=0)\n",
    "                loss = ((epss-out).pow(2).mean(dim=-1)).mean(dim=0)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "                bar.set_description('epoch {}, loss {:.4f}'.format(epoch,sum(losses)/len(losses)))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(valid_loader) as bar:\n",
    "                mses = []\n",
    "                losses = []\n",
    "                losses_for_t = []\n",
    "                for x,_ in bar:\n",
    "                    x = pre_process(x.to(device))\n",
    "                    epss = torch.randn_like(x).reshape(-1,784).to(device)\n",
    "                    ts = torch.randint(0,T,(x.shape[0],),device=device,dtype=torch.long)\n",
    "                    # print(ts)\n",
    "                    alpha_tbars = alpha_bars[ts]\n",
    "                    value = (sqrt(alpha_tbars).reshape(-1,1,1,1)*x).reshape(-1,784)+sqrt(1-alpha_tbars).reshape(-1,1)*epss\n",
    "                    out = model(value,ts)\n",
    "                    mse = F.mse_loss(epss,out)\n",
    "                    mses.append(mse.item())\n",
    "                    loss = ((epss-out).pow(2).mean(dim=-1))\n",
    "                    # loss = (epss-out).pow(2).mean(dim=-1)\n",
    "                    losses_for_t.append((loss.clone().detach(),ts))\n",
    "                    loss = (loss).mean(dim=0)\n",
    "                    losses.append(loss.item())\n",
    "                    bar.set_description('epoch {}, MSE {:.4f}, [Valid] {:.4f}'.format(epoch,sum(mses)/len(mses),sum(losses)/len(losses)))\n",
    "                    \n",
    "        if epoch % eval_interval == 0:\n",
    "            visualize(model,save_dir=os.path.join('./samples',f'diffuse_epoch_{epoch}.png'))\n",
    "            sample(model,save_dir=os.path.join('./samples',f'sample_epoch_{epoch}.png'))\n",
    "            # visualize_denoise(model,save_dir=os.path.join('./samples',f'denoise_epoch_{epoch}.png'))\n",
    "            weights = plot_loss(losses_for_t,save_dir=os.path.join('./samples',f'loss_epoch_{epoch}.png'))\n",
    "            torch.save(model,os.path.join('./samples',f'epoch_{epoch}.pt'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = DDPM().to(device)\n",
    "    print('Number parameters of the model:', sum(p.numel() for p in model.parameters()))\n",
    "    print('Model strcuture:',model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    os.makedirs('./samples',exist_ok=True)\n",
    "    sample(model,save_dir=os.path.join('./samples',f'init.png'))\n",
    "    visualize(model,save_dir=os.path.join('./samples',f'init_visualize.png'))\n",
    "    train(200,model,optimizer,eval_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
