{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 22 08:16:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              40W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/bin/python\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!which python | grep DYY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(3407)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class Res(nn.Module):\n",
    "#     def __init__(self, channel, kernel_size=3,x_size = 28) -> None:\n",
    "#         super().__init__()\n",
    "#         self.channel = channel\n",
    "#         self.conv1=nn.Sequential(\n",
    "#             nn.Conv2d(channel,channel,(kernel_size,kernel_size),padding=(kernel_size-1)//2),\n",
    "#             nn.BatchNorm2d(channel),\n",
    "#         )\n",
    "#         self.conv2= nn.Sequential(\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(channel,channel,(kernel_size,kernel_size),padding=(kernel_size-1)//2),\n",
    "#         )\n",
    "#         self.t_net = nn.Linear(256,self.channel*x_size*x_size)\n",
    "#     def forward(self,x,t):\n",
    "#         res = x.clone()\n",
    "#         x = self.conv1(x)\n",
    "#         x = x + self.t_net(t).reshape(x.shape)\n",
    "#         x = self.conv2(x) + res\n",
    "#         return x\n",
    "\n",
    "# class SinousEmbedding(nn.Module):\n",
    "#     def __init__(self, dim) -> None:\n",
    "#         super().__init__()\n",
    "#         assert dim%2==0,NotImplementedError()\n",
    "#         self.angles = (10000**(-2/dim))**torch.arange(1,dim//2+1,1).cuda()\n",
    "#         self.angles.requires_grad_(False)\n",
    "#     def forward(self,x):\n",
    "#         angles = torch.einsum('m,i->im',self.angles,x)\n",
    "#         return torch.cat((torch.sin(angles),torch.cos(angles)),dim=1)\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, channel,hidden_size=512) -> None:\n",
    "#         super().__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.q_proj = nn.Linear(channel,hidden_size)\n",
    "#         self.k_proj = nn.Linear(channel,hidden_size)\n",
    "#         self.v_proj = nn.Linear(channel,hidden_size)\n",
    "#         self.out_proj = nn.Linear(hidden_size,channel)\n",
    "#     def forward(self,x):\n",
    "#         res = x.clone()\n",
    "#         batch,channel = x.shape[:2]\n",
    "#         seq_len = x.shape[-1]\n",
    "#         x = x.reshape(batch,channel,seq_len*seq_len).transpose(1,2)\n",
    "#         v = self.v_proj(x)\n",
    "#         q = self.q_proj(x)\n",
    "#         k = self.k_proj(x)\n",
    "#         att_sc = torch.einsum('bic,bjc->bij',q,k)*((self.hidden_size)**-0.5)\n",
    "#         att_sc = torch.softmax(att_sc,dim=-1)\n",
    "#         att_out = torch.einsum('bij,bjc->bic',att_sc,v)\n",
    "#         ans = self.out_proj(att_out).transpose(1,2).reshape(batch,channel,seq_len,seq_len)\n",
    "#         return ans+res\n",
    "\n",
    "\n",
    "# class ResBlockWithAttention(nn.Module):\n",
    "#     def __init__(self, in_channel, out_channel,x_size,with_attention=True,kernel_size=3) -> None:\n",
    "#         super().__init__()\n",
    "#         self.conv=nn.Sequential(\n",
    "#             nn.Conv2d(in_channel,out_channel,(kernel_size,kernel_size),padding=(kernel_size-1)//2),\n",
    "#             nn.BatchNorm2d(out_channel),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.reses = nn.ModuleList(\n",
    "#             [Res(out_channel,kernel_size,x_size) for _ in range(4)]\n",
    "#         )\n",
    "#         if with_attention:\n",
    "#             self.attentions = nn.ModuleList(\n",
    "#                 [Attention(out_channel) for _ in range(4)]\n",
    "#             )\n",
    "        \n",
    "#     def forward(self,x,t):\n",
    "#         x = self.conv(x)\n",
    "#         for i,ly in enumerate(self.reses):\n",
    "#             x = ly(x,t)\n",
    "#             if hasattr(self,'attentions'):\n",
    "#                 x = self.attentions[i](x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class DDPM(nn.Module):\n",
    "#     def __init__(self, *args, **kwargs) -> None:\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.in_size = 28 * 28\n",
    "#         self.t_embedding = SinousEmbedding(dim=256)\n",
    "#         self.up= nn.ModuleList([\n",
    "#             ResBlockWithAttention(1,64,x_size=28,with_attention=False), # 28 28\n",
    "#             nn.MaxPool2d(kernel_size=(2,2)), # 14 14\n",
    "#             ResBlockWithAttention(64,128,x_size=14), # 14 14\n",
    "#             nn.MaxPool2d(kernel_size=(2,2)), # 7 7\n",
    "#             ResBlockWithAttention(128,256,x_size=7), # 7 7\n",
    "#         ])\n",
    "#         self.middle = nn.ModuleList([\n",
    "#             nn.Conv2d(256,256,kernel_size=(5,5),padding=2),\n",
    "#             nn.ReLU(),\n",
    "#             Attention(256)\n",
    "#         ])\n",
    "#         self.down= nn.ModuleList([\n",
    "#             ResBlockWithAttention(512,128,x_size=7), # 7 7\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             ResBlockWithAttention(256,64,x_size=14),\n",
    "#             nn.Upsample(scale_factor=2),\n",
    "#             ResBlockWithAttention(128,1,x_size=28,with_attention=False),       \n",
    "#         ])\n",
    "\n",
    "#     def forward(self,x,t):\n",
    "#         x = x.reshape(-1,1,28,28)\n",
    "#         ttensor = self.t_embedding(t)\n",
    "#         batch = x.shape[0]\n",
    "#         ups = []\n",
    "#         for i,ly in enumerate(self.up):\n",
    "#             if isinstance(ly,ResBlockWithAttention):\n",
    "#                 x = ly(x,ttensor)\n",
    "#                 cl = x.clone()\n",
    "#                 ups.append(cl)\n",
    "#             else:\n",
    "#                 x = ly(x)\n",
    "#         for ly in self.middle:\n",
    "#             x = ly(x)\n",
    "#         for ly in self.down:\n",
    "#             if isinstance(ly,ResBlockWithAttention):\n",
    "#                 old = ups.pop()\n",
    "#                 x = ly(torch.cat((x,old),dim=1),ttensor)\n",
    "#             else:\n",
    "#                 x = ly(x)\n",
    "#         x = x.reshape(batch,-1)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class SinousEmbedding(nn.Module):\n",
    "#     def __init__(self, dim) -> None:\n",
    "#         super().__init__()\n",
    "#         assert dim%2==0,NotImplementedError()\n",
    "#         self.angles = (1000.**(-2/dim))**torch.arange(1,dim//2+1,1,dtype=torch.float).cuda()\n",
    "#         self.angles.requires_grad_(False)\n",
    "#     def forward(self,x):\n",
    "#         angles = torch.einsum('m,i->im',self.angles,x.float())\n",
    "#         return torch.cat((torch.sin(angles),torch.cos(angles)),dim=1)\n",
    "\n",
    "# class DDPM(nn.Module):\n",
    "#     def __init__(self, *args, **kwargs) -> None:\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.in_size = 28 * 28\n",
    "#         self.t_embedding_dim = 256\n",
    "#         self.t_embedding = SinousEmbedding(dim=self.t_embedding_dim)\n",
    "#         self.up = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(784+self.t_embedding_dim,64),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(64,32),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#             # nn.Sequential(\n",
    "#             #     nn.Linear(256,256),\n",
    "#             #     # nn.LeakyReLU(0.1),\n",
    "#             # ),\n",
    "#         ])\n",
    "#         self.middle = nn.ModuleList([\n",
    "#             nn.Linear(32,32),\n",
    "#             # nn.LeakyReLU(0.1),\n",
    "#         ])\n",
    "#         self.down= nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(32,32),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#             # nn.Sequential(\n",
    "#             #     nn.Linear(256,256),\n",
    "#             #     # nn.LeakyReLU(0.1),\n",
    "#             # ),\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(32,64),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#         ])\n",
    "#         self.end_mlp = nn.Linear(64,784)\n",
    "#         self.apply_init()\n",
    "\n",
    "#     def apply_init(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.xavier_normal_(m.weight)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     def forward(self,x,t):\n",
    "#         x = x.reshape(-1,784)\n",
    "#         ttensor = self.t_embedding(t) # [batch, 256]\n",
    "#         batch = x.shape[0]\n",
    "#         xc = x.clone()\n",
    "#         ups = []\n",
    "#         x = torch.cat((x,ttensor),dim=-1)\n",
    "#         for ly in self.up:\n",
    "#             x = ly(x)\n",
    "#             ups.append(x.clone())\n",
    "#         for ly in self.middle:\n",
    "#             x = ly(x)\n",
    "#         for ly in self.down:\n",
    "#             x = ly(x) + ups.pop()\n",
    "\n",
    "#         x = self.end_mlp(x)\n",
    "#         x = (x + xc)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_range(x):\n",
    "    print('min:',x.min().item(),'max:',x.max().item(),'std:',x.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SinousEmbedding(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        assert dim%2==0,NotImplementedError()\n",
    "        self.angles = (10000.**(-2/dim))**torch.arange(1,dim//2+1,1,dtype=torch.float).cuda()\n",
    "        self.angles.requires_grad_(False)\n",
    "    def forward(self,x):\n",
    "        angles = torch.einsum('m,i->im',self.angles,x.float())\n",
    "        return torch.cat((torch.sin(angles),torch.cos(angles)),dim=1)\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,channels=128,kernel_size=3,t_dim=64) -> None:\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "        self.conv1 = nn.Conv2d(channels,channels,kernel_size=kernel_size,padding=kernel_size//2)\n",
    "        self.t_net = nn.Linear(t_dim,channels)\n",
    "        self.conv2 = nn.Conv2d(channels,channels,kernel_size=kernel_size,padding=kernel_size//2)\n",
    "        self.conv2.weight.data.fill_(0)\n",
    "        self.conv2.bias.data.fill_(0)\n",
    "    \n",
    "    def forward(self,x,t):\n",
    "        xc = x.clone()\n",
    "        x = self.norm(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
    "        x = self.conv1(x.relu())\n",
    "        x = x + self.t_net(t).unsqueeze(-1).unsqueeze(-1).expand(t.shape[0],x.shape[1],x.shape[2],x.shape[3])\n",
    "        x = self.conv2(x.relu())\n",
    "        return x + xc\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self,channels=128,attn_dim=32) -> None:\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "        self.Q = nn.Conv2d(channels,attn_dim,kernel_size=1,bias=False)\n",
    "        self.K = nn.Conv2d(channels,attn_dim,kernel_size=1,bias=False)\n",
    "        self.V = nn.Conv2d(channels,channels,kernel_size=1,bias=False)\n",
    "        self.out_proj = nn.Conv2d(channels,channels,kernel_size=1)\n",
    "        self.Q.weight.data.normal_(0,0.02)\n",
    "        self.K.weight.data.normal_(0,0.02)\n",
    "        self.V.weight.data.normal_(0,0.02)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        xc = x.clone()\n",
    "        x = self.norm(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
    "        q = self.Q(x)\n",
    "        k = self.K(x)\n",
    "        v = self.V(x)\n",
    "        attn_score = torch.einsum('bchw,bcxy->bhwxy',q,k).reshape(q.shape[0],*q.shape[-2:],-1)\n",
    "        attn_score = attn_score.softmax(dim=-1).reshape(q.shape[0],*q.shape[-2:],*k.shape[-2:])\n",
    "        return self.out_proj(torch.einsum('bhwxy,bcxy->bchw',attn_score,v)) + xc\n",
    "\n",
    "class F_x_t(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,out_channels,out_size,kernel_size=3,t_shape=64,attn=False,residual=True) -> None:\n",
    "        super().__init__()\n",
    "        # self.t_channels = out_channels // 2\n",
    "        # self.conv_channels = out_channels - self.t_channels\n",
    "        self.t_channels = out_channels\n",
    "        self.conv_channels = out_channels\n",
    "        self.conv = nn.Conv2d(in_channels, self.conv_channels, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        self.out_size = out_size\n",
    "        self.fc = nn.Linear(t_shape, self.t_channels)\n",
    "        self.attn = attn\n",
    "        self.residual = residual\n",
    "        if attn:\n",
    "            self.attentions = nn.ModuleList([Attention(channels=self.conv_channels,attn_dim=out_channels) for _ in range(2)])\n",
    "        if residual:\n",
    "            self.ress = nn.ModuleList([ResidualBlock(channels=out_channels,kernel_size=kernel_size,t_dim=t_shape) for _ in range(2)])\n",
    "        # self.fc = nn.Embedding(t_shape, self.t_num)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if self.t_channels == 0:\n",
    "            raise NotImplementedError()\n",
    "            return self.conv(x)\n",
    "        # return torch.cat([self.conv(x),self.fc(t).unsqueeze(-1).unsqueeze(-1).expand(t.shape[0], self.t_channels, self.out_size, self.out_size)],dim=1).relu()\n",
    "        val = self.conv(x) + self.fc(t).unsqueeze(-1).unsqueeze(-1).expand(t.shape[0], self.t_channels, self.out_size, self.out_size)\n",
    "        if self.residual:\n",
    "            for i,ly in enumerate(self.ress):\n",
    "                val = ly(val,t)\n",
    "                if self.attn:\n",
    "                    val = self.attentions[i](val)\n",
    "        return val\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.t_embedding_dim = 32\n",
    "        self.t_embedding = SinousEmbedding(dim=self.t_embedding_dim)\n",
    "        self.up= nn.ModuleList([\n",
    "            F_x_t(in_channels=1,out_channels=32,out_size=32,kernel_size=3,t_shape=self.t_embedding_dim),\n",
    "            F_x_t(in_channels=32,out_channels=64,out_size=16,kernel_size=3,t_shape=self.t_embedding_dim,attn=True),\n",
    "            F_x_t(in_channels=64,out_channels=128,out_size=8,kernel_size=3,t_shape=self.t_embedding_dim,attn=True),\n",
    "            # ResidualBlock(channels=128,kernel_size=3,t_dim=self.t_embedding_dim),\n",
    "            # F_x_t(in_channels=128,out_channels=128,out_size=4,kernel_size=1,t_shape=self.t_embedding_dim),\n",
    "        ])\n",
    "        self.middle = nn.ModuleList([\n",
    "            # nn.Identity()\n",
    "            F_x_t(in_channels=128,out_channels=128,out_size=4,kernel_size=3,t_shape=self.t_embedding_dim,attn=True),\n",
    "            # ResidualBlock(channels=128,kernel_size=3,t_dim=self.t_embedding_dim),\n",
    "            # F_x_t(in_channels=128,out_channels=128,out_size=4,kernel_size=1,t_shape=self.t_embedding_dim,attn=False),\n",
    "        ])\n",
    "        self.down= nn.ModuleList([\n",
    "            # F_x_t(in_channels=128,out_channels=128,out_size=2,kernel_size=1,t_shape=self.t_embedding_dim),\n",
    "            F_x_t(in_channels=128,out_channels=64,out_size=8,kernel_size=3,t_shape=self.t_embedding_dim,attn=True),\n",
    "            F_x_t(in_channels=64,out_channels=32,out_size=16,kernel_size=3,t_shape=self.t_embedding_dim,attn=True),\n",
    "            F_x_t(in_channels=32,out_channels=16,out_size=32,kernel_size=3,t_shape=self.t_embedding_dim),\n",
    "        ])\n",
    "        # self.end_mlp = nn.Conv2d(32,1,kernel_size=3,padding=1)\n",
    "        self.end_mlp = nn.Conv2d(16,1,kernel_size=1)\n",
    "\n",
    "    def forward(self,x,t):\n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        x = F.pad(x,(2,2,2,2),mode='constant',value=0)\n",
    "        ttensor = self.t_embedding(t) # [batch, 256]\n",
    "        batch = x.shape[0]\n",
    "        # xc = x.clone()            print(attn_score.shape)\n",
    "\n",
    "        ups = []\n",
    "        for i,ly in enumerate(self.up):\n",
    "            x = ly(x,ttensor)\n",
    "            ups.append(x.clone()) # append: 28x28, 14x14\n",
    "            x = nn.AvgPool2d(2)(x)\n",
    "            # print('up,',i);print_range(x)\n",
    "        for i,ly in enumerate(self.middle):\n",
    "            # x = ly(x,ttensor)\n",
    "            x = ly(x,ttensor)\n",
    "            # print('middle,',i);print_range(x)\n",
    "        for i,ly in enumerate(self.down):\n",
    "            x = nn.Upsample(scale_factor=2)(x) + ups.pop() # 14x14, 28x28\n",
    "            x = ly(x,ttensor)\n",
    "            # x = nn.Upsample(scale_factor=2)(x) + ups.pop()\n",
    "            # print('down,',i);print_range(x)\n",
    "        x = self.end_mlp(x)\n",
    "        x = x[:,:,2:30,2:30]\n",
    "        return x.reshape(batch,28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended /home/zhh24/DeepLearning\n",
      "tensor([9.9996e-01, 9.9991e-01, 9.9986e-01, 9.9981e-01, 9.9975e-01, 9.9968e-01,\n",
      "        9.9961e-01, 9.9953e-01, 9.9945e-01, 9.9937e-01, 9.9928e-01, 9.9918e-01,\n",
      "        9.9908e-01, 9.9898e-01, 9.9887e-01, 9.9876e-01, 9.9864e-01, 9.9851e-01,\n",
      "        9.9839e-01, 9.9825e-01, 9.9811e-01, 9.9797e-01, 9.9782e-01, 9.9767e-01,\n",
      "        9.9751e-01, 9.9735e-01, 9.9718e-01, 9.9701e-01, 9.9683e-01, 9.9665e-01,\n",
      "        9.9647e-01, 9.9627e-01, 9.9608e-01, 9.9588e-01, 9.9567e-01, 9.9546e-01,\n",
      "        9.9525e-01, 9.9502e-01, 9.9480e-01, 9.9457e-01, 9.9434e-01, 9.9410e-01,\n",
      "        9.9385e-01, 9.9360e-01, 9.9335e-01, 9.9309e-01, 9.9283e-01, 9.9256e-01,\n",
      "        9.9229e-01, 9.9201e-01, 9.9172e-01, 9.9144e-01, 9.9115e-01, 9.9085e-01,\n",
      "        9.9055e-01, 9.9024e-01, 9.8993e-01, 9.8961e-01, 9.8929e-01, 9.8897e-01,\n",
      "        9.8864e-01, 9.8830e-01, 9.8796e-01, 9.8762e-01, 9.8727e-01, 9.8691e-01,\n",
      "        9.8656e-01, 9.8619e-01, 9.8582e-01, 9.8545e-01, 9.8507e-01, 9.8469e-01,\n",
      "        9.8430e-01, 9.8391e-01, 9.8352e-01, 9.8312e-01, 9.8271e-01, 9.8230e-01,\n",
      "        9.8188e-01, 9.8146e-01, 9.8104e-01, 9.8061e-01, 9.8018e-01, 9.7974e-01,\n",
      "        9.7930e-01, 9.7885e-01, 9.7840e-01, 9.7794e-01, 9.7748e-01, 9.7701e-01,\n",
      "        9.7654e-01, 9.7606e-01, 9.7558e-01, 9.7510e-01, 9.7461e-01, 9.7412e-01,\n",
      "        9.7362e-01, 9.7311e-01, 9.7261e-01, 9.7209e-01, 9.7158e-01, 9.7105e-01,\n",
      "        9.7053e-01, 9.7000e-01, 9.6946e-01, 9.6892e-01, 9.6838e-01, 9.6783e-01,\n",
      "        9.6727e-01, 9.6672e-01, 9.6615e-01, 9.6559e-01, 9.6502e-01, 9.6444e-01,\n",
      "        9.6386e-01, 9.6327e-01, 9.6268e-01, 9.6209e-01, 9.6149e-01, 9.6089e-01,\n",
      "        9.6028e-01, 9.5967e-01, 9.5905e-01, 9.5843e-01, 9.5780e-01, 9.5717e-01,\n",
      "        9.5654e-01, 9.5590e-01, 9.5526e-01, 9.5461e-01, 9.5396e-01, 9.5330e-01,\n",
      "        9.5264e-01, 9.5198e-01, 9.5131e-01, 9.5063e-01, 9.4995e-01, 9.4927e-01,\n",
      "        9.4858e-01, 9.4789e-01, 9.4720e-01, 9.4650e-01, 9.4579e-01, 9.4508e-01,\n",
      "        9.4437e-01, 9.4365e-01, 9.4293e-01, 9.4220e-01, 9.4147e-01, 9.4074e-01,\n",
      "        9.4000e-01, 9.3926e-01, 9.3851e-01, 9.3776e-01, 9.3700e-01, 9.3624e-01,\n",
      "        9.3548e-01, 9.3471e-01, 9.3393e-01, 9.3316e-01, 9.3238e-01, 9.3159e-01,\n",
      "        9.3080e-01, 9.3001e-01, 9.2921e-01, 9.2841e-01, 9.2760e-01, 9.2679e-01,\n",
      "        9.2597e-01, 9.2516e-01, 9.2433e-01, 9.2351e-01, 9.2267e-01, 9.2184e-01,\n",
      "        9.2100e-01, 9.2016e-01, 9.1931e-01, 9.1846e-01, 9.1760e-01, 9.1674e-01,\n",
      "        9.1588e-01, 9.1501e-01, 9.1414e-01, 9.1326e-01, 9.1238e-01, 9.1150e-01,\n",
      "        9.1061e-01, 9.0972e-01, 9.0882e-01, 9.0792e-01, 9.0702e-01, 9.0611e-01,\n",
      "        9.0520e-01, 9.0428e-01, 9.0336e-01, 9.0244e-01, 9.0151e-01, 9.0058e-01,\n",
      "        8.9965e-01, 8.9871e-01, 8.9776e-01, 8.9682e-01, 8.9587e-01, 8.9491e-01,\n",
      "        8.9395e-01, 8.9299e-01, 8.9202e-01, 8.9105e-01, 8.9008e-01, 8.8910e-01,\n",
      "        8.8812e-01, 8.8714e-01, 8.8615e-01, 8.8516e-01, 8.8416e-01, 8.8316e-01,\n",
      "        8.8216e-01, 8.8115e-01, 8.8014e-01, 8.7912e-01, 8.7810e-01, 8.7708e-01,\n",
      "        8.7606e-01, 8.7503e-01, 8.7399e-01, 8.7296e-01, 8.7192e-01, 8.7087e-01,\n",
      "        8.6982e-01, 8.6877e-01, 8.6772e-01, 8.6666e-01, 8.6560e-01, 8.6453e-01,\n",
      "        8.6346e-01, 8.6239e-01, 8.6132e-01, 8.6024e-01, 8.5915e-01, 8.5807e-01,\n",
      "        8.5698e-01, 8.5588e-01, 8.5479e-01, 8.5369e-01, 8.5258e-01, 8.5147e-01,\n",
      "        8.5036e-01, 8.4925e-01, 8.4813e-01, 8.4701e-01, 8.4589e-01, 8.4476e-01,\n",
      "        8.4363e-01, 8.4250e-01, 8.4136e-01, 8.4022e-01, 8.3907e-01, 8.3793e-01,\n",
      "        8.3677e-01, 8.3562e-01, 8.3446e-01, 8.3330e-01, 8.3214e-01, 8.3097e-01,\n",
      "        8.2980e-01, 8.2863e-01, 8.2745e-01, 8.2627e-01, 8.2509e-01, 8.2390e-01,\n",
      "        8.2271e-01, 8.2152e-01, 8.2033e-01, 8.1913e-01, 8.1793e-01, 8.1672e-01,\n",
      "        8.1551e-01, 8.1430e-01, 8.1309e-01, 8.1187e-01, 8.1065e-01, 8.0943e-01,\n",
      "        8.0820e-01, 8.0697e-01, 8.0574e-01, 8.0451e-01, 8.0327e-01, 8.0203e-01,\n",
      "        8.0078e-01, 7.9954e-01, 7.9829e-01, 7.9703e-01, 7.9578e-01, 7.9452e-01,\n",
      "        7.9326e-01, 7.9200e-01, 7.9073e-01, 7.8946e-01, 7.8819e-01, 7.8691e-01,\n",
      "        7.8563e-01, 7.8435e-01, 7.8307e-01, 7.8178e-01, 7.8049e-01, 7.7920e-01,\n",
      "        7.7791e-01, 7.7661e-01, 7.7531e-01, 7.7401e-01, 7.7270e-01, 7.7139e-01,\n",
      "        7.7008e-01, 7.6877e-01, 7.6745e-01, 7.6614e-01, 7.6481e-01, 7.6349e-01,\n",
      "        7.6216e-01, 7.6084e-01, 7.5950e-01, 7.5817e-01, 7.5683e-01, 7.5550e-01,\n",
      "        7.5415e-01, 7.5281e-01, 7.5147e-01, 7.5012e-01, 7.4877e-01, 7.4741e-01,\n",
      "        7.4606e-01, 7.4470e-01, 7.4334e-01, 7.4197e-01, 7.4061e-01, 7.3924e-01,\n",
      "        7.3787e-01, 7.3650e-01, 7.3513e-01, 7.3375e-01, 7.3237e-01, 7.3099e-01,\n",
      "        7.2960e-01, 7.2822e-01, 7.2683e-01, 7.2544e-01, 7.2405e-01, 7.2265e-01,\n",
      "        7.2126e-01, 7.1986e-01, 7.1846e-01, 7.1705e-01, 7.1565e-01, 7.1424e-01,\n",
      "        7.1283e-01, 7.1142e-01, 7.1001e-01, 7.0859e-01, 7.0717e-01, 7.0575e-01,\n",
      "        7.0433e-01, 7.0291e-01, 7.0148e-01, 7.0005e-01, 6.9863e-01, 6.9719e-01,\n",
      "        6.9576e-01, 6.9433e-01, 6.9289e-01, 6.9145e-01, 6.9001e-01, 6.8857e-01,\n",
      "        6.8712e-01, 6.8567e-01, 6.8423e-01, 6.8278e-01, 6.8132e-01, 6.7987e-01,\n",
      "        6.7842e-01, 6.7696e-01, 6.7550e-01, 6.7404e-01, 6.7258e-01, 6.7111e-01,\n",
      "        6.6965e-01, 6.6818e-01, 6.6671e-01, 6.6524e-01, 6.6377e-01, 6.6230e-01,\n",
      "        6.6082e-01, 6.5935e-01, 6.5787e-01, 6.5639e-01, 6.5491e-01, 6.5342e-01,\n",
      "        6.5194e-01, 6.5045e-01, 6.4897e-01, 6.4748e-01, 6.4599e-01, 6.4450e-01,\n",
      "        6.4300e-01, 6.4151e-01, 6.4001e-01, 6.3852e-01, 6.3702e-01, 6.3552e-01,\n",
      "        6.3402e-01, 6.3252e-01, 6.3101e-01, 6.2951e-01, 6.2800e-01, 6.2649e-01,\n",
      "        6.2499e-01, 6.2348e-01, 6.2196e-01, 6.2045e-01, 6.1894e-01, 6.1742e-01,\n",
      "        6.1591e-01, 6.1439e-01, 6.1287e-01, 6.1136e-01, 6.0984e-01, 6.0831e-01,\n",
      "        6.0679e-01, 6.0527e-01, 6.0374e-01, 6.0222e-01, 6.0069e-01, 5.9917e-01,\n",
      "        5.9764e-01, 5.9611e-01, 5.9458e-01, 5.9305e-01, 5.9152e-01, 5.8998e-01,\n",
      "        5.8845e-01, 5.8692e-01, 5.8538e-01, 5.8384e-01, 5.8231e-01, 5.8077e-01,\n",
      "        5.7923e-01, 5.7769e-01, 5.7615e-01, 5.7461e-01, 5.7307e-01, 5.7153e-01,\n",
      "        5.6998e-01, 5.6844e-01, 5.6690e-01, 5.6535e-01, 5.6381e-01, 5.6226e-01,\n",
      "        5.6071e-01, 5.5916e-01, 5.5762e-01, 5.5607e-01, 5.5452e-01, 5.5297e-01,\n",
      "        5.5142e-01, 5.4987e-01, 5.4832e-01, 5.4677e-01, 5.4521e-01, 5.4366e-01,\n",
      "        5.4211e-01, 5.4056e-01, 5.3900e-01, 5.3745e-01, 5.3589e-01, 5.3434e-01,\n",
      "        5.3278e-01, 5.3123e-01, 5.2967e-01, 5.2812e-01, 5.2656e-01, 5.2500e-01,\n",
      "        5.2345e-01, 5.2189e-01, 5.2033e-01, 5.1878e-01, 5.1722e-01, 5.1566e-01,\n",
      "        5.1410e-01, 5.1255e-01, 5.1099e-01, 5.0943e-01, 5.0787e-01, 5.0631e-01,\n",
      "        5.0475e-01, 5.0319e-01, 5.0164e-01, 5.0008e-01, 4.9852e-01, 4.9696e-01,\n",
      "        4.9540e-01, 4.9384e-01, 4.9229e-01, 4.9073e-01, 4.8917e-01, 4.8761e-01,\n",
      "        4.8605e-01, 4.8449e-01, 4.8294e-01, 4.8138e-01, 4.7982e-01, 4.7826e-01,\n",
      "        4.7671e-01, 4.7515e-01, 4.7359e-01, 4.7204e-01, 4.7048e-01, 4.6893e-01,\n",
      "        4.6737e-01, 4.6582e-01, 4.6426e-01, 4.6271e-01, 4.6115e-01, 4.5960e-01,\n",
      "        4.5805e-01, 4.5649e-01, 4.5494e-01, 4.5339e-01, 4.5184e-01, 4.5029e-01,\n",
      "        4.4874e-01, 4.4719e-01, 4.4564e-01, 4.4409e-01, 4.4254e-01, 4.4099e-01,\n",
      "        4.3944e-01, 4.3790e-01, 4.3635e-01, 4.3480e-01, 4.3326e-01, 4.3172e-01,\n",
      "        4.3017e-01, 4.2863e-01, 4.2709e-01, 4.2555e-01, 4.2400e-01, 4.2246e-01,\n",
      "        4.2093e-01, 4.1939e-01, 4.1785e-01, 4.1631e-01, 4.1478e-01, 4.1324e-01,\n",
      "        4.1171e-01, 4.1017e-01, 4.0864e-01, 4.0711e-01, 4.0558e-01, 4.0405e-01,\n",
      "        4.0252e-01, 4.0099e-01, 3.9946e-01, 3.9794e-01, 3.9641e-01, 3.9489e-01,\n",
      "        3.9336e-01, 3.9184e-01, 3.9032e-01, 3.8880e-01, 3.8728e-01, 3.8576e-01,\n",
      "        3.8425e-01, 3.8273e-01, 3.8122e-01, 3.7970e-01, 3.7819e-01, 3.7668e-01,\n",
      "        3.7517e-01, 3.7366e-01, 3.7215e-01, 3.7065e-01, 3.6914e-01, 3.6764e-01,\n",
      "        3.6614e-01, 3.6464e-01, 3.6314e-01, 3.6164e-01, 3.6014e-01, 3.5865e-01,\n",
      "        3.5715e-01, 3.5566e-01, 3.5417e-01, 3.5268e-01, 3.5119e-01, 3.4970e-01,\n",
      "        3.4822e-01, 3.4673e-01, 3.4525e-01, 3.4377e-01, 3.4229e-01, 3.4081e-01,\n",
      "        3.3933e-01, 3.3786e-01, 3.3638e-01, 3.3491e-01, 3.3344e-01, 3.3197e-01,\n",
      "        3.3051e-01, 3.2904e-01, 3.2758e-01, 3.2612e-01, 3.2466e-01, 3.2320e-01,\n",
      "        3.2174e-01, 3.2028e-01, 3.1883e-01, 3.1738e-01, 3.1593e-01, 3.1448e-01,\n",
      "        3.1303e-01, 3.1159e-01, 3.1015e-01, 3.0871e-01, 3.0727e-01, 3.0583e-01,\n",
      "        3.0440e-01, 3.0296e-01, 3.0153e-01, 3.0010e-01, 2.9867e-01, 2.9725e-01,\n",
      "        2.9582e-01, 2.9440e-01, 2.9298e-01, 2.9157e-01, 2.9015e-01, 2.8874e-01,\n",
      "        2.8732e-01, 2.8592e-01, 2.8451e-01, 2.8310e-01, 2.8170e-01, 2.8030e-01,\n",
      "        2.7890e-01, 2.7750e-01, 2.7611e-01, 2.7472e-01, 2.7333e-01, 2.7194e-01,\n",
      "        2.7055e-01, 2.6917e-01, 2.6779e-01, 2.6641e-01, 2.6503e-01, 2.6366e-01,\n",
      "        2.6228e-01, 2.6091e-01, 2.5955e-01, 2.5818e-01, 2.5682e-01, 2.5546e-01,\n",
      "        2.5410e-01, 2.5274e-01, 2.5139e-01, 2.5004e-01, 2.4869e-01, 2.4734e-01,\n",
      "        2.4600e-01, 2.4466e-01, 2.4332e-01, 2.4198e-01, 2.4065e-01, 2.3932e-01,\n",
      "        2.3799e-01, 2.3667e-01, 2.3534e-01, 2.3402e-01, 2.3270e-01, 2.3139e-01,\n",
      "        2.3007e-01, 2.2876e-01, 2.2746e-01, 2.2615e-01, 2.2485e-01, 2.2355e-01,\n",
      "        2.2225e-01, 2.2096e-01, 2.1966e-01, 2.1837e-01, 2.1709e-01, 2.1580e-01,\n",
      "        2.1452e-01, 2.1325e-01, 2.1197e-01, 2.1070e-01, 2.0943e-01, 2.0816e-01,\n",
      "        2.0690e-01, 2.0564e-01, 2.0438e-01, 2.0312e-01, 2.0187e-01, 2.0062e-01,\n",
      "        1.9937e-01, 1.9813e-01, 1.9689e-01, 1.9565e-01, 1.9442e-01, 1.9318e-01,\n",
      "        1.9195e-01, 1.9073e-01, 1.8950e-01, 1.8828e-01, 1.8707e-01, 1.8585e-01,\n",
      "        1.8464e-01, 1.8344e-01, 1.8223e-01, 1.8103e-01, 1.7983e-01, 1.7863e-01,\n",
      "        1.7744e-01, 1.7625e-01, 1.7507e-01, 1.7388e-01, 1.7270e-01, 1.7153e-01,\n",
      "        1.7035e-01, 1.6918e-01, 1.6802e-01, 1.6685e-01, 1.6569e-01, 1.6454e-01,\n",
      "        1.6338e-01, 1.6223e-01, 1.6108e-01, 1.5994e-01, 1.5880e-01, 1.5766e-01,\n",
      "        1.5653e-01, 1.5540e-01, 1.5427e-01, 1.5314e-01, 1.5202e-01, 1.5091e-01,\n",
      "        1.4979e-01, 1.4868e-01, 1.4757e-01, 1.4647e-01, 1.4537e-01, 1.4427e-01,\n",
      "        1.4318e-01, 1.4209e-01, 1.4100e-01, 1.3992e-01, 1.3884e-01, 1.3776e-01,\n",
      "        1.3669e-01, 1.3562e-01, 1.3456e-01, 1.3350e-01, 1.3244e-01, 1.3138e-01,\n",
      "        1.3033e-01, 1.2928e-01, 1.2824e-01, 1.2720e-01, 1.2616e-01, 1.2513e-01,\n",
      "        1.2410e-01, 1.2307e-01, 1.2205e-01, 1.2103e-01, 1.2002e-01, 1.1901e-01,\n",
      "        1.1800e-01, 1.1700e-01, 1.1600e-01, 1.1500e-01, 1.1401e-01, 1.1302e-01,\n",
      "        1.1203e-01, 1.1105e-01, 1.1008e-01, 1.0910e-01, 1.0813e-01, 1.0717e-01,\n",
      "        1.0620e-01, 1.0525e-01, 1.0429e-01, 1.0334e-01, 1.0239e-01, 1.0145e-01,\n",
      "        1.0051e-01, 9.9576e-02, 9.8644e-02, 9.7717e-02, 9.6793e-02, 9.5874e-02,\n",
      "        9.4958e-02, 9.4046e-02, 9.3138e-02, 9.2234e-02, 9.1334e-02, 9.0438e-02,\n",
      "        8.9547e-02, 8.8659e-02, 8.7775e-02, 8.6895e-02, 8.6019e-02, 8.5147e-02,\n",
      "        8.4279e-02, 8.3415e-02, 8.2555e-02, 8.1699e-02, 8.0848e-02, 8.0000e-02,\n",
      "        7.9156e-02, 7.8317e-02, 7.7482e-02, 7.6650e-02, 7.5823e-02, 7.5000e-02,\n",
      "        7.4181e-02, 7.3366e-02, 7.2556e-02, 7.1749e-02, 7.0947e-02, 7.0149e-02,\n",
      "        6.9355e-02, 6.8565e-02, 6.7780e-02, 6.6998e-02, 6.6221e-02, 6.5448e-02,\n",
      "        6.4679e-02, 6.3915e-02, 6.3154e-02, 6.2398e-02, 6.1647e-02, 6.0899e-02,\n",
      "        6.0156e-02, 5.9417e-02, 5.8682e-02, 5.7952e-02, 5.7226e-02, 5.6504e-02,\n",
      "        5.5786e-02, 5.5073e-02, 5.4364e-02, 5.3660e-02, 5.2959e-02, 5.2264e-02,\n",
      "        5.1572e-02, 5.0885e-02, 5.0202e-02, 4.9524e-02, 4.8850e-02, 4.8180e-02,\n",
      "        4.7515e-02, 4.6854e-02, 4.6197e-02, 4.5545e-02, 4.4898e-02, 4.4254e-02,\n",
      "        4.3616e-02, 4.2981e-02, 4.2351e-02, 4.1726e-02, 4.1105e-02, 4.0488e-02,\n",
      "        3.9876e-02, 3.9268e-02, 3.8665e-02, 3.8067e-02, 3.7472e-02, 3.6883e-02,\n",
      "        3.6297e-02, 3.5717e-02, 3.5141e-02, 3.4569e-02, 3.4002e-02, 3.3439e-02,\n",
      "        3.2881e-02, 3.2327e-02, 3.1778e-02, 3.1234e-02, 3.0694e-02, 3.0159e-02,\n",
      "        2.9628e-02, 2.9102e-02, 2.8580e-02, 2.8063e-02, 2.7551e-02, 2.7043e-02,\n",
      "        2.6539e-02, 2.6041e-02, 2.5547e-02, 2.5057e-02, 2.4572e-02, 2.4092e-02,\n",
      "        2.3616e-02, 2.3145e-02, 2.2679e-02, 2.2217e-02, 2.1760e-02, 2.1308e-02,\n",
      "        2.0860e-02, 2.0417e-02, 1.9978e-02, 1.9545e-02, 1.9116e-02, 1.8691e-02,\n",
      "        1.8271e-02, 1.7856e-02, 1.7446e-02, 1.7040e-02, 1.6639e-02, 1.6243e-02,\n",
      "        1.5851e-02, 1.5464e-02, 1.5082e-02, 1.4704e-02, 1.4331e-02, 1.3963e-02,\n",
      "        1.3600e-02, 1.3241e-02, 1.2887e-02, 1.2538e-02, 1.2194e-02, 1.1854e-02,\n",
      "        1.1519e-02, 1.1189e-02, 1.0863e-02, 1.0543e-02, 1.0227e-02, 9.9155e-03,\n",
      "        9.6091e-03, 9.3074e-03, 9.0105e-03, 8.7183e-03, 8.4310e-03, 8.1484e-03,\n",
      "        7.8705e-03, 7.5975e-03, 7.3293e-03, 7.0658e-03, 6.8071e-03, 6.5532e-03,\n",
      "        6.3041e-03, 6.0598e-03, 5.8203e-03, 5.5856e-03, 5.3557e-03, 5.1307e-03,\n",
      "        4.9104e-03, 4.6949e-03, 4.4842e-03, 4.2784e-03, 4.0774e-03, 3.8812e-03,\n",
      "        3.6898e-03, 3.5032e-03, 3.3214e-03, 3.1445e-03, 2.9724e-03, 2.8052e-03,\n",
      "        2.6427e-03, 2.4851e-03, 2.3323e-03, 2.1844e-03, 2.0413e-03, 1.9031e-03,\n",
      "        1.7696e-03, 1.6411e-03, 1.5173e-03, 1.3984e-03, 1.2843e-03, 1.1752e-03,\n",
      "        1.0708e-03, 9.7127e-04, 8.7660e-04, 7.8679e-04, 7.0181e-04, 6.2170e-04,\n",
      "        5.4643e-04, 4.7602e-04, 4.1046e-04, 3.4975e-04, 2.9389e-04, 2.4290e-04,\n",
      "        1.9675e-04, 1.5547e-04, 1.1904e-04, 8.7458e-05, 6.0738e-05, 3.8876e-05,\n",
      "        2.1871e-05, 9.7226e-06, 2.4330e-06, 2.4330e-09])\n",
      "range of bars tensor(2.4330e-09) tensor(1.0000)\n",
      "range of sigmas, tensor(0.0047) tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                    | 0/188 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number parameters of the model: 2282673\n",
      "Model strcuture: DDPM(\n",
      "  (t_embedding): SinousEmbedding()\n",
      "  (up): ModuleList(\n",
      "    (0): F_x_t(\n",
      "      (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (ress): ModuleList(\n",
      "        (0): ResidualBlock(\n",
      "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): F_x_t(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (Q): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (K): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (V): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (out_proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Attention(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (Q): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (K): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (V): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (out_proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (ress): ModuleList(\n",
      "        (0): ResidualBlock(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=64, bias=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=64, bias=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): F_x_t(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (Q): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (K): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (V): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (out_proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Attention(\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (Q): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (K): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (V): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (out_proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (ress): ModuleList(\n",
      "        (0): ResidualBlock(\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (middle): ModuleList(\n",
      "    (0): F_x_t(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (Q): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (K): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (V): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (out_proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Attention(\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (Q): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (K): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (V): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (out_proj): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (ress): ModuleList(\n",
      "        (0): ResidualBlock(\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down): ModuleList(\n",
      "    (0): F_x_t(\n",
      "      (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (Q): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (K): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (V): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (out_proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Attention(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (Q): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (K): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (V): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (out_proj): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (ress): ModuleList(\n",
      "        (0): ResidualBlock(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=64, bias=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=64, bias=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): F_x_t(\n",
      "      (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (attentions): ModuleList(\n",
      "        (0): Attention(\n",
      "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (Q): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (K): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (V): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (out_proj): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Attention(\n",
      "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (Q): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (K): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (V): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (out_proj): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (ress): ModuleList(\n",
      "        (0): ResidualBlock(\n",
      "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): F_x_t(\n",
      "      (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (fc): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (ress): ModuleList(\n",
      "        (0): ResidualBlock(\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=16, bias=True)\n",
      "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (1): ResidualBlock(\n",
      "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "          (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (t_net): Linear(in_features=32, out_features=16, bias=True)\n",
      "          (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (end_mlp): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.2841: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:25<00:00,  1.39it/s]\n",
      "epoch 0, MSE 0.1151, [Valid] 0.1151: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type DDPM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type SinousEmbedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type F_x_t. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ResidualBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "epoch 1, loss 0.0975: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 1, MSE 0.0844, [Valid] 0.0844: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.02it/s]\n",
      "epoch 2, loss 0.0789: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 2, MSE 0.0731, [Valid] 0.0731: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.08it/s]\n",
      "epoch 3, loss 0.0700: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 3, MSE 0.0662, [Valid] 0.0662: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.27it/s]\n",
      "epoch 4, loss 0.0651: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 4, MSE 0.0622, [Valid] 0.0622: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.25it/s]\n",
      "epoch 5, loss 0.0617: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 5, MSE 0.0595, [Valid] 0.0595: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_5.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 0.0591: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 6, MSE 0.0591, [Valid] 0.0591: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.33it/s]\n",
      "epoch 7, loss 0.0585: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 7, MSE 0.0588, [Valid] 0.0588: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00,  9.99it/s]\n",
      "epoch 8, loss 0.0562: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 8, MSE 0.0554, [Valid] 0.0554: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.26it/s]\n",
      "epoch 9, loss 0.0549: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 9, MSE 0.0542, [Valid] 0.0542: 100%|███████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00,  9.97it/s]\n",
      "epoch 10, loss 0.0541: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 10, MSE 0.0535, [Valid] 0.0535: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_10.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 11, loss 0.0532: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 11, MSE 0.0533, [Valid] 0.0533: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.32it/s]\n",
      "epoch 12, loss 0.0532: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 12, MSE 0.0539, [Valid] 0.0539: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.22it/s]\n",
      "epoch 13, loss 0.0521: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 13, MSE 0.0513, [Valid] 0.0513: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.16it/s]\n",
      "epoch 14, loss 0.0509: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 14, MSE 0.0502, [Valid] 0.0502: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.34it/s]\n",
      "epoch 15, loss 0.0502: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 15, MSE 0.0496, [Valid] 0.0496: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_15.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 16, loss 0.0505: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 16, MSE 0.0503, [Valid] 0.0503: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.11it/s]\n",
      "epoch 17, loss 0.0491: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 17, MSE 0.0504, [Valid] 0.0504: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.33it/s]\n",
      "epoch 18, loss 0.0495: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 18, MSE 0.0488, [Valid] 0.0488: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.07it/s]\n",
      "epoch 19, loss 0.0493: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 19, MSE 0.0474, [Valid] 0.0474: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.05it/s]\n",
      "epoch 20, loss 0.0482: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 20, MSE 0.0479, [Valid] 0.0479: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_20.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 21, loss 0.0478: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 21, MSE 0.0479, [Valid] 0.0479: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.03it/s]\n",
      "epoch 22, loss 0.0479: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 22, MSE 0.0473, [Valid] 0.0473: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.30it/s]\n",
      "epoch 23, loss 0.0468: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 23, MSE 0.0468, [Valid] 0.0468: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00,  9.93it/s]\n",
      "epoch 24, loss 0.0466: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 24, MSE 0.0477, [Valid] 0.0477: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.22it/s]\n",
      "epoch 25, loss 0.0476: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 25, MSE 0.0464, [Valid] 0.0464: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_25.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 26, loss 0.0461: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 26, MSE 0.0451, [Valid] 0.0451: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.04it/s]\n",
      "epoch 27, loss 0.0456: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 27, MSE 0.0452, [Valid] 0.0452: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.01it/s]\n",
      "epoch 28, loss 0.0456: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 28, MSE 0.0455, [Valid] 0.0455: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.30it/s]\n",
      "epoch 29, loss 0.0449: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 29, MSE 0.0452, [Valid] 0.0452: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.32it/s]\n",
      "epoch 30, loss 0.0453: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 30, MSE 0.0441, [Valid] 0.0441: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_30.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 31, loss 0.0459: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 31, MSE 0.0439, [Valid] 0.0439: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.09it/s]\n",
      "epoch 32, loss 0.0448: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 32, MSE 0.0442, [Valid] 0.0442: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.10it/s]\n",
      "epoch 33, loss 0.0441: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 33, MSE 0.0443, [Valid] 0.0443: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.37it/s]\n",
      "epoch 34, loss 0.0443: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 34, MSE 0.0435, [Valid] 0.0435: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00,  9.98it/s]\n",
      "epoch 35, loss 0.0438: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 35, MSE 0.0448, [Valid] 0.0448: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_35.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 36, loss 0.0441: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 36, MSE 0.0438, [Valid] 0.0438: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.13it/s]\n",
      "epoch 37, loss 0.0439: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 37, MSE 0.0433, [Valid] 0.0433: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.33it/s]\n",
      "epoch 38, loss 0.0441: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 38, MSE 0.0442, [Valid] 0.0442: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.08it/s]\n",
      "epoch 39, loss 0.0432: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 39, MSE 0.0437, [Valid] 0.0437: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.28it/s]\n",
      "epoch 40, loss 0.0430: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 40, MSE 0.0424, [Valid] 0.0424: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_40.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 41, loss 0.0432: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:22<00:00,  1.39it/s]\n",
      "epoch 41, MSE 0.0431, [Valid] 0.0431: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.09it/s]\n",
      "epoch 42, loss 0.0429: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 42, MSE 0.0432, [Valid] 0.0432: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.15it/s]\n",
      "epoch 43, loss 0.0431: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 43, MSE 0.0429, [Valid] 0.0429: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.34it/s]\n",
      "epoch 44, loss 0.0426: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 44, MSE 0.0426, [Valid] 0.0426: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.31it/s]\n",
      "epoch 45, loss 0.0430: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 45, MSE 0.0425, [Valid] 0.0425: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00,  9.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_45.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 46, loss 0.0425: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 46, MSE 0.0420, [Valid] 0.0420: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.06it/s]\n",
      "epoch 47, loss 0.0426: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 47, MSE 0.0424, [Valid] 0.0424: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.33it/s]\n",
      "epoch 48, loss 0.0421: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 48, MSE 0.0419, [Valid] 0.0419: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.06it/s]\n",
      "epoch 49, loss 0.0426: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 49, MSE 0.0430, [Valid] 0.0430: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.08it/s]\n",
      "epoch 50, loss 0.0426: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 50, MSE 0.0419, [Valid] 0.0419: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_50.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 51, loss 0.0420: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 51, MSE 0.0423, [Valid] 0.0423: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00,  9.99it/s]\n",
      "epoch 52, loss 0.0421: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 52, MSE 0.0417, [Valid] 0.0417: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.31it/s]\n",
      "epoch 53, loss 0.0414: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 53, MSE 0.0418, [Valid] 0.0418: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.02it/s]\n",
      "epoch 54, loss 0.0424: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:23<00:00,  1.28it/s]\n",
      "epoch 54, MSE 0.0416, [Valid] 0.0416: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  9.32it/s]\n",
      "epoch 55, loss 0.0418: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:26<00:00,  1.38it/s]\n",
      "epoch 55, MSE 0.0413, [Valid] 0.0413: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_55.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 56, loss 0.0418: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 56, MSE 0.0416, [Valid] 0.0416: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.26it/s]\n",
      "epoch 57, loss 0.0417: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 57, MSE 0.0416, [Valid] 0.0416: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.28it/s]\n",
      "epoch 58, loss 0.0417: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 58, MSE 0.0415, [Valid] 0.0415: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.09it/s]\n",
      "epoch 59, loss 0.0416: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 59, MSE 0.0409, [Valid] 0.0409: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.03it/s]\n",
      "epoch 60, loss 0.0413: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 60, MSE 0.0411, [Valid] 0.0411: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_60.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 61, loss 0.0419: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 61, MSE 0.0410, [Valid] 0.0410: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.10it/s]\n",
      "epoch 62, loss 0.0409: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 62, MSE 0.0404, [Valid] 0.0404: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.27it/s]\n",
      "epoch 63, loss 0.0414: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:24<00:00,  1.28it/s]\n",
      "epoch 63, MSE 0.0411, [Valid] 0.0411: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  9.35it/s]\n",
      "epoch 64, loss 0.0411: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:28<00:00,  1.32it/s]\n",
      "epoch 64, MSE 0.0404, [Valid] 0.0404: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  9.26it/s]\n",
      "epoch 65, loss 0.0417: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:31<00:00,  1.07it/s]\n",
      "epoch 65, MSE 0.0401, [Valid] 0.0401: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_65.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 66, loss 0.0411: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [03:00<00:00,  1.07it/s]\n",
      "epoch 66, MSE 0.0409, [Valid] 0.0409: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.65it/s]\n",
      "epoch 67, loss 0.0409: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [03:00<00:00,  1.07it/s]\n",
      "epoch 67, MSE 0.0410, [Valid] 0.0410: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.60it/s]\n",
      "epoch 68, loss 0.0412: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [03:00<00:00,  1.07it/s]\n",
      "epoch 68, MSE 0.0408, [Valid] 0.0408: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  8.65it/s]\n",
      "epoch 69, loss 0.0409: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:39<00:00,  1.27it/s]\n",
      "epoch 69, MSE 0.0410, [Valid] 0.0410: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:05<00:00,  9.22it/s]\n",
      "epoch 70, loss 0.0410: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:26<00:00,  1.37it/s]\n",
      "epoch 70, MSE 0.0408, [Valid] 0.0408: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_70.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 71, loss 0.0409: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 71, MSE 0.0405, [Valid] 0.0405: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.17it/s]\n",
      "epoch 72, loss 0.0406: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 72, MSE 0.0404, [Valid] 0.0404: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.30it/s]\n",
      "epoch 73, loss 0.0407: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 73, MSE 0.0402, [Valid] 0.0402: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.28it/s]\n",
      "epoch 74, loss 0.0405: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 74, MSE 0.0410, [Valid] 0.0410: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.09it/s]\n",
      "epoch 75, loss 0.0405: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 75, MSE 0.0402, [Valid] 0.0402: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_75.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 76, loss 0.0407: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 76, MSE 0.0400, [Valid] 0.0400: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00,  9.95it/s]\n",
      "epoch 77, loss 0.0399: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 77, MSE 0.0402, [Valid] 0.0402: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.23it/s]\n",
      "epoch 78, loss 0.0403: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 78, MSE 0.0400, [Valid] 0.0400: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.13it/s]\n",
      "epoch 79, loss 0.0403: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 79, MSE 0.0397, [Valid] 0.0397: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.11it/s]\n",
      "epoch 80, loss 0.0400: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.37it/s]\n",
      "epoch 80, MSE 0.0398, [Valid] 0.0398: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_80.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 81, loss 0.0406: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 81, MSE 0.0402, [Valid] 0.0402: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.27it/s]\n",
      "epoch 82, loss 0.0400: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.34it/s]\n",
      "epoch 82, MSE 0.0399, [Valid] 0.0399: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00,  9.87it/s]\n",
      "epoch 83, loss 0.0397: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:23<00:00,  1.39it/s]\n",
      "epoch 83, MSE 0.0398, [Valid] 0.0398: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.35it/s]\n",
      "epoch 84, loss 0.0403: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 84, MSE 0.0394, [Valid] 0.0394: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.35it/s]\n",
      "epoch 85, loss 0.0400: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 85, MSE 0.0397, [Valid] 0.0397: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_85.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 86, loss 0.0399: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 86, MSE 0.0399, [Valid] 0.0399: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.01it/s]\n",
      "epoch 87, loss 0.0401: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.36it/s]\n",
      "epoch 87, MSE 0.0397, [Valid] 0.0397: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.12it/s]\n",
      "epoch 88, loss 0.0401: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 88, MSE 0.0397, [Valid] 0.0397: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.29it/s]\n",
      "epoch 89, loss 0.0396: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 89, MSE 0.0390, [Valid] 0.0390: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.08it/s]\n",
      "epoch 90, loss 0.0397: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.39it/s]\n",
      "epoch 90, MSE 0.0399, [Valid] 0.0399: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualize to /home/zhh24/samples/diffuse_epoch_90.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 91, loss 0.0399: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 188/188 [02:20<00:00,  1.38it/s]\n",
      "epoch 91, MSE 0.0401, [Valid] 0.0401: 100%|██████████████████████████████████████████████████████████████████████████████████████| 47/47 [00:04<00:00, 10.32it/s]\n",
      "epoch 92, loss 0.0392:  22%|█████████████████████▊                                                                              | 41/188 [00:30<01:49,  1.34it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8080/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# parent_dir = os.path.abspath('/root/DeepLearning')\n",
    "parent_dir = os.path.abspath('/home/zhh24/DeepLearning')\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "print('appended',parent_dir)\n",
    "\n",
    "import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "mnist = utils.MNIST(batch_size=256,data_aug=False)\n",
    "train_loader = mnist.train_dataloader\n",
    "valid_loader = mnist.valid_dataloader\n",
    "T=1000\n",
    "# beta1=1e-4 # variance of lowest temperature\n",
    "# betaT=6e-2 # variance of highest temperature\n",
    "eps = 8e-3\n",
    "steps=torch.linspace(0,T,steps=T+1,dtype=torch.float)\n",
    "f_t=torch.cos(((steps/T+eps)/(1.0+eps))*3.14159*0.5)**2\n",
    "betas=torch.clamp(1.0-f_t[1:]/f_t[:T],0.0,0.999)\n",
    "\n",
    "# step = torch.log(torch.tensor(betaT/beta1))/(T-1)\n",
    "# betas = beta1 * torch.exp(step*torch.arange(T,dtype=torch.float).to(device))\n",
    "# step = (betaT-beta1)/(T-1)\n",
    "# betas = torch.arange(T,dtype=torch.float,device=device) * step + beta1\n",
    "\n",
    "\n",
    "alphas = 1-betas\n",
    "alpha_bars = alphas.clone()\n",
    "for i in range(1,T):\n",
    "    alpha_bars[i] *= alpha_bars[i-1]\n",
    "\n",
    "                    # we re-define a way to generate hyperparameters\n",
    "                    # alpha_bar_0 = .9\n",
    "                    # # alpha_bar_mid = .3\n",
    "                    # alpha_bar_T = 1e-3\n",
    "                    # alpha_bars = torch.zeros(T,dtype=torch.float)\n",
    "                    # # alpha_bars[:T//2] = alpha_bar_0 + (alpha_bar_mid-alpha_bar_0) * torch.arange(T//2,dtype=torch.float,device=device) / (T//2)\n",
    "                    # # alpha_bars[T//2:] = alpha_bar_mid + (alpha_bar_T-alpha_bar_mid) * torch.arange(T//2,dtype=torch.float,device=device) / (T//2)\n",
    "                    # alpha_bars = alpha_bar_0 + (alpha_bar_T-alpha_bar_0) * torch.arange(T,dtype=torch.float,device=device) / T\n",
    "                    # alphas = alpha_bars.clone()\n",
    "                    # for i in range(1,T):\n",
    "                    #     alphas[i] = alpha_bars[i] / alpha_bars[i-1]\n",
    "                    # betas = 1-alphas\n",
    "\n",
    "print(alpha_bars)\n",
    "print('range of bars',alpha_bars.min(),alpha_bars.max())\n",
    "# print(alphas)\n",
    "\n",
    "sqrt = torch.sqrt\n",
    "sigmas = sqrt(betas * (1-alpha_bars / alphas)/(1-alpha_bars))\n",
    "sigmas[0] = 1\n",
    "print('range of sigmas,',sigmas.min(),sigmas.max())\n",
    "alphas = alphas.to(device)\n",
    "alpha_bars = alpha_bars.to(device)\n",
    "betas = betas.to(device)\n",
    "sigmas = sigmas.to(device)\n",
    "weights = torch.ones(T,dtype=torch.float,device=device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model:DDPM,save_dir):\n",
    "    x = torch.randn([100,784]).to(device)\n",
    "    for t in range(T-1,-1,-1):\n",
    "        sigmaz = torch.randn_like(x)*sigmas[t]\n",
    "        if t==0:\n",
    "            sigmaz = 0\n",
    "        noise_pred = model(x,t*torch.ones(x.shape[0],dtype=torch.long,device=device))\n",
    "        x0_pred = (x - noise_pred * sqrt(1 - alpha_bars[t])) / sqrt(alpha_bars[t]).clamp(-1,1)\n",
    "        mean_pred = (sqrt(alphas[t]) * (1-alpha_bars[t]/alphas[t]) * x + sqrt(alpha_bars[t]/alphas[t]) * (1-alphas[t]) * x0_pred) / (1-alpha_bars[t])\n",
    "        x = mean_pred + sigmaz\n",
    "        # x = torch.clamp(x,0,1)\n",
    "    grid = torchvision.utils.make_grid(post_process(x).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize(model,save_dir):\n",
    "    interval = (T-1) // 20\n",
    "    x = torch.randn([10,784]).to(device)\n",
    "    x_history = []\n",
    "    for t in range(T-1,-1,-1):\n",
    "        sigmaz = torch.randn_like(x)*((betas[t])**0.5).to(device)\n",
    "        if t==0:\n",
    "            sigmaz = 0\n",
    "        x = (x-(1-alphas[t])/(sqrt(1-alpha_bars[t]))*model(x,t*torch.ones(x.shape[0],dtype=torch.long,device=device)))/(sqrt(alphas[t]))+sigmaz\n",
    "        # x = torch.clamp(x,0,1)\n",
    "        x_history.append(x)\n",
    "    # print('cat.shape',torch.cat(x_history,dim=0).shape)\n",
    "    grid = torchvision.utils.make_grid(post_process(torch.stack(x_history,dim=0)[::interval,...]).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "    print('Saved visualize to',os.path.abspath(save_dir))\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_denoise(model,save_dir):\n",
    "    # get 10 images from the dataset\n",
    "    x,_ = next(iter(valid_loader))\n",
    "    x = x[:20,...].reshape(20,784).to(device)\n",
    "    x = pre_process(x)\n",
    "    t = torch.tensor([i * T // 20 for i in range(20)],dtype=torch.long,device=device)\n",
    "    noise = torch.randn_like(x).reshape(-1,784)\n",
    "    v1 = (sqrt(alpha_bars[t]).reshape(-1,1)*x).reshape(-1,784)\n",
    "    v2 = sqrt(1-alpha_bars[t]).reshape(-1,1)*noise\n",
    "    x_corr = v1+v2\n",
    "    est = model(x_corr,t)\n",
    "    x_rec = (x_corr - sqrt(1-alpha_bars[t]).reshape(-1,1)*est)/(sqrt(alpha_bars[t])).reshape(-1,1)\n",
    "    grid_orig = torchvision.utils.make_grid(post_process(x).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    grid_corr = torchvision.utils.make_grid(post_process(x_corr).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    grid_rec = torchvision.utils.make_grid(post_process(x_rec).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    # add noise level infomation to the image\n",
    "    noise_level = (1-alpha_bars[t]).reshape(-1).tolist()\n",
    "    ori_mse = noise.pow(2).mean(dim=1).reshape(-1).tolist()\n",
    "    mse = ((est-noise)**2).mean(dim=1).reshape(-1).tolist()\n",
    "    print(noise_level)\n",
    "    print(ori_mse)\n",
    "    print(mse)\n",
    "    grid = torch.cat([grid_orig,grid_corr,grid_rec],dim=1)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "    print('Saved denoise to',os.path.abspath(save_dir))\n",
    "\n",
    "def plot_loss(losses,save_dir):\n",
    "    losses_vals, t_vals = zip(*losses)\n",
    "    losses_vals = torch.cat(losses_vals,dim=0)\n",
    "    t_vals = torch.cat(t_vals,dim=0)\n",
    "    # print('t_vals',t_vals)\n",
    "    # print('losses_vals',losses_vals)\n",
    "\n",
    "    results = []\n",
    "    for t in range(T):\n",
    "        this_t = abs(t_vals.float()-float(t))<0.5\n",
    "        results.append(torch.sum(torch.where(this_t,losses_vals,torch.tensor(0.,device=device))).item() / (torch.sum(this_t.float())+1e-3).item())\n",
    "    plt.plot(results)\n",
    "    plt.ylim(0,max(results)* 1.2)\n",
    "    plt.savefig(save_dir)\n",
    "    plt.close()\n",
    "    # weights = (torch.tensor(results,device=device)) # weights\n",
    "    weights = torch.ones(T,dtype=torch.float,device=device)\n",
    "    # weights[:10]=0\n",
    "    # weights[10:80] /= 100\n",
    "    return weights\n",
    "\n",
    "def pre_process(x):\n",
    "    # do the logit transform\n",
    "    # return (torch.log(x+1e-3)-torch.log(1-x+1e-3))\n",
    "    return x*2-1 #MODIFIED\n",
    "    return (x+1)/2\n",
    "\n",
    "def post_process(x):\n",
    "    # return torch.sigmoid(x)\n",
    "    return (x+1)/2 #MODIFIED\n",
    "    return x*2-1\n",
    "\n",
    "def train(epochs,model:DDPM,optimizer,eval_interval=5):\n",
    "    global weights\n",
    "    for epoch in range(epochs):\n",
    "        # print('weights normalized:',weights/weights.sum())\n",
    "        all_ts = torch.distributions.Categorical(weights).sample((50000,))\n",
    "        cnt = 0\n",
    "        model.train()\n",
    "        with tqdm(train_loader) as bar:\n",
    "            losses = []\n",
    "            for x,_ in bar:\n",
    "                cnt += x.shape[0]\n",
    "                x = pre_process(x.to(device))\n",
    "                epss = torch.randn_like(x).reshape(-1,784).to(device)\n",
    "                # ts = torch.randint(0,T,(x.shape[0],),device=device,dtype=torch.long)\n",
    "                ts = all_ts[cnt-x.shape[0]:cnt]\n",
    "                alpha_tbars = alpha_bars[ts]\n",
    "                value = (sqrt(alpha_tbars).reshape(-1,1,1,1)*x).reshape(-1,784)+sqrt(1-alpha_tbars).reshape(-1,1)*epss\n",
    "                out = model(value,ts) # [batch,784]\n",
    "                # loss = ((epss-out).pow(2).mean(dim=-1) * (betas[ts])/(2*alphas[ts]*(1-alpha_tbars))).sum(dim=0)\n",
    "                loss = ((epss-out).pow(2).mean(dim=-1)).mean(dim=0)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "                bar.set_description('epoch {}, loss {:.4f}'.format(epoch,sum(losses)/len(losses)))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(valid_loader) as bar:\n",
    "                mses = []\n",
    "                losses = []\n",
    "                losses_for_t = []\n",
    "                for x,_ in bar:\n",
    "                    x = pre_process(x.to(device))\n",
    "                    epss = torch.randn_like(x).reshape(-1,784).to(device)\n",
    "                    ts = torch.randint(0,T,(x.shape[0],),device=device,dtype=torch.long)\n",
    "                    # print(ts)\n",
    "                    alpha_tbars = alpha_bars[ts]\n",
    "                    value = (sqrt(alpha_tbars).reshape(-1,1,1,1)*x).reshape(-1,784)+sqrt(1-alpha_tbars).reshape(-1,1)*epss\n",
    "                    out = model(value,ts)\n",
    "                    mse = F.mse_loss(epss,out)\n",
    "                    mses.append(mse.item())\n",
    "                    loss = ((epss-out).pow(2).mean(dim=-1))\n",
    "                    # loss = (epss-out).pow(2).mean(dim=-1)\n",
    "                    losses_for_t.append((loss.clone().detach(),ts))\n",
    "                    loss = (loss).mean(dim=0)\n",
    "                    losses.append(loss.item())\n",
    "                    bar.set_description('epoch {}, MSE {:.4f}, [Valid] {:.4f}'.format(epoch,sum(mses)/len(mses),sum(losses)/len(losses)))\n",
    "                    \n",
    "        if epoch % eval_interval == 0:\n",
    "            visualize(model,save_dir=os.path.join('./samples',f'diffuse_epoch_{epoch}.png'))\n",
    "            sample(model,save_dir=os.path.join('./samples',f'sample_epoch_{epoch}.png'))\n",
    "            # visualize_denoise(model,save_dir=os.path.join('./samples',f'denoise_epoch_{epoch}.png'))\n",
    "            weights = plot_loss(losses_for_t,save_dir=os.path.join('./samples',f'loss_epoch_{epoch}.png'))\n",
    "            torch.save(model,os.path.join('./samples',f'epoch_{epoch}.pt'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = DDPM().to(device)\n",
    "    print('Number parameters of the model:', sum(p.numel() for p in model.parameters()))\n",
    "    print('Model strcuture:',model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=2e-4)\n",
    "    os.makedirs('./samples',exist_ok=True)\n",
    "    # sample(model,save_dir=os.path.join('./samples',f'init.png'))\n",
    "    # visualize(model,save_dir=os.path.join('./samples',f'init_visualize.png'))\n",
    "    train(200,model,optimizer,eval_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
