{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Sep 15 17:00:45 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              40W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "/nobackup/users/zhh24/anaconda3/envs/DYY/bin/python\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!which python | grep DYY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class SinousEmbedding(nn.Module):\n",
    "#     def __init__(self, dim) -> None:\n",
    "#         super().__init__()\n",
    "#         assert dim%2==0,NotImplementedError()\n",
    "#         self.angles = (1000.**(-2/dim))**torch.arange(1,dim//2+1,1,dtype=torch.float).cuda()\n",
    "#         self.angles.requires_grad_(False)\n",
    "#     def forward(self,x):\n",
    "#         angles = torch.einsum('m,i->im',self.angles,x.float())\n",
    "#         return torch.cat((torch.sin(angles),torch.cos(angles)),dim=1)\n",
    "\n",
    "# class DDPM(nn.Module):\n",
    "#     def __init__(self, *args, **kwargs) -> None:\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.in_size = 28 * 28\n",
    "#         self.t_embedding_dim = 256\n",
    "#         self.t_embedding = SinousEmbedding(dim=self.t_embedding_dim)\n",
    "#         self.up = nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(784+self.t_embedding_dim,64),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(64,32),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#             # nn.Sequential(\n",
    "#             #     nn.Linear(256,256),\n",
    "#             #     # nn.LeakyReLU(0.1),\n",
    "#             # ),\n",
    "#         ])\n",
    "#         self.middle = nn.ModuleList([\n",
    "#             nn.Linear(32,32),\n",
    "#             # nn.LeakyReLU(0.1),\n",
    "#         ])\n",
    "#         self.down= nn.ModuleList([\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(32,32),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#             # nn.Sequential(\n",
    "#             #     nn.Linear(256,256),\n",
    "#             #     # nn.LeakyReLU(0.1),\n",
    "#             # ),\n",
    "#             nn.Sequential(\n",
    "#                 nn.Linear(32,64),\n",
    "#                 nn.ReLU(),\n",
    "#             ),\n",
    "#         ])\n",
    "#         self.end_mlp = nn.Linear(64,784)\n",
    "#         self.apply_init()\n",
    "\n",
    "#     def apply_init(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.xavier_normal_(m.weight)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "\n",
    "#     def forward(self,x,t):\n",
    "#         x = x.reshape(-1,784)\n",
    "#         ttensor = self.t_embedding(t) # [batch, 256]\n",
    "#         batch = x.shape[0]\n",
    "#         xc = x.clone()\n",
    "#         ups = []\n",
    "#         x = torch.cat((x,ttensor),dim=-1)\n",
    "#         for ly in self.up:\n",
    "#             x = ly(x)\n",
    "#             ups.append(x.clone())\n",
    "#         for ly in self.middle:\n",
    "#             x = ly(x)\n",
    "#         for ly in self.down:\n",
    "#             x = ly(x) + ups.pop()\n",
    "\n",
    "#         x = self.end_mlp(x)\n",
    "#         x = (x + xc)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# class SinousEmbedding(nn.Module):\n",
    "#     def __init__(self, dim) -> None:\n",
    "#         super().__init__()\n",
    "#         assert dim%2==0,NotImplementedError()\n",
    "#         self.angles = (500.**(-2/dim))**torch.arange(1,dim//2+1,1,dtype=torch.float).cuda()\n",
    "#         self.angles.requires_grad_(False)\n",
    "#     def forward(self,x):\n",
    "#         angles = torch.einsum('m,i->im',self.angles,x.float())\n",
    "#         return torch.cat((torch.sin(angles),torch.cos(angles)),dim=1)\n",
    "\n",
    "class F_x_t(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,out_channels,out_size,kernel_size=3,t_shape=64,t_num=100) -> None:\n",
    "        super().__init__()\n",
    "        # self.t_channels = out_channels // 2\n",
    "        # self.conv_channels = out_channels - self.t_channels\n",
    "        self.t_channels = out_channels\n",
    "        self.t_num = t_num\n",
    "        self.conv_channels = out_channels\n",
    "        self.conv = nn.Conv2d(in_channels, self.conv_channels, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "        self.out_size = out_size\n",
    "        # self.fc = nn.Linear(t_shape, self.t_channels)\n",
    "        self.fc = nn.Embedding(t_shape, self.t_num)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if self.t_channels == 0:\n",
    "            return self.conv(x)\n",
    "        # return torch.cat([self.conv(x),self.fc(t).unsqueeze(-1).unsqueeze(-1).expand(t.shape[0], self.t_channels, self.out_size, self.out_size)],dim=1).relu()\n",
    "        return (self.conv(x) + self.fc(t).unsqueeze(-1).unsqueeze(-1).expand(t.shape[0], self.t_channels, self.out_size, self.out_size))\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.t_embedding_dim = 32\n",
    "        self.t_num = 100\n",
    "        # self.t_embedding = SinousEmbedding(dim=self.t_embedding_dim)\n",
    "        self.up= nn.ModuleList([\n",
    "            F_x_t(in_channels=1,out_channels=64,out_size=32,kernel_size=7,t_shape=self.t_embedding_dim),\n",
    "            F_x_t(in_channels=64,out_channels=128,out_size=16,kernel_size=5,t_shape=self.t_embedding_dim),\n",
    "            F_x_t(in_channels=128,out_channels=128,out_size=8,kernel_size=3,t_shape=self.t_embedding_dim),\n",
    "        ])\n",
    "        self.middle = nn.ModuleList([\n",
    "            nn.Identity()\n",
    "        ])\n",
    "        self.down= nn.ModuleList([\n",
    "            F_x_t(in_channels=128,out_channels=128,out_size=4,kernel_size=3,t_shape=self.t_embedding_dim),\n",
    "            F_x_t(in_channels=128,out_channels=128,out_size=8,kernel_size=5,t_shape=self.t_embedding_dim),\n",
    "            F_x_t(in_channels=128,out_channels=64,out_size=16,kernel_size=7,t_shape=self.t_embedding_dim),\n",
    "        ])\n",
    "        self.end_mlp = nn.Conv2d(64,1,kernel_size=5,padding=2)\n",
    "        self.apply_init()\n",
    "    \n",
    "    def apply_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self,x,t):\n",
    "        x = x.reshape(-1,1,28,28)\n",
    "        x = F.pad(x,(2,2,2,2),mode='constant',value=0)\n",
    "        ttensor = self.t_embedding(t) # [batch, 256]\n",
    "        batch = x.shape[0]\n",
    "        # xc = x.clone()\n",
    "        ups = []\n",
    "        for ly in self.up:\n",
    "            x = ly(x,ttensor)\n",
    "            ups.append(x.clone()) # append: 28x28, 14x14\n",
    "            x = nn.AvgPool2d(2)(x)\n",
    "        for ly in self.middle:\n",
    "            x = ly(x)\n",
    "        for ly in self.down:\n",
    "            x = ly(x,ttensor)\n",
    "            x = nn.Upsample(scale_factor=2)(x) + ups.pop()\n",
    "        x = self.end_mlp(x)\n",
    "        x = x[:,:,2:30,2:30]\n",
    "        return x.reshape(batch,28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appended /home/zhh24/DeepLearning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4165893/1925303444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# betas = beta1 * torch.exp(step*torch.arange(T,dtype=torch.float).to(device))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbetaT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mbetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbetaT\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath('/home/zhh24/DeepLearning')\n",
    "\n",
    "sys.path.append(parent_dir)\n",
    "print('appended',parent_dir)\n",
    "\n",
    "import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "mnist = utils.MNIST(batch_size=256)\n",
    "train_loader = mnist.train_dataloader\n",
    "valid_loader = mnist.valid_dataloader\n",
    "T=100\n",
    "beta1=1e-4 # variance of lowest temperature\n",
    "betaT=3e-2 # variance of highest temperature\n",
    "# step = torch.log(torch.tensor(betaT/beta1))/(T-1)\n",
    "# betas = beta1 * torch.exp(step*torch.arange(T,dtype=torch.float).to(device))\n",
    "step = (betaT-beta1)/(T-1)\n",
    "betas = torch.arange(beta1,betaT+step,step).to(device)\n",
    "\n",
    "alphas = 1-betas\n",
    "alpha_bars = alphas.clone()\n",
    "for i in range(1,T):\n",
    "    alpha_bars[i] *= alpha_bars[i-1]\n",
    "print(alpha_bars)\n",
    "print(alphas)\n",
    "sqrt = torch.sqrt\n",
    "sigmas = sqrt(betas)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model:DDPM,save_dir):\n",
    "    x = torch.randn([100,784]).to(device)\n",
    "    for t in range(T-1,-1,-1):\n",
    "        sigmaz = torch.randn_like(x)*sigmas[t]\n",
    "        if t==0:\n",
    "            sigmaz = 0\n",
    "        x = (x-(1-alphas[t])/(sqrt(1-alpha_bars[t]))*model(x,t*torch.ones(x.shape[0],dtype=torch.long,device=device)))/(sqrt(alphas[t]))+sigmaz\n",
    "        # x = torch.clamp(x,0,1)\n",
    "    grid = torchvision.utils.make_grid(post_process(x).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize(model,save_dir):\n",
    "    x = torch.randn([10,784]).to(device)\n",
    "    x_history = []\n",
    "    for t in range(T-1,-1,-1):\n",
    "        sigmaz = torch.randn_like(x)*((betas[t])**0.5).to(device)\n",
    "        if t==0:\n",
    "            sigmaz = 0\n",
    "        x = (x-(1-alphas[t])/(sqrt(1-alpha_bars[t]))*model(x,t*torch.ones(x.shape[0],dtype=torch.long,device=device)))/(sqrt(alphas[t]))+sigmaz\n",
    "        # x = torch.clamp(x,0,1)\n",
    "        x_history.append(x)\n",
    "    # print('cat.shape',torch.cat(x_history,dim=0).shape)\n",
    "    grid = torchvision.utils.make_grid(post_process(torch.cat(x_history,dim=0)[3::4,...]).reshape(-1,1,28,28).cpu(), nrow=10)\n",
    "    torchvision.utils.save_image(grid, save_dir)\n",
    "    print('Saved visualize to',os.path.abspath(save_dir))\n",
    "\n",
    "def pre_process(x):\n",
    "    # do the logit transform\n",
    "    # return (torch.log(x+1e-3)-torch.log(1-x+1e-3))\n",
    "    return (x+1)/2\n",
    "\n",
    "def post_process(x):\n",
    "    # return torch.sigmoid(x)\n",
    "    return x*2-1\n",
    "\n",
    "def train(epochs,model:DDPM,optimizer,eval_interval=5):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        with tqdm(train_loader) as bar:\n",
    "            losses = []\n",
    "            for x,_ in bar:\n",
    "                x = pre_process(x.to(device))\n",
    "                epss = torch.randn_like(x).reshape(-1,784).to(device)\n",
    "                ts = torch.randint(0,T,(x.shape[0],),device=device,dtype=torch.long)\n",
    "                alpha_tbars = alpha_bars[ts]\n",
    "                value = (sqrt(alpha_tbars).reshape(-1,1,1,1)*x).reshape(-1,784)+sqrt(1-alpha_tbars).reshape(-1,1)*epss\n",
    "                out = model(value,ts) # [batch,784]\n",
    "                # loss = ((epss-out).pow(2).mean(dim=-1) * (betas[ts])/(2*alphas[ts]*(1-alpha_tbars))).sum(dim=0)\n",
    "                loss = (epss-out).pow(2).mean(dim=-1).mean(dim=0)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "                bar.set_description('epoch {}, loss {:.4f}'.format(epoch,sum(losses)/len(losses)))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(valid_loader) as bar:\n",
    "                mses = []\n",
    "                losses = []\n",
    "                for x,_ in bar:\n",
    "                    x = pre_process(x.to(device))\n",
    "                    epss = torch.randn_like(x).reshape(-1,784).to(device)\n",
    "                    ts = torch.randint(0,T,(x.shape[0],),device=device,dtype=torch.long)\n",
    "                    alpha_tbars = alpha_bars[ts]\n",
    "                    value = (sqrt(alpha_tbars).reshape(-1,1,1,1)*x).reshape(-1,784)+sqrt(1-alpha_tbars).reshape(-1,1)*epss\n",
    "                    out = model(value,ts)\n",
    "                    mse = F.mse_loss(epss,out)\n",
    "                    mses.append(mse.item())\n",
    "                    # loss = ((epss-out).pow(2).mean(dim=-1) * (betas[ts])/(2*alphas[ts]*(1-alpha_tbars))).sum(dim=0)\n",
    "                    loss = (epss-out).pow(2).mean(dim=-1).mean(dim=0)\n",
    "                    losses.append(loss.item())\n",
    "                    bar.set_description('epoch {}, MSE {:.4f}, [Valid] {:.4f}'.format(epoch,sum(mses)/len(mses),sum(losses)/len(losses)))\n",
    "                    \n",
    "        if epoch % eval_interval == 0:\n",
    "            visualize(model,save_dir=os.path.join('./samples',f'diffuse_epoch_{epoch}.png'))\n",
    "            sample(model,save_dir=os.path.join('./samples',f'sample_epoch_{epoch}.png'))\n",
    "            torch.save(model,os.path.join('./samples',f'epoch_{epoch}.pt'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = DDPM().to(device)\n",
    "    print('Number parameters of the model:', sum(p.numel() for p in model.parameters()))\n",
    "    print('Model strcuture:',model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "    os.makedirs('./samples',exist_ok=True)\n",
    "    sample(model,save_dir=os.path.join('./samples',f'init.png'))\n",
    "    visualize(model,save_dir=os.path.join('./samples',f'init_visualize.png'))\n",
    "    train(100,model,optimizer,eval_interval=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
